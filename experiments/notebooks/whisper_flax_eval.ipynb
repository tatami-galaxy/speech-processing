{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "| Size   | Layers | Width | Heads | Parameters | English-only                                         | Multilingual                                      |\n",
    "|--------|--------|-------|-------|------------|------------------------------------------------------|---------------------------------------------------|\n",
    "| tiny   | 4      | 384   | 6     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny.)  |\n",
    "| base   | 6      | 512   | 8     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)   |\n",
    "| small  | 12     | 768   | 12    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)  |\n",
    "| medium | 24     | 1024  | 16    | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium) |\n",
    "| large  | 32     | 1280  | 20    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)  |\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujan/opt/anaconda3/envs/asr_jax/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import dirname\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "import time, math\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import jax_utils, traverse_util\n",
    "from flax.jax_utils import unreplicate, pad_shard_unpad\n",
    "import orbax\n",
    "from orbax.checkpoint import checkpoint_utils\n",
    "from flax.training import (\n",
    "    train_state,\n",
    "    orbax_utils\n",
    ")\n",
    "from flax.training.common_utils import (\n",
    "    onehot,\n",
    "    shard,\n",
    "    shard_prng_key,\n",
    "    get_metrics\n",
    ")\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    GenerationConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    FlaxWhisperForConditionalGeneration,\n",
    ")\n",
    "from transformers import (\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "import datasets\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset,\n",
    "    DatasetDict,\n",
    "    Audio\n",
    ")\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# sending telemetry\n",
    "# tracking the example usage helps us better allocate resources to maintain them\n",
    "# the information sent is the one passed as arguments along with your Python/PyTorch versions\n",
    "send_example_telemetry(\"run_summarization\", framework=\"flax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get root directory\n",
    "# only works if run from within 'speech-processing' directory\n",
    "# else replace `root` with correct path\n",
    "root = os.path.abspath('')\n",
    "while root.split('/')[-1] != 'speech-processing':\n",
    "    root = dirname(root)\n",
    "\n",
    "# constants\n",
    "LANG_TO_ID = {\"hindi\" : \"<|hi|>\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# set seed\n",
    "set_seed(seed)\n",
    "\n",
    "# model\n",
    "model_name_or_path = 'openai/whisper-small'\n",
    "model_lang = 'hindi'\n",
    "task = 'transcribe'\n",
    "dtype = 'float32'  # float16\n",
    "generation_max_length = 225\n",
    "per_device_eval_batch_size = 4\n",
    "eval_batch_size = int(per_device_eval_batch_size) * jax.device_count()\n",
    "num_beams = 1\n",
    "label_smoothing_factor = 0.0\n",
    "learning_rate = 1e-5\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "adam_epsilon = 1e-6\n",
    "weight_decay = 0.0\n",
    "train_steps = 2000\n",
    "warmup_steps = 0\n",
    "max_to_keep = 3\n",
    "\n",
    "# flags\n",
    "freeze_encoder = False\n",
    "\n",
    "# data\n",
    "data_dir = 'mozilla-foundation/common_voice_11_0'\n",
    "data_lang = 'hi'\n",
    "max_train_samples = 100  # None\n",
    "max_test_samples = 20  # None\n",
    "\n",
    "# output / checkpoint directory\n",
    "model_str = model_name_or_path.split('/')[-1]\n",
    "data_str = data_dir.split('/')[-1]\n",
    "output_dir = root+'/models/whisper/'+model_str+'_jax_'+data_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    dropout_rng: jnp.ndarray\n",
    "\n",
    "    def replicate(self):\n",
    "        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n",
    "    \"\"\"\n",
    "    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n",
    "    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        batch_idx = jax.random.permutation(rng, len(dataset))\n",
    "        batch_idx = np.asarray(batch_idx)\n",
    "    else:\n",
    "        batch_idx = np.arange(len(dataset))\n",
    "\n",
    "    if drop_last:\n",
    "        steps_per_epoch = len(dataset) // batch_size\n",
    "        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
    "    else:\n",
    "        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
    "        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
    "\n",
    "    for idx in batch_idx:\n",
    "        batch = dataset[idx]\n",
    "        batch = {k: np.array(v) for k, v in batch.items()}\n",
    "\n",
    "        yield batch\n",
    "\n",
    "\n",
    "# in Flax, for seq2seq models we need to pass `decoder_input_ids`\n",
    "# as the Flax models don't accept `labels`, we need to prepare the decoder_input_ids here\n",
    "# `shift_tokens_right` function\n",
    "# copied from transformers.models.bart.modeling_flax_bart.shift_tokens_right\n",
    "def shift_tokens_right(input_ids: np.array, pad_token_id: int, decoder_start_token_id: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "\n",
    "    shifted_input_ids = np.zeros_like(input_ids)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1]\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "    shifted_input_ids = np.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)\n",
    "\n",
    "    return shifted_input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model, Feature extractor, Tokenizer, Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=model_lang, task=task)\n",
    "\n",
    "# We only need to set the task id when the language is specified (i.e. in a multilingual setting)\n",
    "tokenizer.set_prefix_tokens(language=model_lang, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=model_lang, task=task)\n",
    "    \n",
    "# model\n",
    "# FlaxWhisperForConditionalGeneration uses the FlaxWhisperPreTrainedModel forward method,\n",
    "# overrides the __call__ special method\n",
    "# FlaxWhisperForConditionalGeneration -> module_class = FlaxWhisperForConditionalGenerationModule\n",
    "# FlaxWhisperForConditionalGeneration(FlaxWhisperPreTrainedModel)\n",
    "# FlaxWhisperPreTrainedModel -> module = self.module_class\n",
    "# FlaxWhisperPreTrainedModel -> __call__ -> self.module.apply\n",
    "# FlaxWhisperForConditionalGenerationModule -> __call__ -> self.model -> FlaxWhisperModule\n",
    "# input_shape: typing.Tuple[int] = (b, 80, 3000)\n",
    "model = FlaxWhisperForConditionalGeneration.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    seed=seed,\n",
    "    dtype=getattr(jnp, dtype)\n",
    ")\n",
    "\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")  \n",
    "if freeze_encoder:\n",
    "    model.freeze_encoder()\n",
    "    model.model.encoder.gradient_checkpointing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.load:Using the latest cached version of the module from /users/ujan/.cache/huggingface/modules/datasets_modules/datasets/mozilla-foundation--common_voice_11_0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0 (last modified on Thu Feb  9 17:53:09 2023) since it couldn't be found locally at mozilla-foundation/common_voice_11_0., or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.builder:Found cached dataset common_voice_11_0 (/users/ujan/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/hi/11.0.0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0)\n"
     ]
    }
   ],
   "source": [
    "common_voice = DatasetDict()\n",
    "common_voice[\"test\"] = load_dataset(data_dir, data_lang, split=\"test\", use_auth_token=True)\n",
    "\n",
    "# remove unused columns\n",
    "common_voice = common_voice.remove_columns(\n",
    "    [\n",
    "        \"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# select small dataset for testing\n",
    "if max_test_samples is not None:\n",
    "    common_voice[\"test\"] = common_voice[\"test\"].select(range(max_test_samples))\n",
    "\n",
    "# resample to 16kHz\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    }
   ],
   "source": [
    "# tokenizer and generation max length\n",
    "max_length = (\n",
    "    generation_max_length if generation_max_length is not None else model.config.max_length\n",
    ")\n",
    "\n",
    "# function to vectorize dataset\n",
    "# flax models need decoder_input_ids instead of labels\n",
    "# we need fixed length inputs for jitted functions\n",
    "# https://github.com/huggingface/transformers/blob/v4.29.1/src/transformers/models/whisper/feature_extraction_whisper.py#L254\n",
    "#if return_attention_mask:\n",
    "    # rescale from sample (48000) to feature (3000)\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    # 80 x 3000\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    labels = tokenizer(\n",
    "        batch[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "\n",
    "    # labels to compute loss\n",
    "    # 1 x generation length or max length\n",
    "    batch[\"labels\"] = labels[\"input_ids\"].flatten()\n",
    "    decoder_input_ids = shift_tokens_right(\n",
    "        labels[\"input_ids\"], model.config.pad_token_id, model.config.decoder_start_token_id\n",
    "    )\n",
    "    # decoder_input_ids to feed into the flax model\n",
    "    batch[\"decoder_input_ids\"] = np.asarray(decoder_input_ids).flatten()\n",
    "\n",
    "    # we need decoder_attention_mask so we can ignore pad tokens from loss\n",
    "    # completely masks decoder_input_ids\n",
    "    # leaves first pad token (after input ids) unmasked in labels\n",
    "    # need different mask for labels?\n",
    "    batch[\"decoder_attention_mask\"] = labels[\"attention_mask\"].flatten()\n",
    "\n",
    "    return batch\n",
    "\n",
    "# vectorize dataset\n",
    "# input_features, decoder_input_ids, decoder_attention_mask, labels\n",
    "common_voice = common_voice.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=common_voice.column_names[\"test\"],\n",
    "    desc=\"vectorize dataset\"\n",
    ") #, num_proc=2)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = common_voice[\"test\"]\n",
    "\n",
    "# eval steps in test_dataset\n",
    "eval_steps = math.ceil(len(common_voice[\"test\"]) / eval_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "cer = evaluate.load(\"cer\")\n",
    "wer = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    result = {}\n",
    "    predictions = processor.batch_decode(\n",
    "        preds,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    references = processor.batch_decode(\n",
    "        labels,\n",
    "        group_tokens=False,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    # compute cer, wer\n",
    "    result[\"cer\"] = cer.compute(predictions=predictions, references=references)\n",
    "    result[\"wer\"] = wer.compute(predictions=predictions, references=references)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer for state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create learning rate schedule\n",
    "warmup_fn = optax.linear_schedule(\n",
    "    init_value=0.0, end_value=learning_rate, transition_steps=warmup_steps\n",
    ")\n",
    "decay_fn = optax.linear_schedule(\n",
    "        init_value=learning_rate,\n",
    "        end_value=0,\n",
    "        transition_steps=train_steps - warmup_steps,\n",
    "    )\n",
    "linear_decay_lr_schedule_fn = optax.join_schedules(\n",
    "    schedules=[warmup_fn, decay_fn], boundaries=[warmup_steps]\n",
    ")\n",
    "\n",
    "# we use Optax's \"masking\" functionality to not apply weight decay\n",
    "# to bias and LayerNorm scale parameters. decay_mask_fn returns a\n",
    "# mask boolean with the same structure as the parameters.\n",
    "# the mask is True for parameters that should be decayed.\n",
    "def decay_mask_fn(params):\n",
    "    flat_params = traverse_util.flatten_dict(params)\n",
    "    # find out all LayerNorm parameters\n",
    "    layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n",
    "    layer_norm_named_params = {\n",
    "        layer[-2:]\n",
    "        for layer_norm_name in layer_norm_candidates\n",
    "        for layer in flat_params.keys()\n",
    "        if layer_norm_name in \"\".join(layer).lower()\n",
    "    }\n",
    "    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n",
    "    return traverse_util.unflatten_dict(flat_mask)\n",
    "\n",
    "# create adam optimizer\n",
    "adamw = optax.adamw(\n",
    "    learning_rate=linear_decay_lr_schedule_fn,\n",
    "    b1=adam_beta1,\n",
    "    b2=adam_beta2,\n",
    "    eps=adam_epsilon,\n",
    "    weight_decay=weight_decay,\n",
    "    mask=decay_mask_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State and rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(seed)\n",
    "rng, dropout_rng = jax.random.split(rng)\n",
    "\n",
    "# setup train state\n",
    "# FlaxWhisperForConditionalGenerationModule -> __call__ -> self.model -> FlaxWhisperModule\n",
    "state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label smoothed cross entropy\n",
    "def loss_fn(logits, labels, padding_mask, label_smoothing_factor=0.0):\n",
    "    \"\"\"\n",
    "    The label smoothing implementation is adapted from Flax's official example:\n",
    "    https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104\n",
    "    \"\"\"\n",
    "    vocab_size = logits.shape[-1]\n",
    "    confidence = 1.0 - label_smoothing_factor\n",
    "    low_confidence = (1.0 - confidence) / (vocab_size - 1)\n",
    "    normalizing_constant = -(\n",
    "        confidence * jnp.log(confidence) + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)\n",
    "    )\n",
    "    soft_labels = onehot(labels, vocab_size, on_value=confidence, off_value=low_confidence)\n",
    "\n",
    "    loss = optax.softmax_cross_entropy(logits, soft_labels)\n",
    "    loss = loss - normalizing_constant\n",
    "\n",
    "    # ignore padded tokens from loss\n",
    "    loss = loss * padding_mask\n",
    "    loss = loss.sum()\n",
    "    # what is num_labels?\n",
    "    num_labels = padding_mask.sum()\n",
    "    \n",
    "    return loss, num_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define eval fn\n",
    "def eval_step(params, batch, label_smoothing_factor=0.0):\n",
    "    \n",
    "    labels = batch.pop(\"labels\")\n",
    "    logits = model(**batch, params=params, train=False)[0]\n",
    "\n",
    "    loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
    "    num_labels = jax.lax.psum(num_labels, \"batch\")\n",
    "\n",
    "    # true loss = total loss / total samples\n",
    "    loss = jax.lax.psum(loss, \"batch\")\n",
    "    loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
    "\n",
    "    metrics = {\"loss\": loss}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "p_eval_step = jax.pmap(partial(eval_step, label_smoothing_factor=label_smoothing_factor), \"batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation functions\n",
    "\n",
    "def make_generation_config(supress_en=False):\n",
    "\n",
    "    generation_config = GenerationConfig.from_pretrained(model_name_or_path)\n",
    "    gen_dict = generation_config.to_dict()\n",
    "    # add attributes to genration_config\n",
    "    # generation_config does not have \"langauge\", but generate() tries to use it\n",
    "    # can be empty dict here since being set in generate_step\n",
    "    gen_dict[\"language\"] = LANG_TO_ID[model_lang]\n",
    "    if supress_en:\n",
    "        # en tokens to suppress from multilingual vocab\n",
    "        en_tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny.en\")  # change if loaded locally\n",
    "        suppress_en_list = []\n",
    "        for key in en_tokenizer.encoder.keys():\n",
    "            if key in tokenizer.encoder.keys() and key.isalpha():\n",
    "                suppress_en_list.append(key)\n",
    "        # supress english tokens\n",
    "        gen_dict['suppress_tokens'].extend(tokenizer.encode(suppress_en_list, add_special_tokens=False))\n",
    "\n",
    "    # reload with new attributes\n",
    "    generation_config = GenerationConfig.from_dict(gen_dict)\n",
    "\n",
    "    return generation_config\n",
    "\n",
    "\n",
    "# max_length defined after tokenizer\n",
    "num_beams = num_beams if num_beams is not None else model.config.num_beams\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "# generation config\n",
    "generation_config = make_generation_config(supress_en=False)\n",
    "\n",
    "# batch -> input_features, decoder_input_ids, decoder_attention_mask, labels\n",
    "def generate_step(params, batch):\n",
    "    model.params = params\n",
    "    output_ids = model.generate(\n",
    "        batch[\"input_features\"],\n",
    "        generation_config=generation_config,\n",
    "        task=task,\n",
    "        language=LANG_TO_ID[model_lang],  # set lang here\n",
    "        is_multilingual=True,\n",
    "        **gen_kwargs\n",
    "    )   \n",
    "    return output_ids.sequences\n",
    "\n",
    "\n",
    "p_generate_step = jax.pmap(generate_step, \"batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 state.params[<span style=\"color: #808000; text-decoration-color: #808000\">'model'</span>][<span style=\"color: #808000; text-decoration-color: #808000\">'decoder'</span>][<span style=\"color: #808000; text-decoration-color: #808000\">'layers'</span>][<span style=\"color: #808000; text-decoration-color: #808000\">'0'</span>][<span style=\"color: #808000; text-decoration-color: #808000\">'encoder_attn'</span>][<span style=\"color: #808000; text-decoration-color: #808000\">'k_proj'</span>][<span style=\"color: #808000; text-decoration-color: #808000\">'kernel'</span>].is_f     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/Users/ujan/jax/jax/_src/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">core.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">762</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">is_fully_replicated</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 759 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 760 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@property</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 761 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">is_fully_replicated</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 762 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> ConcretizationTypeError(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 763 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│     </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"The is_fully_replicated property was called on the JAX Tracer object {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span><span style=\"color: #808000; text-decoration-color: #808000\">}\"</span>)     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 764 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 765 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">on_device_size_in_bytes</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>):                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ConcretizationTypeError: </span>Abstract tracer value encountered where concrete value is expected: \n",
       "Traced<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ShapedArray</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">float32</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">])</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;with&lt;</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DynamicJaxprTrace</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">level</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">The is_fully_replicated property was called on the JAX Tracer object </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">Traced&lt;</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ShapedArray</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">float32</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">])</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;with&lt;</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DynamicJaxprTrace</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">level</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">This DynamicJaxprTracer was created on line </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/var/folders/by/rktr_w596p97pmt8_cbknvs80000gn/T/ipykernel_4657/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">570412194.py</span><span style=\"color: #000000; text-decoration-color: #000000\">:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;module</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 state.params[\u001b[33m'\u001b[0m\u001b[33mmodel\u001b[0m\u001b[33m'\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mdecoder\u001b[0m\u001b[33m'\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mlayers\u001b[0m\u001b[33m'\u001b[0m][\u001b[33m'\u001b[0m\u001b[33m0\u001b[0m\u001b[33m'\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mencoder_attn\u001b[0m\u001b[33m'\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mk_proj\u001b[0m\u001b[33m'\u001b[0m][\u001b[33m'\u001b[0m\u001b[33mkernel\u001b[0m\u001b[33m'\u001b[0m].is_f     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/Users/ujan/jax/jax/_src/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m762\u001b[0m in \u001b[92mis_fully_replicated\u001b[0m                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 759 \u001b[0m\u001b[2m  \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 760 \u001b[0m\u001b[2m  \u001b[0m\u001b[1;95m@property\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 761 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mis_fully_replicated\u001b[0m(\u001b[96mself\u001b[0m):                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 762 \u001b[2m│   \u001b[0m\u001b[94mraise\u001b[0m ConcretizationTypeError(\u001b[96mself\u001b[0m,                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 763 \u001b[0m\u001b[2m│     \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mThe is_fully_replicated property was called on the JAX Tracer object \u001b[0m\u001b[33m{\u001b[0m\u001b[96mself\u001b[0m\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 764 \u001b[0m\u001b[2m  \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 765 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mon_device_size_in_bytes\u001b[0m(\u001b[96mself\u001b[0m):                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mConcretizationTypeError: \u001b[0mAbstract tracer value encountered where concrete value is expected: \n",
       "Traced\u001b[1m<\u001b[0m\u001b[1;35mShapedArray\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfloat32\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m768\u001b[0m\u001b[39m,\u001b[0m\u001b[1;36m768\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m>with<\u001b[0m\u001b[1;35mDynamicJaxprTrace\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mlevel\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m/\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39mThe is_fully_replicated property was called on the JAX Tracer object \u001b[0m\n",
       "\u001b[39mTraced<\u001b[0m\u001b[1;35mShapedArray\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mfloat32\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m768\u001b[0m\u001b[39m,\u001b[0m\u001b[1;36m768\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m>with<\u001b[0m\u001b[1;35mDynamicJaxprTrace\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mlevel\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m/\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[39mThis DynamicJaxprTracer was created on line \u001b[0m\n",
       "\u001b[35m/var/folders/by/rktr_w596p97pmt8_cbknvs80000gn/T/ipykernel_4657/\u001b[0m\u001b[95m570412194.py\u001b[0m\u001b[39m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[39m<module\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n",
       "See \u001b[4;94mhttps://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state.params['model']['decoder']['layers']['0']['encoder_attn']['k_proj']['kernel'].is_fully_replicated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicate state on all devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = state.replicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init checkpointer\n",
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=max_to_keep, create=True)\n",
    "# checkpoint manager\n",
    "checkpoint_manager = orbax.checkpoint.CheckpointManager(\n",
    "    output_dir,\n",
    "    orbax_checkpointer,\n",
    "    options\n",
    ") \n",
    "\n",
    "#shardings = jax.tree_map(lambda x: x.sharding, state)\n",
    "#restore_args = checkpoint_utils.construct_restore_args(state, shardings)\n",
    "#restore_kwargs = {'restore_args': restore_args}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for debugging checkpointing\n",
    "\n",
    "#ckpt = {'state': state, 'config': model.config}\n",
    "#save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "#checkpoint_manager.save(2000, ckpt, save_kwargs={'save_args': save_args})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints found. restoring latest\n"
     ]
    }
   ],
   "source": [
    "# load from previous checkpoint\n",
    "if os.path.isdir(output_dir):\n",
    "    print('checkpoints found. restoring latest')\n",
    "    # get latest checkpoint\n",
    "    step = checkpoint_manager.latest_step()\n",
    "    restored = checkpoint_manager.restore(step)\n",
    "    #restored = checkpoint_manager.restore(step, restore_kwargs=restore_kwargs)\n",
    "    global_step = step\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"no checkpoint found. set overwrite_output_dir to train from scratch\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = state.replace(\n",
    "    step=step,\n",
    "    params=restored['model']['params'],  # model or state, automate this\n",
    "    tx=adamw,\n",
    "    opt_state=restored['model']['opt_state'],\n",
    "    dropout_rng=restored['model']['dropout_rng']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 80, 768)\n",
      "(1, 3, 80, 768)\n"
     ]
    }
   ],
   "source": [
    "# for debugging checkpointing\n",
    "\n",
    "#print(state.params['model']['encoder']['conv1']['kernel'].shape)\n",
    "#print(new_state.params['model']['encoder']['conv1']['kernel'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_state = jax_utils.replicate(new_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [08:49<00:00, 105.85s/it]\n",
      "100%|██████████| 5/5 [06:36<00:00, 78.42s/it]"
     ]
    }
   ],
   "source": [
    "eval_metrics = []\n",
    "eval_preds = []\n",
    "eval_labels = []\n",
    "result_dict = {}\n",
    "\n",
    "rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "eval_loader = data_loader(input_rng, test_dataset, eval_batch_size, shuffle=True)\n",
    "\n",
    "# eval progress bar\n",
    "eval_bar = tqdm(range(eval_steps), position=0)\n",
    "for batch in eval_loader:\n",
    "    labels = batch[\"labels\"]\n",
    "    \n",
    "    metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
    "        state.params, batch, min_device_batch=per_device_eval_batch_size)  # new_state\n",
    "    eval_metrics.append(metrics)\n",
    "\n",
    "    # generation\n",
    "    generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)  # new_state\n",
    "    eval_preds.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
    "    eval_labels.extend(labels)\n",
    "    \n",
    "    eval_bar.update(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss : 2.311755895614624\n",
      "cer : 0.2821387940841866\n",
      "wer : 0.6627218934911243\n"
     ]
    }
   ],
   "source": [
    "# normalize eval metrics\n",
    "eval_metrics = get_metrics(eval_metrics)\n",
    "eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n",
    "\n",
    "# cer, wer\n",
    "result = compute_metrics(eval_preds, eval_labels)\n",
    "eval_metrics.update(result)\n",
    "                \n",
    "# collect results together\n",
    "result_dict['eval_loss'] = eval_metrics['loss']\n",
    "result_dict['cer'] = eval_metrics['cer']\n",
    "result_dict['wer'] = eval_metrics['wer']\n",
    "\n",
    "# write to terminal\n",
    "for key, val in result_dict.items():\n",
    "    print('{} : {}'.format(key, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlaxWhisperForConditionalGeneration.from_pretrained(\n",
    "    'openai/whisper-medium',\n",
    "    seed=seed,\n",
    "    dtype=getattr(jnp, dtype)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlaxWhisperForConditionalGeneration.from_pretrained(\n",
    "    '/home/ujan/speech-processing/models/whisper/checkpoint-27000',\n",
    "    seed=seed,\n",
    "    from_pt=True,\n",
    "    dtype=getattr(jnp, dtype)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d381011b8b2da54efbee7772c701c6e43d8e200fe4417a4d92316b7d5561dbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
