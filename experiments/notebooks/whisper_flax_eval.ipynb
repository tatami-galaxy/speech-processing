{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "| Size   | Layers | Width | Heads | Parameters | English-only                                         | Multilingual                                      |\n",
    "|--------|--------|-------|-------|------------|------------------------------------------------------|---------------------------------------------------|\n",
    "| tiny   | 4      | 384   | 6     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny.)  |\n",
    "| base   | 6      | 512   | 8     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)   |\n",
    "| small  | 12     | 768   | 12    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)  |\n",
    "| medium | 24     | 1024  | 16    | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium) |\n",
    "| large  | 32     | 1280  | 20    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)  |\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujan/opt/anaconda3/envs/asr_jax/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import dirname, abspath\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "import time, math\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import jax_utils, traverse_util\n",
    "from flax.jax_utils import unreplicate\n",
    "import orbax\n",
    "from flax.training import (\n",
    "    train_state,\n",
    "    orbax_utils\n",
    ")\n",
    "from flax.training.common_utils import (\n",
    "    onehot,\n",
    "    shard,\n",
    "    shard_prng_key,\n",
    "    get_metrics\n",
    ")\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    GenerationConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    FlaxWhisperForConditionalGeneration,\n",
    ")\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    is_tensorboard_available,\n",
    ")\n",
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "import datasets\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset,\n",
    "    DatasetDict,\n",
    "    Audio\n",
    ")\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# sending telemetry\n",
    "# tracking the example usage helps us better allocate resources to maintain them\n",
    "# the information sent is the one passed as arguments along with your Python/PyTorch versions\n",
    "send_example_telemetry(\"run_summarization\", framework=\"flax\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 # get root directory</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 # only works if run from within 'speech-processing' directory</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 # else replace `root` with correct path</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 4 root = abspath(<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__file__</span>)                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> root.split(<span style=\"color: #808000; text-decoration-color: #808000\">'/'</span>)[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] != <span style=\"color: #808000; text-decoration-color: #808000\">'speech-processing'</span>:                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>root = dirname(root)                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'__file__'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m4\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 1 \u001b[0m\u001b[2m# get root directory\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 2 \u001b[0m\u001b[2m# only works if run from within 'speech-processing' directory\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[2m# else replace `root` with correct path\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 4 root = abspath(\u001b[91m__file__\u001b[0m)                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[94mwhile\u001b[0m root.split(\u001b[33m'\u001b[0m\u001b[33m/\u001b[0m\u001b[33m'\u001b[0m)[-\u001b[94m1\u001b[0m] != \u001b[33m'\u001b[0m\u001b[33mspeech-processing\u001b[0m\u001b[33m'\u001b[0m:                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   \u001b[0mroot = dirname(root)                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'__file__'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get root directory\n",
    "# only works if run from within 'speech-processing' directory\n",
    "# else replace `root` with correct path\n",
    "root = abspath(__file__)\n",
    "while root.split('/')[-1] != 'speech-processing':\n",
    "    root = dirname(root)\n",
    "\n",
    "# constants\n",
    "LANG_TO_ID = {\"hindi\" : \"<|hi|>\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# model\n",
    "model_name_or_path = 'openai/whisper-small'\n",
    "model_lang = 'hindi'\n",
    "task = 'transcribe'\n",
    "dtype = 'float32'  # float16\n",
    "\n",
    "# data\n",
    "data_dir = 'mozilla-foundation/common_voice_11_0'\n",
    "data_lang = 'hi'\n",
    "max_train_samples = None\n",
    "max_test_samples = None\n",
    "\n",
    "# flags\n",
    "freeze_encoder = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    dropout_rng: jnp.ndarray\n",
    "\n",
    "    def replicate(self):\n",
    "        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n",
    "    \"\"\"\n",
    "    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n",
    "    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        batch_idx = jax.random.permutation(rng, len(dataset))\n",
    "        batch_idx = np.asarray(batch_idx)\n",
    "    else:\n",
    "        batch_idx = np.arange(len(dataset))\n",
    "\n",
    "    if drop_last:\n",
    "        steps_per_epoch = len(dataset) // batch_size\n",
    "        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
    "    else:\n",
    "        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
    "        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
    "\n",
    "    for idx in batch_idx:\n",
    "        batch = dataset[idx]\n",
    "        batch = {k: np.array(v) for k, v in batch.items()}\n",
    "\n",
    "        yield batch\n",
    "\n",
    "\n",
    "# in Flax, for seq2seq models we need to pass `decoder_input_ids`\n",
    "# as the Flax models don't accept `labels`, we need to prepare the decoder_input_ids here\n",
    "# `shift_tokens_right` function\n",
    "# copied from transformers.models.bart.modeling_flax_bart.shift_tokens_right\n",
    "def shift_tokens_right(input_ids: np.array, pad_token_id: int, decoder_start_token_id: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "\n",
    "    shifted_input_ids = np.zeros_like(input_ids)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1]\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "    shifted_input_ids = np.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)\n",
    "\n",
    "    return shifted_input_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model, Feature extractor, Tokenizer, Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=model_lang, task=task)\n",
    "\n",
    "# We only need to set the task id when the language is specified (i.e. in a multilingual setting)\n",
    "tokenizer.set_prefix_tokens(language=model_lang, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=model_lang, task=task)\n",
    "    \n",
    "# model\n",
    "# FlaxWhisperForConditionalGeneration uses the FlaxWhisperPreTrainedModel forward method,\n",
    "# overrides the __call__ special method\n",
    "# FlaxWhisperForConditionalGeneration -> module_class = FlaxWhisperForConditionalGenerationModule\n",
    "# FlaxWhisperForConditionalGeneration(FlaxWhisperPreTrainedModel)\n",
    "# FlaxWhisperPreTrainedModel -> module = self.module_class\n",
    "# FlaxWhisperPreTrainedModel -> __call__ -> self.module.apply\n",
    "# FlaxWhisperForConditionalGenerationModule -> __call__ -> self.model -> FlaxWhisperModule\n",
    "# input_shape: typing.Tuple[int] = (b, 80, 3000)\n",
    "model = FlaxWhisperForConditionalGeneration.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    seed=seed,\n",
    "    dtype=getattr(jnp, dtype)\n",
    ")\n",
    "\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")  \n",
    "if freeze_encoder:\n",
    "    model.freeze_encoder()\n",
    "    model.model.encoder.gradient_checkpointing = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(data_dir, data_lang, split=\"train+validation\", use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(data_dir, data_lang, split=\"test\", use_auth_token=True)\n",
    "\n",
    "# remove unused columns\n",
    "common_voice = common_voice.remove_columns(\n",
    "    [\n",
    "        \"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# select small dataset for testing\n",
    "if max_train_samples is not None:\n",
    "    common_voice[\"train\"] = common_voice[\"train\"].select(range(max_train_samples))\n",
    "\n",
    "if max_test_samples is not None:\n",
    "    common_voice[\"test\"] = common_voice[\"test\"].select(range(max_test_samples))\n",
    "\n",
    "# resample to 16kHz\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer and generation max length\n",
    "max_length = (\n",
    "    args.generation_max_length if args.generation_max_length is not None else model.config.max_length\n",
    ")\n",
    "\n",
    "# function to vectorize dataset\n",
    "# flax models need decoder_input_ids instead of labels\n",
    "# we need fixed length inputs for jitted functions\n",
    "# https://github.com/huggingface/transformers/blob/v4.29.1/src/transformers/models/whisper/feature_extraction_whisper.py#L254\n",
    "#if return_attention_mask:\n",
    "    # rescale from sample (48000) to feature (3000)\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    # 80 x 3000\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    labels = tokenizer(\n",
    "        batch[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "\n",
    "    # labels to compute loss\n",
    "    # 1 x generation length or max length\n",
    "    batch[\"labels\"] = labels[\"input_ids\"].flatten()\n",
    "    decoder_input_ids = shift_tokens_right(\n",
    "        labels[\"input_ids\"], model.config.pad_token_id, model.config.decoder_start_token_id\n",
    "    )\n",
    "    # decoder_input_ids to feed into the flax model\n",
    "    batch[\"decoder_input_ids\"] = np.asarray(decoder_input_ids).flatten()\n",
    "\n",
    "    # we need decoder_attention_mask so we can ignore pad tokens from loss\n",
    "    # completely masks decoder_input_ids\n",
    "    # leaves first pad token (after input ids) unmasked in labels\n",
    "    # need different mask for labels?\n",
    "    batch[\"decoder_attention_mask\"] = labels[\"attention_mask\"].flatten()\n",
    "\n",
    "    return batch\n",
    "\n",
    "# vectorize dataset\n",
    "# input_features, decoder_input_ids, decoder_attention_mask, labels\n",
    "common_voice = common_voice.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=common_voice.column_names[\"train\"],\n",
    "    desc=\"vectorize dataset\"\n",
    ") #, num_proc=2)\n",
    "\n",
    "# train and test datasets\n",
    "train_dataset = common_voice[\"train\"]\n",
    "test_dataset = common_voice[\"test\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr_jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d381011b8b2da54efbee7772c701c6e43d8e200fe4417a4d92316b7d5561dbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
