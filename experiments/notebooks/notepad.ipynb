{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bfe8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.fx import symbolic_trace\n",
    "import torch.fx as fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09a192d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.Parameter(torch.rand(3, 4))\n",
    "        self.linear = torch.nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x + self.param).clamp(min=0.0, max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf2865f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = MyModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d33331d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_traced : torch.fx.GraphModule = symbolic_trace(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34be375b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %x : [#users=1] = placeholder[target=x]\n",
      "    %param : [#users=1] = get_attr[target=param]\n",
      "    %add : [#users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {})\n",
      "    %linear : [#users=1] = call_module[target=linear](args = (%add,), kwargs = {})\n",
      "    %clamp : [#users=1] = call_method[target=clamp](args = (%linear,), kwargs = {min: 0.0, max: 1.0})\n",
      "    return clamp\n"
     ]
    }
   ],
   "source": [
    "print(symbolic_traced.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dde07baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    param = self.param\n",
      "    add = x + param;  x = param = None\n",
      "    linear = self.linear(add);  add = None\n",
      "    clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\n",
      "    return clamp\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(symbolic_traced.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105435a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7e28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(m: nn.Module,\n",
    "              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n",
    "    # Step 1: Acquire a Graph representing the code in `m`\n",
    "\n",
    "    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n",
    "    # fx.Tracer.trace and constructing a GraphModule. We'll\n",
    "    # split that out in our transform to allow the caller to\n",
    "    # customize tracing behavior.\n",
    "    graph : torch.fx.Graph = tracer_class().trace(m)\n",
    "\n",
    "    # Step 2: Modify this Graph or create a new one\n",
    "    graph = ...\n",
    "\n",
    "    # Step 3: Construct a Module to return\n",
    "    return torch.fx.GraphModule(m, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(m : nn.Module) -> nn.Module:\n",
    "    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n",
    "\n",
    "    # Modify gm.graph\n",
    "    # <...>\n",
    "\n",
    "    # Recompile the forward() method of `gm` from its Graph\n",
    "    gm.recompile()\n",
    "\n",
    "    return gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea3a3aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name           target                                                   args                kwargs\n",
      "-------------  -------------  -------------------------------------------------------  ------------------  -----------\n",
      "placeholder    x              x                                                        ()                  {}\n",
      "get_attr       linear_weight  linear.weight                                            ()                  {}\n",
      "call_function  add            <built-in function add>                                  (x, linear_weight)  {}\n",
      "call_module    linear         linear                                                   (add,)              {}\n",
      "call_method    relu           relu                                                     (linear,)           {}\n",
      "call_function  sum_1          <built-in method sum of type object at 0x7f55f11bd540>   (relu,)             {'dim': -1}\n",
      "call_function  topk           <built-in method topk of type object at 0x7f55f11bd540>  (sum_1, 3)          {}\n",
      "output         output         output                                                   (topk,)             {}\n"
     ]
    }
   ],
   "source": [
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.param = torch.nn.Parameter(torch.rand(3, 4))\n",
    "        self.linear = torch.nn.Linear(4, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.topk(torch.sum(\n",
    "            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n",
    "\n",
    "m = MyModule()\n",
    "gm = torch.fx.symbolic_trace(m)\n",
    "\n",
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58937d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample module\n",
    "class M(torch.nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        return torch.add(x, y)\n",
    "\n",
    "def transform(m: torch.nn.Module,\n",
    "              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n",
    "    graph : fx.Graph = tracer_class().trace(m)\n",
    "    # FX represents its Graph as an ordered list of\n",
    "    # nodes, so we can iterate through them.\n",
    "    for node in graph.nodes:\n",
    "        # Checks if we're calling a function (i.e:\n",
    "        # torch.add)\n",
    "        if node.op == 'call_function':\n",
    "            # The target attribute is the function\n",
    "            # that call_function calls.\n",
    "            if node.target == torch.add:\n",
    "                node.target = torch.mul\n",
    "\n",
    "    graph.lint() # Does some checks to make sure the\n",
    "                 # Graph is well-formed.\n",
    "\n",
    "    return fx.GraphModule(m, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34395112",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = M()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2e97340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "619f953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = transform(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d6ac4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c5483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a5636c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TraceError",
     "evalue": "symbolically traced variables cannot be used as inputs to control flow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m symbolic_traced : torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule \u001b[38;5;241m=\u001b[39m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:1109\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03mSymbolic tracing API\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03m    GraphModule: a Module created from the recorded operations from ``root``.\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m tracer \u001b[38;5;241m=\u001b[39m Tracer()\n\u001b[0;32m-> 1109\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1111\u001b[0m     root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1112\u001b[0m )\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:778\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    772\u001b[0m             _autowrap_check(\n\u001b[1;32m    773\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    774\u001b[0m             )\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    776\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    777\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 778\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    779\u001b[0m             {},\n\u001b[1;32m    780\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    781\u001b[0m         )\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1194\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.forward\u001b[0;34m(self, input_features, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1190\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1191\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1192\u001b[0m         )\n\u001b[0;32m-> 1194\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1209\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1211\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:756\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _orig_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    751\u001b[0m _autowrap_check(\n\u001b[1;32m    752\u001b[0m     patcher,\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    755\u001b[0m )\n\u001b[0;32m--> 756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:467\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_stack[_scope\u001b[38;5;241m.\u001b[39mmodule_path] \u001b[38;5;241m=\u001b[39m _scope\u001b[38;5;241m.\u001b[39mmodule_type\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 467\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:749\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1055\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, input_features, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1048\u001b[0m         input_features,\n\u001b[1;32m   1049\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1053\u001b[0m     )\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[0;32m-> 1055\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1056\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1057\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1058\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1059\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1060\u001b[0m     )\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/proxy.py:413\u001b[0m, in \u001b[0;36mProxy.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m'\u001b[39m, assert_fn, (\u001b[38;5;28mself\u001b[39m,), {})\n\u001b[1;32m    411\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/proxy.py:276\u001b[0m, in \u001b[0;36mTracerBase.to_bool\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_bool\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called when a proxy object is being converted to a boolean, such as\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    when used in control flow.  Normally we don't know what to do because\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    we don't know the value of the proxy, but a custom tracer can attach more\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    information to the graph node using create_node and can choose to return a value.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraceError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbolically traced variables cannot be used as inputs to control flow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTraceError\u001b[0m: symbolically traced variables cannot be used as inputs to control flow"
     ]
    }
   ],
   "source": [
    "symbolic_traced : torch.fx.GraphModule = symbolic_trace(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bba676c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88397f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32365b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f80688be110>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  # This is all you need to use both PyTorch and TorchScript!\n",
    "print(torch.__version__)\n",
    "torch.manual_seed(191009)  # set the seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "881a14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionGate(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x\n",
    "\n",
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.dg = MyDecisionGate()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n",
    "        return new_h, new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "523a1d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.2155, -0.0853,  0.3186,  0.6389],\n",
      "        [ 0.5300,  0.5265,  0.6738,  0.5114],\n",
      "        [ 0.3993,  0.5134, -0.2223,  0.4750]], grad_fn=<TanhBackward0>), tensor([[ 0.2155, -0.0853,  0.3186,  0.6389],\n",
      "        [ 0.5300,  0.5265,  0.6738,  0.5114],\n",
      "        [ 0.3993,  0.5134, -0.2223,  0.4750]], grad_fn=<TanhBackward0>))\n"
     ]
    }
   ],
   "source": [
    "my_cell = MyCell()\n",
    "x = torch.rand(3, 4)\n",
    "h = torch.rand(3, 4)\n",
    "print(my_cell(x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64fcc484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCell(\n",
      "  original_name=MyCell\n",
      "  (linear): Linear(original_name=Linear)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0123,  0.0657,  0.7297,  0.6066],\n",
       "         [-0.2156,  0.7301,  0.3347,  0.3326],\n",
       "         [-0.2860, -0.1094,  0.4924,  0.5432]], grad_fn=<TanhBackward0>),\n",
       " tensor([[ 0.0123,  0.0657,  0.7297,  0.6066],\n",
       "         [-0.2156,  0.7301,  0.3347,  0.3326],\n",
       "         [-0.2860, -0.1094,  0.4924,  0.5432]], grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell()\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "print(traced_cell)\n",
    "traced_cell(x, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc912fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self.1 : __torch__.MyCell,\n",
      "      %x : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu),\n",
      "      %h : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu)):\n",
      "  %linear : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear\"](%self.1)\n",
      "  %20 : Tensor = prim::CallMethod[name=\"forward\"](%linear, %x)\n",
      "  %11 : int = prim::Constant[value=1]() # /var/folders/by/rktr_w596p97pmt8_cbknvs80000gn/T/ipykernel_4017/260609686.py:7:0\n",
      "  %12 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::add(%20, %h, %11) # /var/folders/by/rktr_w596p97pmt8_cbknvs80000gn/T/ipykernel_4017/260609686.py:7:0\n",
      "  %13 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::tanh(%12) # /var/folders/by/rktr_w596p97pmt8_cbknvs80000gn/T/ipykernel_4017/260609686.py:7:0\n",
      "  %14 : (Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu), Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu)) = prim::TupleConstruct(%13, %13)\n",
      "  return (%14)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_cell.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a04c6e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  linear = self.linear\n",
      "  _0 = torch.tanh(torch.add((linear).forward(x, ), h))\n",
      "  return (_0, _0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(traced_cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "307edded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.0123,  0.0657,  0.7297,  0.6066],\n",
      "        [-0.2156,  0.7301,  0.3347,  0.3326],\n",
      "        [-0.2860, -0.1094,  0.4924,  0.5432]], grad_fn=<TanhBackward0>), tensor([[ 0.0123,  0.0657,  0.7297,  0.6066],\n",
      "        [-0.2156,  0.7301,  0.3347,  0.3326],\n",
      "        [-0.2860, -0.1094,  0.4924,  0.5432]], grad_fn=<TanhBackward0>))\n",
      "(tensor([[ 0.0123,  0.0657,  0.7297,  0.6066],\n",
      "        [-0.2156,  0.7301,  0.3347,  0.3326],\n",
      "        [-0.2860, -0.1094,  0.4924,  0.5432]],\n",
      "       grad_fn=<DifferentiableGraphBackward>), tensor([[ 0.0123,  0.0657,  0.7297,  0.6066],\n",
      "        [-0.2156,  0.7301,  0.3347,  0.3326],\n",
      "        [-0.2860, -0.1094,  0.4924,  0.5432]],\n",
      "       grad_fn=<DifferentiableGraphBackward>))\n"
     ]
    }
   ],
   "source": [
    "print(my_cell(x, h))\n",
    "print(traced_cell(x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e1e6569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    argument_1: Tensor) -> Tensor:\n",
      "  return torch.neg(argument_1)\n",
      "\n",
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  dg = self.dg\n",
      "  linear = self.linear\n",
      "  _0 = torch.add((dg).forward((linear).forward(x, ), ), h)\n",
      "  _1 = torch.tanh(_0)\n",
      "  return (_1, _1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/by/rktr_w596p97pmt8_cbknvs80000gn/T/ipykernel_4017/4234398751.py:3: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.sum() > 0:\n"
     ]
    }
   ],
   "source": [
    "class MyDecisionGate(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x\n",
    "\n",
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self, dg):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.dg = dg\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell(MyDecisionGate())\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))  # control flow erased\n",
    "\n",
    "print(traced_cell.dg.code)\n",
    "print(traced_cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "074587b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    x: Tensor) -> Tensor:\n",
      "  if bool(torch.gt(torch.sum(x), 0)):\n",
      "    _0 = x\n",
      "  else:\n",
      "    _0 = torch.neg(x)\n",
      "  return _0\n",
      "\n",
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  dg = self.dg\n",
      "  linear = self.linear\n",
      "  _0 = torch.add((dg).forward((linear).forward(x, ), ), h)\n",
      "  new_h = torch.tanh(_0)\n",
      "  return (new_h, new_h)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scripted_gate = torch.jit.script(MyDecisionGate())\n",
    "\n",
    "my_cell = MyCell(scripted_gate)\n",
    "scripted_cell = torch.jit.script(my_cell)\n",
    "\n",
    "print(scripted_gate.code)\n",
    "print(scripted_cell.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "413c482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.1185, 0.8707, 0.5898, 0.7261],\n",
      "        [0.4802, 0.2835, 0.4302, 0.9021],\n",
      "        [0.4490, 0.9199, 0.6838, 0.6419]], grad_fn=<TanhBackward0>), tensor([[0.1185, 0.8707, 0.5898, 0.7261],\n",
      "        [0.4802, 0.2835, 0.4302, 0.9021],\n",
      "        [0.4490, 0.9199, 0.6838, 0.6419]], grad_fn=<TanhBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# New inputs\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "print(scripted_cell(x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f8955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f182f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd662fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers.utils.fx import symbolic_trace\n",
    "from transformers import AutoConfig, AutoFeatureExtractor, AutoTokenizer, AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.fx import symbolic_trace\n",
    "import torch.fx as fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de1f6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'openai/whisper-small'\n",
    "data_dir = 'mozilla-foundation/common_voice_11_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93fa02cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset from mozilla-foundation/common_voice_11_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 10581it [00:00, 19630.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "print('loading dataset from {}'.format(data_dir))\n",
    "\n",
    "raw_datasets = load_dataset(data_dir, \"zh-CN\", split=\"test\", streaming=True)\n",
    "text_column_name = 'sentence'\n",
    "\n",
    "\n",
    "# model, tokenizer, feature extractor, processor\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    #cache_dir=args.cache_dir,\n",
    "    #revision=args.model_revision,\n",
    "    #use_auth_token=True if args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "model_config.update({\"forced_decoder_ids\": [], \"suppress_tokens\": []})\n",
    "\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    #cache_dir=args.cache_dir,\n",
    "    #revision=args.model_revision,\n",
    "    #use_auth_token=True if args.use_auth_token else None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    #cache_dir=args.cache_dir,\n",
    "    #use_fast=model_args.use_fast_tokenizer,\n",
    "    #revision=model_args.model_revision,\n",
    "    #use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "tokenizer.set_prefix_tokens(language='chinese', task='transcribe')\n",
    "\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=model_config,\n",
    "    #cache_dir=args.cache_dir,\n",
    "    #revision=args.model_revision,\n",
    "    #use_auth_token=True if args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "    \n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language='chinese', task='transcribe')\n",
    "\n",
    "\n",
    "dataset = raw_datasets\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "sample = next(iter(dataset))\n",
    "\n",
    "\n",
    "inputs = processor(\n",
    "    sample['audio'][\"array\"],\n",
    "    sampling_rate=feature_extractor.sampling_rate,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\")\n",
    "                \n",
    "input_features = inputs.input_features\n",
    "attention_mask = inputs.attention_mask\n",
    "decoder_input_ids = torch.tensor([model.config.decoder_start_token_id]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b1377a8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Model WhisperForConditionalGeneration is not supported yet, supported models: AlbertForMaskedLM, AlbertForMultipleChoice, AlbertForPreTraining, AlbertForQuestionAnswering, AlbertForSequenceClassification, AlbertForTokenClassification, AlbertModel, AltCLIPModel, AltCLIPTextModel, AltCLIPVisionModel, BartForCausalLM, BartForConditionalGeneration, BartForQuestionAnswering, BartForSequenceClassification, BartModel, BertForMaskedLM, BertForMultipleChoice, BertForNextSentencePrediction, BertForPreTraining, BertForQuestionAnswering, BertForSequenceClassification, BertForTokenClassification, BertLMHeadModel, BertModel, BlenderbotForCausalLM, BlenderbotForConditionalGeneration, BlenderbotModel, BlenderbotSmallForCausalLM, BlenderbotSmallForConditionalGeneration, BlenderbotSmallModel, BloomForCausalLM, BloomForQuestionAnswering, BloomForSequenceClassification, BloomForTokenClassification, BloomModel, CLIPModel, CLIPTextModel, CLIPTextModelWithProjection, CLIPVisionModel, CLIPVisionModelWithProjection, ConvNextBackbone, ConvNextForImageClassification, ConvNextModel, DebertaForMaskedLM, DebertaForQuestionAnswering, DebertaForSequenceClassification, DebertaForTokenClassification, DebertaModel, DebertaV2ForMaskedLM, DebertaV2ForMultipleChoice, DebertaV2ForQuestionAnswering, DebertaV2ForSequenceClassification, DebertaV2ForTokenClassification, DebertaV2Model, DistilBertForMaskedLM, DistilBertForMultipleChoice, DistilBertForQuestionAnswering, DistilBertForSequenceClassification, DistilBertForTokenClassification, DistilBertModel, DonutSwinModel, ElectraForCausalLM, ElectraForMaskedLM, ElectraForMultipleChoice, ElectraForPreTraining, ElectraForQuestionAnswering, ElectraForSequenceClassification, ElectraForTokenClassification, ElectraModel, GPT2DoubleHeadsModel, GPT2ForQuestionAnswering, GPT2ForSequenceClassification, GPT2ForTokenClassification, GPT2LMHeadModel, GPT2Model, GPTJForCausalLM, GPTJForQuestionAnswering, GPTJForSequenceClassification, GPTJModel, GPTNeoForCausalLM, GPTNeoForQuestionAnswering, GPTNeoForSequenceClassification, GPTNeoForTokenClassification, GPTNeoModel, GitVisionModel, HubertForCTC, HubertForSequenceClassification, HubertModel, LayoutLMForMaskedLM, LayoutLMForQuestionAnswering, LayoutLMForSequenceClassification, LayoutLMForTokenClassification, LayoutLMModel, LxmertForPreTraining, LxmertForQuestionAnswering, LxmertModel, M2M100ForConditionalGeneration, M2M100Model, MBartForCausalLM, MBartForConditionalGeneration, MBartForQuestionAnswering, MBartForSequenceClassification, MBartModel, MT5ForConditionalGeneration, MT5ForQuestionAnswering, MT5Model, MarianForCausalLM, MarianMTModel, MarianModel, MegatronBertForCausalLM, MegatronBertForMaskedLM, MegatronBertForMultipleChoice, MegatronBertForNextSentencePrediction, MegatronBertForPreTraining, MegatronBertForQuestionAnswering, MegatronBertForSequenceClassification, MegatronBertForTokenClassification, MegatronBertModel, MobileBertForMaskedLM, MobileBertForMultipleChoice, MobileBertForNextSentencePrediction, MobileBertForPreTraining, MobileBertForQuestionAnswering, MobileBertForSequenceClassification, MobileBertForTokenClassification, MobileBertModel, NezhaForMaskedLM, NezhaForMultipleChoice, NezhaForNextSentencePrediction, NezhaForPreTraining, NezhaForQuestionAnswering, NezhaForSequenceClassification, NezhaForTokenClassification, NezhaModel, OPTForCausalLM, OPTForQuestionAnswering, OPTForSequenceClassification, OPTModel, PLBartForCausalLM, PLBartForConditionalGeneration, PLBartForSequenceClassification, PLBartModel, PeftModelForCausalLM, PeftModelForSeq2SeqLM, PegasusForCausalLM, PegasusForConditionalGeneration, PegasusModel, ResNetBackbone, ResNetForImageClassification, ResNetModel, RobertaForCausalLM, RobertaForMaskedLM, RobertaForMultipleChoice, RobertaForQuestionAnswering, RobertaForSequenceClassification, RobertaForTokenClassification, RobertaModel, SegformerForImageClassification, SegformerForSemanticSegmentation, SegformerModel, Speech2Text2Decoder, Speech2Text2ForCausalLM, Speech2TextForConditionalGeneration, Speech2TextModel, SwinBackbone, SwinForImageClassification, SwinForMaskedImageModeling, SwinModel, T5ForConditionalGeneration, T5ForQuestionAnswering, T5Model, TrOCRDecoder, TrOCRForCausalLM, ViTForImageClassification, ViTForMaskedImageModeling, ViTModel, Wav2Vec2ForCTC, Wav2Vec2ForMaskedLM, Wav2Vec2ForPreTraining, Wav2Vec2ForSequenceClassification, Wav2Vec2Model, XGLMForCausalLM, XGLMModel",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_features\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecoder_input_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/utils/fx.py:1246\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(model, input_names, disable_check, tracer_cls)\u001b[0m\n\u001b[1;32m   1243\u001b[0m concrete_args \u001b[38;5;241m=\u001b[39m get_concrete_args(model, input_names)\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m disable_check:\n\u001b[0;32m-> 1246\u001b[0m     \u001b[43mcheck_if_model_is_supported\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;66;03m# Tracing.\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m tracer \u001b[38;5;241m=\u001b[39m tracer_cls()\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/utils/fx.py:1204\u001b[0m, in \u001b[0;36mcheck_if_model_is_supported\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _SUPPORTED_MODELS:\n\u001b[1;32m   1203\u001b[0m     supported_model_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_SUPPORTED_MODELS)\n\u001b[0;32m-> 1204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1205\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported yet, supported models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_model_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1206\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Model WhisperForConditionalGeneration is not supported yet, supported models: AlbertForMaskedLM, AlbertForMultipleChoice, AlbertForPreTraining, AlbertForQuestionAnswering, AlbertForSequenceClassification, AlbertForTokenClassification, AlbertModel, AltCLIPModel, AltCLIPTextModel, AltCLIPVisionModel, BartForCausalLM, BartForConditionalGeneration, BartForQuestionAnswering, BartForSequenceClassification, BartModel, BertForMaskedLM, BertForMultipleChoice, BertForNextSentencePrediction, BertForPreTraining, BertForQuestionAnswering, BertForSequenceClassification, BertForTokenClassification, BertLMHeadModel, BertModel, BlenderbotForCausalLM, BlenderbotForConditionalGeneration, BlenderbotModel, BlenderbotSmallForCausalLM, BlenderbotSmallForConditionalGeneration, BlenderbotSmallModel, BloomForCausalLM, BloomForQuestionAnswering, BloomForSequenceClassification, BloomForTokenClassification, BloomModel, CLIPModel, CLIPTextModel, CLIPTextModelWithProjection, CLIPVisionModel, CLIPVisionModelWithProjection, ConvNextBackbone, ConvNextForImageClassification, ConvNextModel, DebertaForMaskedLM, DebertaForQuestionAnswering, DebertaForSequenceClassification, DebertaForTokenClassification, DebertaModel, DebertaV2ForMaskedLM, DebertaV2ForMultipleChoice, DebertaV2ForQuestionAnswering, DebertaV2ForSequenceClassification, DebertaV2ForTokenClassification, DebertaV2Model, DistilBertForMaskedLM, DistilBertForMultipleChoice, DistilBertForQuestionAnswering, DistilBertForSequenceClassification, DistilBertForTokenClassification, DistilBertModel, DonutSwinModel, ElectraForCausalLM, ElectraForMaskedLM, ElectraForMultipleChoice, ElectraForPreTraining, ElectraForQuestionAnswering, ElectraForSequenceClassification, ElectraForTokenClassification, ElectraModel, GPT2DoubleHeadsModel, GPT2ForQuestionAnswering, GPT2ForSequenceClassification, GPT2ForTokenClassification, GPT2LMHeadModel, GPT2Model, GPTJForCausalLM, GPTJForQuestionAnswering, GPTJForSequenceClassification, GPTJModel, GPTNeoForCausalLM, GPTNeoForQuestionAnswering, GPTNeoForSequenceClassification, GPTNeoForTokenClassification, GPTNeoModel, GitVisionModel, HubertForCTC, HubertForSequenceClassification, HubertModel, LayoutLMForMaskedLM, LayoutLMForQuestionAnswering, LayoutLMForSequenceClassification, LayoutLMForTokenClassification, LayoutLMModel, LxmertForPreTraining, LxmertForQuestionAnswering, LxmertModel, M2M100ForConditionalGeneration, M2M100Model, MBartForCausalLM, MBartForConditionalGeneration, MBartForQuestionAnswering, MBartForSequenceClassification, MBartModel, MT5ForConditionalGeneration, MT5ForQuestionAnswering, MT5Model, MarianForCausalLM, MarianMTModel, MarianModel, MegatronBertForCausalLM, MegatronBertForMaskedLM, MegatronBertForMultipleChoice, MegatronBertForNextSentencePrediction, MegatronBertForPreTraining, MegatronBertForQuestionAnswering, MegatronBertForSequenceClassification, MegatronBertForTokenClassification, MegatronBertModel, MobileBertForMaskedLM, MobileBertForMultipleChoice, MobileBertForNextSentencePrediction, MobileBertForPreTraining, MobileBertForQuestionAnswering, MobileBertForSequenceClassification, MobileBertForTokenClassification, MobileBertModel, NezhaForMaskedLM, NezhaForMultipleChoice, NezhaForNextSentencePrediction, NezhaForPreTraining, NezhaForQuestionAnswering, NezhaForSequenceClassification, NezhaForTokenClassification, NezhaModel, OPTForCausalLM, OPTForQuestionAnswering, OPTForSequenceClassification, OPTModel, PLBartForCausalLM, PLBartForConditionalGeneration, PLBartForSequenceClassification, PLBartModel, PeftModelForCausalLM, PeftModelForSeq2SeqLM, PegasusForCausalLM, PegasusForConditionalGeneration, PegasusModel, ResNetBackbone, ResNetForImageClassification, ResNetModel, RobertaForCausalLM, RobertaForMaskedLM, RobertaForMultipleChoice, RobertaForQuestionAnswering, RobertaForSequenceClassification, RobertaForTokenClassification, RobertaModel, SegformerForImageClassification, SegformerForSemanticSegmentation, SegformerModel, Speech2Text2Decoder, Speech2Text2ForCausalLM, Speech2TextForConditionalGeneration, Speech2TextModel, SwinBackbone, SwinForImageClassification, SwinForMaskedImageModeling, SwinModel, T5ForConditionalGeneration, T5ForQuestionAnswering, T5Model, TrOCRDecoder, TrOCRForCausalLM, ViTForImageClassification, ViTForMaskedImageModeling, ViTModel, Wav2Vec2ForCTC, Wav2Vec2ForMaskedLM, Wav2Vec2ForPreTraining, Wav2Vec2ForSequenceClassification, Wav2Vec2Model, XGLMForCausalLM, XGLMModel"
     ]
    }
   ],
   "source": [
    "symbolic_trace(model, ['input_features', 'attention_mask', 'decoder_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09edceb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad99fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "265c4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c669c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_traced : torch.fx.GraphModule = symbolic_trace(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "056376e1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, input_features, attention_mask = None, head_mask = None, output_attentions = None, output_hidden_states = None, return_dict = None):\n",
      "    conv1 = self.conv1(input_features);  input_features = None\n",
      "    gelu = torch._C._nn.gelu(conv1);  conv1 = None\n",
      "    conv2 = self.conv2(gelu);  gelu = None\n",
      "    gelu_1 = torch._C._nn.gelu(conv2);  conv2 = None\n",
      "    permute = gelu_1.permute(0, 2, 1);  gelu_1 = None\n",
      "    embed_positions_weight = self.embed_positions.weight\n",
      "    add = permute + embed_positions_weight;  permute = embed_positions_weight = None\n",
      "    dropout = torch.nn.functional.dropout(add, p = 0.0, training = False, inplace = False);  add = None\n",
      "    getitem = head_mask[0]\n",
      "    layers_0_self_attn_layer_norm = getattr(self.layers, \"0\").self_attn_layer_norm(dropout)\n",
      "    size = layers_0_self_attn_layer_norm.size()\n",
      "    getitem_1 = size[0]\n",
      "    getitem_2 = size[1]\n",
      "    getitem_3 = size[2];  size = None\n",
      "    layers_0_self_attn_q_proj = getattr(self.layers, \"0\").self_attn.q_proj(layers_0_self_attn_layer_norm)\n",
      "    mul = layers_0_self_attn_q_proj * 0.125;  layers_0_self_attn_q_proj = None\n",
      "    layers_0_self_attn_k_proj = getattr(self.layers, \"0\").self_attn.k_proj(layers_0_self_attn_layer_norm)\n",
      "    view = layers_0_self_attn_k_proj.view(getitem_1, -1, 12, 64);  layers_0_self_attn_k_proj = None\n",
      "    transpose = view.transpose(1, 2);  view = None\n",
      "    contiguous = transpose.contiguous();  transpose = None\n",
      "    layers_0_self_attn_v_proj = getattr(self.layers, \"0\").self_attn.v_proj(layers_0_self_attn_layer_norm);  layers_0_self_attn_layer_norm = None\n",
      "    view_1 = layers_0_self_attn_v_proj.view(getitem_1, -1, 12, 64);  layers_0_self_attn_v_proj = None\n",
      "    transpose_1 = view_1.transpose(1, 2);  view_1 = None\n",
      "    contiguous_1 = transpose_1.contiguous();  transpose_1 = None\n",
      "    mul_1 = getitem_1 * 12\n",
      "    view_2 = mul.view(getitem_1, getitem_2, 12, 64);  mul = None\n",
      "    transpose_2 = view_2.transpose(1, 2);  view_2 = None\n",
      "    contiguous_2 = transpose_2.contiguous();  transpose_2 = None\n",
      "    view_3 = contiguous_2.view(mul_1, -1, 64);  contiguous_2 = None\n",
      "    reshape = contiguous.reshape(mul_1, -1, 64);  contiguous = None\n",
      "    reshape_1 = contiguous_1.reshape(mul_1, -1, 64);  contiguous_1 = mul_1 = None\n",
      "    size_1 = reshape.size(1)\n",
      "    transpose_3 = reshape.transpose(1, 2);  reshape = None\n",
      "    bmm = torch.bmm(view_3, transpose_3);  view_3 = transpose_3 = None\n",
      "    softmax = torch.nn.functional.softmax(bmm, dim = -1, _stacklevel = 3, dtype = None);  bmm = None\n",
      "    dropout_1 = torch.nn.functional.dropout(softmax, p = 0.0, training = False, inplace = False);  softmax = None\n",
      "    bmm_1 = torch.bmm(dropout_1, reshape_1);  dropout_1 = reshape_1 = None\n",
      "    view_4 = bmm_1.view(getitem_1, 12, getitem_2, 64);  bmm_1 = None\n",
      "    transpose_4 = view_4.transpose(1, 2);  view_4 = None\n",
      "    reshape_2 = transpose_4.reshape(getitem_1, getitem_2, 768);  transpose_4 = getitem_1 = getitem_2 = None\n",
      "    layers_0_self_attn_out_proj = getattr(self.layers, \"0\").self_attn.out_proj(reshape_2);  reshape_2 = None\n",
      "    dropout_2 = torch.nn.functional.dropout(layers_0_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_0_self_attn_out_proj = None\n",
      "    add_1 = dropout + dropout_2;  dropout = dropout_2 = None\n",
      "    layers_0_final_layer_norm = getattr(self.layers, \"0\").final_layer_norm(add_1)\n",
      "    layers_0_fc1 = getattr(self.layers, \"0\").fc1(layers_0_final_layer_norm);  layers_0_final_layer_norm = None\n",
      "    gelu_2 = torch._C._nn.gelu(layers_0_fc1);  layers_0_fc1 = None\n",
      "    dropout_3 = torch.nn.functional.dropout(gelu_2, p = 0.0, training = False, inplace = False);  gelu_2 = None\n",
      "    layers_0_fc2 = getattr(self.layers, \"0\").fc2(dropout_3);  dropout_3 = None\n",
      "    dropout_4 = torch.nn.functional.dropout(layers_0_fc2, p = 0.0, training = False, inplace = False);  layers_0_fc2 = None\n",
      "    add_2 = add_1 + dropout_4;  add_1 = dropout_4 = None\n",
      "    getitem_4 = head_mask[1]\n",
      "    layers_1_self_attn_layer_norm = getattr(self.layers, \"1\").self_attn_layer_norm(add_2)\n",
      "    size_2 = layers_1_self_attn_layer_norm.size()\n",
      "    getitem_5 = size_2[0]\n",
      "    getitem_6 = size_2[1]\n",
      "    getitem_7 = size_2[2];  size_2 = None\n",
      "    layers_1_self_attn_q_proj = getattr(self.layers, \"1\").self_attn.q_proj(layers_1_self_attn_layer_norm)\n",
      "    mul_2 = layers_1_self_attn_q_proj * 0.125;  layers_1_self_attn_q_proj = None\n",
      "    layers_1_self_attn_k_proj = getattr(self.layers, \"1\").self_attn.k_proj(layers_1_self_attn_layer_norm)\n",
      "    view_5 = layers_1_self_attn_k_proj.view(getitem_5, -1, 12, 64);  layers_1_self_attn_k_proj = None\n",
      "    transpose_5 = view_5.transpose(1, 2);  view_5 = None\n",
      "    contiguous_3 = transpose_5.contiguous();  transpose_5 = None\n",
      "    layers_1_self_attn_v_proj = getattr(self.layers, \"1\").self_attn.v_proj(layers_1_self_attn_layer_norm);  layers_1_self_attn_layer_norm = None\n",
      "    view_6 = layers_1_self_attn_v_proj.view(getitem_5, -1, 12, 64);  layers_1_self_attn_v_proj = None\n",
      "    transpose_6 = view_6.transpose(1, 2);  view_6 = None\n",
      "    contiguous_4 = transpose_6.contiguous();  transpose_6 = None\n",
      "    mul_3 = getitem_5 * 12\n",
      "    view_7 = mul_2.view(getitem_5, getitem_6, 12, 64);  mul_2 = None\n",
      "    transpose_7 = view_7.transpose(1, 2);  view_7 = None\n",
      "    contiguous_5 = transpose_7.contiguous();  transpose_7 = None\n",
      "    view_8 = contiguous_5.view(mul_3, -1, 64);  contiguous_5 = None\n",
      "    reshape_3 = contiguous_3.reshape(mul_3, -1, 64);  contiguous_3 = None\n",
      "    reshape_4 = contiguous_4.reshape(mul_3, -1, 64);  contiguous_4 = mul_3 = None\n",
      "    size_3 = reshape_3.size(1)\n",
      "    transpose_8 = reshape_3.transpose(1, 2);  reshape_3 = None\n",
      "    bmm_2 = torch.bmm(view_8, transpose_8);  view_8 = transpose_8 = None\n",
      "    softmax_1 = torch.nn.functional.softmax(bmm_2, dim = -1, _stacklevel = 3, dtype = None);  bmm_2 = None\n",
      "    dropout_5 = torch.nn.functional.dropout(softmax_1, p = 0.0, training = False, inplace = False);  softmax_1 = None\n",
      "    bmm_3 = torch.bmm(dropout_5, reshape_4);  dropout_5 = reshape_4 = None\n",
      "    view_9 = bmm_3.view(getitem_5, 12, getitem_6, 64);  bmm_3 = None\n",
      "    transpose_9 = view_9.transpose(1, 2);  view_9 = None\n",
      "    reshape_5 = transpose_9.reshape(getitem_5, getitem_6, 768);  transpose_9 = getitem_5 = getitem_6 = None\n",
      "    layers_1_self_attn_out_proj = getattr(self.layers, \"1\").self_attn.out_proj(reshape_5);  reshape_5 = None\n",
      "    dropout_6 = torch.nn.functional.dropout(layers_1_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_1_self_attn_out_proj = None\n",
      "    add_3 = add_2 + dropout_6;  add_2 = dropout_6 = None\n",
      "    layers_1_final_layer_norm = getattr(self.layers, \"1\").final_layer_norm(add_3)\n",
      "    layers_1_fc1 = getattr(self.layers, \"1\").fc1(layers_1_final_layer_norm);  layers_1_final_layer_norm = None\n",
      "    gelu_3 = torch._C._nn.gelu(layers_1_fc1);  layers_1_fc1 = None\n",
      "    dropout_7 = torch.nn.functional.dropout(gelu_3, p = 0.0, training = False, inplace = False);  gelu_3 = None\n",
      "    layers_1_fc2 = getattr(self.layers, \"1\").fc2(dropout_7);  dropout_7 = None\n",
      "    dropout_8 = torch.nn.functional.dropout(layers_1_fc2, p = 0.0, training = False, inplace = False);  layers_1_fc2 = None\n",
      "    add_4 = add_3 + dropout_8;  add_3 = dropout_8 = None\n",
      "    getitem_8 = head_mask[2]\n",
      "    layers_2_self_attn_layer_norm = getattr(self.layers, \"2\").self_attn_layer_norm(add_4)\n",
      "    size_4 = layers_2_self_attn_layer_norm.size()\n",
      "    getitem_9 = size_4[0]\n",
      "    getitem_10 = size_4[1]\n",
      "    getitem_11 = size_4[2];  size_4 = None\n",
      "    layers_2_self_attn_q_proj = getattr(self.layers, \"2\").self_attn.q_proj(layers_2_self_attn_layer_norm)\n",
      "    mul_4 = layers_2_self_attn_q_proj * 0.125;  layers_2_self_attn_q_proj = None\n",
      "    layers_2_self_attn_k_proj = getattr(self.layers, \"2\").self_attn.k_proj(layers_2_self_attn_layer_norm)\n",
      "    view_10 = layers_2_self_attn_k_proj.view(getitem_9, -1, 12, 64);  layers_2_self_attn_k_proj = None\n",
      "    transpose_10 = view_10.transpose(1, 2);  view_10 = None\n",
      "    contiguous_6 = transpose_10.contiguous();  transpose_10 = None\n",
      "    layers_2_self_attn_v_proj = getattr(self.layers, \"2\").self_attn.v_proj(layers_2_self_attn_layer_norm);  layers_2_self_attn_layer_norm = None\n",
      "    view_11 = layers_2_self_attn_v_proj.view(getitem_9, -1, 12, 64);  layers_2_self_attn_v_proj = None\n",
      "    transpose_11 = view_11.transpose(1, 2);  view_11 = None\n",
      "    contiguous_7 = transpose_11.contiguous();  transpose_11 = None\n",
      "    mul_5 = getitem_9 * 12\n",
      "    view_12 = mul_4.view(getitem_9, getitem_10, 12, 64);  mul_4 = None\n",
      "    transpose_12 = view_12.transpose(1, 2);  view_12 = None\n",
      "    contiguous_8 = transpose_12.contiguous();  transpose_12 = None\n",
      "    view_13 = contiguous_8.view(mul_5, -1, 64);  contiguous_8 = None\n",
      "    reshape_6 = contiguous_6.reshape(mul_5, -1, 64);  contiguous_6 = None\n",
      "    reshape_7 = contiguous_7.reshape(mul_5, -1, 64);  contiguous_7 = mul_5 = None\n",
      "    size_5 = reshape_6.size(1)\n",
      "    transpose_13 = reshape_6.transpose(1, 2);  reshape_6 = None\n",
      "    bmm_4 = torch.bmm(view_13, transpose_13);  view_13 = transpose_13 = None\n",
      "    softmax_2 = torch.nn.functional.softmax(bmm_4, dim = -1, _stacklevel = 3, dtype = None);  bmm_4 = None\n",
      "    dropout_9 = torch.nn.functional.dropout(softmax_2, p = 0.0, training = False, inplace = False);  softmax_2 = None\n",
      "    bmm_5 = torch.bmm(dropout_9, reshape_7);  dropout_9 = reshape_7 = None\n",
      "    view_14 = bmm_5.view(getitem_9, 12, getitem_10, 64);  bmm_5 = None\n",
      "    transpose_14 = view_14.transpose(1, 2);  view_14 = None\n",
      "    reshape_8 = transpose_14.reshape(getitem_9, getitem_10, 768);  transpose_14 = getitem_9 = getitem_10 = None\n",
      "    layers_2_self_attn_out_proj = getattr(self.layers, \"2\").self_attn.out_proj(reshape_8);  reshape_8 = None\n",
      "    dropout_10 = torch.nn.functional.dropout(layers_2_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_2_self_attn_out_proj = None\n",
      "    add_5 = add_4 + dropout_10;  add_4 = dropout_10 = None\n",
      "    layers_2_final_layer_norm = getattr(self.layers, \"2\").final_layer_norm(add_5)\n",
      "    layers_2_fc1 = getattr(self.layers, \"2\").fc1(layers_2_final_layer_norm);  layers_2_final_layer_norm = None\n",
      "    gelu_4 = torch._C._nn.gelu(layers_2_fc1);  layers_2_fc1 = None\n",
      "    dropout_11 = torch.nn.functional.dropout(gelu_4, p = 0.0, training = False, inplace = False);  gelu_4 = None\n",
      "    layers_2_fc2 = getattr(self.layers, \"2\").fc2(dropout_11);  dropout_11 = None\n",
      "    dropout_12 = torch.nn.functional.dropout(layers_2_fc2, p = 0.0, training = False, inplace = False);  layers_2_fc2 = None\n",
      "    add_6 = add_5 + dropout_12;  add_5 = dropout_12 = None\n",
      "    getitem_12 = head_mask[3]\n",
      "    layers_3_self_attn_layer_norm = getattr(self.layers, \"3\").self_attn_layer_norm(add_6)\n",
      "    size_6 = layers_3_self_attn_layer_norm.size()\n",
      "    getitem_13 = size_6[0]\n",
      "    getitem_14 = size_6[1]\n",
      "    getitem_15 = size_6[2];  size_6 = None\n",
      "    layers_3_self_attn_q_proj = getattr(self.layers, \"3\").self_attn.q_proj(layers_3_self_attn_layer_norm)\n",
      "    mul_6 = layers_3_self_attn_q_proj * 0.125;  layers_3_self_attn_q_proj = None\n",
      "    layers_3_self_attn_k_proj = getattr(self.layers, \"3\").self_attn.k_proj(layers_3_self_attn_layer_norm)\n",
      "    view_15 = layers_3_self_attn_k_proj.view(getitem_13, -1, 12, 64);  layers_3_self_attn_k_proj = None\n",
      "    transpose_15 = view_15.transpose(1, 2);  view_15 = None\n",
      "    contiguous_9 = transpose_15.contiguous();  transpose_15 = None\n",
      "    layers_3_self_attn_v_proj = getattr(self.layers, \"3\").self_attn.v_proj(layers_3_self_attn_layer_norm);  layers_3_self_attn_layer_norm = None\n",
      "    view_16 = layers_3_self_attn_v_proj.view(getitem_13, -1, 12, 64);  layers_3_self_attn_v_proj = None\n",
      "    transpose_16 = view_16.transpose(1, 2);  view_16 = None\n",
      "    contiguous_10 = transpose_16.contiguous();  transpose_16 = None\n",
      "    mul_7 = getitem_13 * 12\n",
      "    view_17 = mul_6.view(getitem_13, getitem_14, 12, 64);  mul_6 = None\n",
      "    transpose_17 = view_17.transpose(1, 2);  view_17 = None\n",
      "    contiguous_11 = transpose_17.contiguous();  transpose_17 = None\n",
      "    view_18 = contiguous_11.view(mul_7, -1, 64);  contiguous_11 = None\n",
      "    reshape_9 = contiguous_9.reshape(mul_7, -1, 64);  contiguous_9 = None\n",
      "    reshape_10 = contiguous_10.reshape(mul_7, -1, 64);  contiguous_10 = mul_7 = None\n",
      "    size_7 = reshape_9.size(1)\n",
      "    transpose_18 = reshape_9.transpose(1, 2);  reshape_9 = None\n",
      "    bmm_6 = torch.bmm(view_18, transpose_18);  view_18 = transpose_18 = None\n",
      "    softmax_3 = torch.nn.functional.softmax(bmm_6, dim = -1, _stacklevel = 3, dtype = None);  bmm_6 = None\n",
      "    dropout_13 = torch.nn.functional.dropout(softmax_3, p = 0.0, training = False, inplace = False);  softmax_3 = None\n",
      "    bmm_7 = torch.bmm(dropout_13, reshape_10);  dropout_13 = reshape_10 = None\n",
      "    view_19 = bmm_7.view(getitem_13, 12, getitem_14, 64);  bmm_7 = None\n",
      "    transpose_19 = view_19.transpose(1, 2);  view_19 = None\n",
      "    reshape_11 = transpose_19.reshape(getitem_13, getitem_14, 768);  transpose_19 = getitem_13 = getitem_14 = None\n",
      "    layers_3_self_attn_out_proj = getattr(self.layers, \"3\").self_attn.out_proj(reshape_11);  reshape_11 = None\n",
      "    dropout_14 = torch.nn.functional.dropout(layers_3_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_3_self_attn_out_proj = None\n",
      "    add_7 = add_6 + dropout_14;  add_6 = dropout_14 = None\n",
      "    layers_3_final_layer_norm = getattr(self.layers, \"3\").final_layer_norm(add_7)\n",
      "    layers_3_fc1 = getattr(self.layers, \"3\").fc1(layers_3_final_layer_norm);  layers_3_final_layer_norm = None\n",
      "    gelu_5 = torch._C._nn.gelu(layers_3_fc1);  layers_3_fc1 = None\n",
      "    dropout_15 = torch.nn.functional.dropout(gelu_5, p = 0.0, training = False, inplace = False);  gelu_5 = None\n",
      "    layers_3_fc2 = getattr(self.layers, \"3\").fc2(dropout_15);  dropout_15 = None\n",
      "    dropout_16 = torch.nn.functional.dropout(layers_3_fc2, p = 0.0, training = False, inplace = False);  layers_3_fc2 = None\n",
      "    add_8 = add_7 + dropout_16;  add_7 = dropout_16 = None\n",
      "    getitem_16 = head_mask[4]\n",
      "    layers_4_self_attn_layer_norm = getattr(self.layers, \"4\").self_attn_layer_norm(add_8)\n",
      "    size_8 = layers_4_self_attn_layer_norm.size()\n",
      "    getitem_17 = size_8[0]\n",
      "    getitem_18 = size_8[1]\n",
      "    getitem_19 = size_8[2];  size_8 = None\n",
      "    layers_4_self_attn_q_proj = getattr(self.layers, \"4\").self_attn.q_proj(layers_4_self_attn_layer_norm)\n",
      "    mul_8 = layers_4_self_attn_q_proj * 0.125;  layers_4_self_attn_q_proj = None\n",
      "    layers_4_self_attn_k_proj = getattr(self.layers, \"4\").self_attn.k_proj(layers_4_self_attn_layer_norm)\n",
      "    view_20 = layers_4_self_attn_k_proj.view(getitem_17, -1, 12, 64);  layers_4_self_attn_k_proj = None\n",
      "    transpose_20 = view_20.transpose(1, 2);  view_20 = None\n",
      "    contiguous_12 = transpose_20.contiguous();  transpose_20 = None\n",
      "    layers_4_self_attn_v_proj = getattr(self.layers, \"4\").self_attn.v_proj(layers_4_self_attn_layer_norm);  layers_4_self_attn_layer_norm = None\n",
      "    view_21 = layers_4_self_attn_v_proj.view(getitem_17, -1, 12, 64);  layers_4_self_attn_v_proj = None\n",
      "    transpose_21 = view_21.transpose(1, 2);  view_21 = None\n",
      "    contiguous_13 = transpose_21.contiguous();  transpose_21 = None\n",
      "    mul_9 = getitem_17 * 12\n",
      "    view_22 = mul_8.view(getitem_17, getitem_18, 12, 64);  mul_8 = None\n",
      "    transpose_22 = view_22.transpose(1, 2);  view_22 = None\n",
      "    contiguous_14 = transpose_22.contiguous();  transpose_22 = None\n",
      "    view_23 = contiguous_14.view(mul_9, -1, 64);  contiguous_14 = None\n",
      "    reshape_12 = contiguous_12.reshape(mul_9, -1, 64);  contiguous_12 = None\n",
      "    reshape_13 = contiguous_13.reshape(mul_9, -1, 64);  contiguous_13 = mul_9 = None\n",
      "    size_9 = reshape_12.size(1)\n",
      "    transpose_23 = reshape_12.transpose(1, 2);  reshape_12 = None\n",
      "    bmm_8 = torch.bmm(view_23, transpose_23);  view_23 = transpose_23 = None\n",
      "    softmax_4 = torch.nn.functional.softmax(bmm_8, dim = -1, _stacklevel = 3, dtype = None);  bmm_8 = None\n",
      "    dropout_17 = torch.nn.functional.dropout(softmax_4, p = 0.0, training = False, inplace = False);  softmax_4 = None\n",
      "    bmm_9 = torch.bmm(dropout_17, reshape_13);  dropout_17 = reshape_13 = None\n",
      "    view_24 = bmm_9.view(getitem_17, 12, getitem_18, 64);  bmm_9 = None\n",
      "    transpose_24 = view_24.transpose(1, 2);  view_24 = None\n",
      "    reshape_14 = transpose_24.reshape(getitem_17, getitem_18, 768);  transpose_24 = getitem_17 = getitem_18 = None\n",
      "    layers_4_self_attn_out_proj = getattr(self.layers, \"4\").self_attn.out_proj(reshape_14);  reshape_14 = None\n",
      "    dropout_18 = torch.nn.functional.dropout(layers_4_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_4_self_attn_out_proj = None\n",
      "    add_9 = add_8 + dropout_18;  add_8 = dropout_18 = None\n",
      "    layers_4_final_layer_norm = getattr(self.layers, \"4\").final_layer_norm(add_9)\n",
      "    layers_4_fc1 = getattr(self.layers, \"4\").fc1(layers_4_final_layer_norm);  layers_4_final_layer_norm = None\n",
      "    gelu_6 = torch._C._nn.gelu(layers_4_fc1);  layers_4_fc1 = None\n",
      "    dropout_19 = torch.nn.functional.dropout(gelu_6, p = 0.0, training = False, inplace = False);  gelu_6 = None\n",
      "    layers_4_fc2 = getattr(self.layers, \"4\").fc2(dropout_19);  dropout_19 = None\n",
      "    dropout_20 = torch.nn.functional.dropout(layers_4_fc2, p = 0.0, training = False, inplace = False);  layers_4_fc2 = None\n",
      "    add_10 = add_9 + dropout_20;  add_9 = dropout_20 = None\n",
      "    getitem_20 = head_mask[5]\n",
      "    layers_5_self_attn_layer_norm = getattr(self.layers, \"5\").self_attn_layer_norm(add_10)\n",
      "    size_10 = layers_5_self_attn_layer_norm.size()\n",
      "    getitem_21 = size_10[0]\n",
      "    getitem_22 = size_10[1]\n",
      "    getitem_23 = size_10[2];  size_10 = None\n",
      "    layers_5_self_attn_q_proj = getattr(self.layers, \"5\").self_attn.q_proj(layers_5_self_attn_layer_norm)\n",
      "    mul_10 = layers_5_self_attn_q_proj * 0.125;  layers_5_self_attn_q_proj = None\n",
      "    layers_5_self_attn_k_proj = getattr(self.layers, \"5\").self_attn.k_proj(layers_5_self_attn_layer_norm)\n",
      "    view_25 = layers_5_self_attn_k_proj.view(getitem_21, -1, 12, 64);  layers_5_self_attn_k_proj = None\n",
      "    transpose_25 = view_25.transpose(1, 2);  view_25 = None\n",
      "    contiguous_15 = transpose_25.contiguous();  transpose_25 = None\n",
      "    layers_5_self_attn_v_proj = getattr(self.layers, \"5\").self_attn.v_proj(layers_5_self_attn_layer_norm);  layers_5_self_attn_layer_norm = None\n",
      "    view_26 = layers_5_self_attn_v_proj.view(getitem_21, -1, 12, 64);  layers_5_self_attn_v_proj = None\n",
      "    transpose_26 = view_26.transpose(1, 2);  view_26 = None\n",
      "    contiguous_16 = transpose_26.contiguous();  transpose_26 = None\n",
      "    mul_11 = getitem_21 * 12\n",
      "    view_27 = mul_10.view(getitem_21, getitem_22, 12, 64);  mul_10 = None\n",
      "    transpose_27 = view_27.transpose(1, 2);  view_27 = None\n",
      "    contiguous_17 = transpose_27.contiguous();  transpose_27 = None\n",
      "    view_28 = contiguous_17.view(mul_11, -1, 64);  contiguous_17 = None\n",
      "    reshape_15 = contiguous_15.reshape(mul_11, -1, 64);  contiguous_15 = None\n",
      "    reshape_16 = contiguous_16.reshape(mul_11, -1, 64);  contiguous_16 = mul_11 = None\n",
      "    size_11 = reshape_15.size(1)\n",
      "    transpose_28 = reshape_15.transpose(1, 2);  reshape_15 = None\n",
      "    bmm_10 = torch.bmm(view_28, transpose_28);  view_28 = transpose_28 = None\n",
      "    softmax_5 = torch.nn.functional.softmax(bmm_10, dim = -1, _stacklevel = 3, dtype = None);  bmm_10 = None\n",
      "    dropout_21 = torch.nn.functional.dropout(softmax_5, p = 0.0, training = False, inplace = False);  softmax_5 = None\n",
      "    bmm_11 = torch.bmm(dropout_21, reshape_16);  dropout_21 = reshape_16 = None\n",
      "    view_29 = bmm_11.view(getitem_21, 12, getitem_22, 64);  bmm_11 = None\n",
      "    transpose_29 = view_29.transpose(1, 2);  view_29 = None\n",
      "    reshape_17 = transpose_29.reshape(getitem_21, getitem_22, 768);  transpose_29 = getitem_21 = getitem_22 = None\n",
      "    layers_5_self_attn_out_proj = getattr(self.layers, \"5\").self_attn.out_proj(reshape_17);  reshape_17 = None\n",
      "    dropout_22 = torch.nn.functional.dropout(layers_5_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_5_self_attn_out_proj = None\n",
      "    add_11 = add_10 + dropout_22;  add_10 = dropout_22 = None\n",
      "    layers_5_final_layer_norm = getattr(self.layers, \"5\").final_layer_norm(add_11)\n",
      "    layers_5_fc1 = getattr(self.layers, \"5\").fc1(layers_5_final_layer_norm);  layers_5_final_layer_norm = None\n",
      "    gelu_7 = torch._C._nn.gelu(layers_5_fc1);  layers_5_fc1 = None\n",
      "    dropout_23 = torch.nn.functional.dropout(gelu_7, p = 0.0, training = False, inplace = False);  gelu_7 = None\n",
      "    layers_5_fc2 = getattr(self.layers, \"5\").fc2(dropout_23);  dropout_23 = None\n",
      "    dropout_24 = torch.nn.functional.dropout(layers_5_fc2, p = 0.0, training = False, inplace = False);  layers_5_fc2 = None\n",
      "    add_12 = add_11 + dropout_24;  add_11 = dropout_24 = None\n",
      "    getitem_24 = head_mask[6]\n",
      "    layers_6_self_attn_layer_norm = getattr(self.layers, \"6\").self_attn_layer_norm(add_12)\n",
      "    size_12 = layers_6_self_attn_layer_norm.size()\n",
      "    getitem_25 = size_12[0]\n",
      "    getitem_26 = size_12[1]\n",
      "    getitem_27 = size_12[2];  size_12 = None\n",
      "    layers_6_self_attn_q_proj = getattr(self.layers, \"6\").self_attn.q_proj(layers_6_self_attn_layer_norm)\n",
      "    mul_12 = layers_6_self_attn_q_proj * 0.125;  layers_6_self_attn_q_proj = None\n",
      "    layers_6_self_attn_k_proj = getattr(self.layers, \"6\").self_attn.k_proj(layers_6_self_attn_layer_norm)\n",
      "    view_30 = layers_6_self_attn_k_proj.view(getitem_25, -1, 12, 64);  layers_6_self_attn_k_proj = None\n",
      "    transpose_30 = view_30.transpose(1, 2);  view_30 = None\n",
      "    contiguous_18 = transpose_30.contiguous();  transpose_30 = None\n",
      "    layers_6_self_attn_v_proj = getattr(self.layers, \"6\").self_attn.v_proj(layers_6_self_attn_layer_norm);  layers_6_self_attn_layer_norm = None\n",
      "    view_31 = layers_6_self_attn_v_proj.view(getitem_25, -1, 12, 64);  layers_6_self_attn_v_proj = None\n",
      "    transpose_31 = view_31.transpose(1, 2);  view_31 = None\n",
      "    contiguous_19 = transpose_31.contiguous();  transpose_31 = None\n",
      "    mul_13 = getitem_25 * 12\n",
      "    view_32 = mul_12.view(getitem_25, getitem_26, 12, 64);  mul_12 = None\n",
      "    transpose_32 = view_32.transpose(1, 2);  view_32 = None\n",
      "    contiguous_20 = transpose_32.contiguous();  transpose_32 = None\n",
      "    view_33 = contiguous_20.view(mul_13, -1, 64);  contiguous_20 = None\n",
      "    reshape_18 = contiguous_18.reshape(mul_13, -1, 64);  contiguous_18 = None\n",
      "    reshape_19 = contiguous_19.reshape(mul_13, -1, 64);  contiguous_19 = mul_13 = None\n",
      "    size_13 = reshape_18.size(1)\n",
      "    transpose_33 = reshape_18.transpose(1, 2);  reshape_18 = None\n",
      "    bmm_12 = torch.bmm(view_33, transpose_33);  view_33 = transpose_33 = None\n",
      "    softmax_6 = torch.nn.functional.softmax(bmm_12, dim = -1, _stacklevel = 3, dtype = None);  bmm_12 = None\n",
      "    dropout_25 = torch.nn.functional.dropout(softmax_6, p = 0.0, training = False, inplace = False);  softmax_6 = None\n",
      "    bmm_13 = torch.bmm(dropout_25, reshape_19);  dropout_25 = reshape_19 = None\n",
      "    view_34 = bmm_13.view(getitem_25, 12, getitem_26, 64);  bmm_13 = None\n",
      "    transpose_34 = view_34.transpose(1, 2);  view_34 = None\n",
      "    reshape_20 = transpose_34.reshape(getitem_25, getitem_26, 768);  transpose_34 = getitem_25 = getitem_26 = None\n",
      "    layers_6_self_attn_out_proj = getattr(self.layers, \"6\").self_attn.out_proj(reshape_20);  reshape_20 = None\n",
      "    dropout_26 = torch.nn.functional.dropout(layers_6_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_6_self_attn_out_proj = None\n",
      "    add_13 = add_12 + dropout_26;  add_12 = dropout_26 = None\n",
      "    layers_6_final_layer_norm = getattr(self.layers, \"6\").final_layer_norm(add_13)\n",
      "    layers_6_fc1 = getattr(self.layers, \"6\").fc1(layers_6_final_layer_norm);  layers_6_final_layer_norm = None\n",
      "    gelu_8 = torch._C._nn.gelu(layers_6_fc1);  layers_6_fc1 = None\n",
      "    dropout_27 = torch.nn.functional.dropout(gelu_8, p = 0.0, training = False, inplace = False);  gelu_8 = None\n",
      "    layers_6_fc2 = getattr(self.layers, \"6\").fc2(dropout_27);  dropout_27 = None\n",
      "    dropout_28 = torch.nn.functional.dropout(layers_6_fc2, p = 0.0, training = False, inplace = False);  layers_6_fc2 = None\n",
      "    add_14 = add_13 + dropout_28;  add_13 = dropout_28 = None\n",
      "    getitem_28 = head_mask[7]\n",
      "    layers_7_self_attn_layer_norm = getattr(self.layers, \"7\").self_attn_layer_norm(add_14)\n",
      "    size_14 = layers_7_self_attn_layer_norm.size()\n",
      "    getitem_29 = size_14[0]\n",
      "    getitem_30 = size_14[1]\n",
      "    getitem_31 = size_14[2];  size_14 = None\n",
      "    layers_7_self_attn_q_proj = getattr(self.layers, \"7\").self_attn.q_proj(layers_7_self_attn_layer_norm)\n",
      "    mul_14 = layers_7_self_attn_q_proj * 0.125;  layers_7_self_attn_q_proj = None\n",
      "    layers_7_self_attn_k_proj = getattr(self.layers, \"7\").self_attn.k_proj(layers_7_self_attn_layer_norm)\n",
      "    view_35 = layers_7_self_attn_k_proj.view(getitem_29, -1, 12, 64);  layers_7_self_attn_k_proj = None\n",
      "    transpose_35 = view_35.transpose(1, 2);  view_35 = None\n",
      "    contiguous_21 = transpose_35.contiguous();  transpose_35 = None\n",
      "    layers_7_self_attn_v_proj = getattr(self.layers, \"7\").self_attn.v_proj(layers_7_self_attn_layer_norm);  layers_7_self_attn_layer_norm = None\n",
      "    view_36 = layers_7_self_attn_v_proj.view(getitem_29, -1, 12, 64);  layers_7_self_attn_v_proj = None\n",
      "    transpose_36 = view_36.transpose(1, 2);  view_36 = None\n",
      "    contiguous_22 = transpose_36.contiguous();  transpose_36 = None\n",
      "    mul_15 = getitem_29 * 12\n",
      "    view_37 = mul_14.view(getitem_29, getitem_30, 12, 64);  mul_14 = None\n",
      "    transpose_37 = view_37.transpose(1, 2);  view_37 = None\n",
      "    contiguous_23 = transpose_37.contiguous();  transpose_37 = None\n",
      "    view_38 = contiguous_23.view(mul_15, -1, 64);  contiguous_23 = None\n",
      "    reshape_21 = contiguous_21.reshape(mul_15, -1, 64);  contiguous_21 = None\n",
      "    reshape_22 = contiguous_22.reshape(mul_15, -1, 64);  contiguous_22 = mul_15 = None\n",
      "    size_15 = reshape_21.size(1)\n",
      "    transpose_38 = reshape_21.transpose(1, 2);  reshape_21 = None\n",
      "    bmm_14 = torch.bmm(view_38, transpose_38);  view_38 = transpose_38 = None\n",
      "    softmax_7 = torch.nn.functional.softmax(bmm_14, dim = -1, _stacklevel = 3, dtype = None);  bmm_14 = None\n",
      "    dropout_29 = torch.nn.functional.dropout(softmax_7, p = 0.0, training = False, inplace = False);  softmax_7 = None\n",
      "    bmm_15 = torch.bmm(dropout_29, reshape_22);  dropout_29 = reshape_22 = None\n",
      "    view_39 = bmm_15.view(getitem_29, 12, getitem_30, 64);  bmm_15 = None\n",
      "    transpose_39 = view_39.transpose(1, 2);  view_39 = None\n",
      "    reshape_23 = transpose_39.reshape(getitem_29, getitem_30, 768);  transpose_39 = getitem_29 = getitem_30 = None\n",
      "    layers_7_self_attn_out_proj = getattr(self.layers, \"7\").self_attn.out_proj(reshape_23);  reshape_23 = None\n",
      "    dropout_30 = torch.nn.functional.dropout(layers_7_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_7_self_attn_out_proj = None\n",
      "    add_15 = add_14 + dropout_30;  add_14 = dropout_30 = None\n",
      "    layers_7_final_layer_norm = getattr(self.layers, \"7\").final_layer_norm(add_15)\n",
      "    layers_7_fc1 = getattr(self.layers, \"7\").fc1(layers_7_final_layer_norm);  layers_7_final_layer_norm = None\n",
      "    gelu_9 = torch._C._nn.gelu(layers_7_fc1);  layers_7_fc1 = None\n",
      "    dropout_31 = torch.nn.functional.dropout(gelu_9, p = 0.0, training = False, inplace = False);  gelu_9 = None\n",
      "    layers_7_fc2 = getattr(self.layers, \"7\").fc2(dropout_31);  dropout_31 = None\n",
      "    dropout_32 = torch.nn.functional.dropout(layers_7_fc2, p = 0.0, training = False, inplace = False);  layers_7_fc2 = None\n",
      "    add_16 = add_15 + dropout_32;  add_15 = dropout_32 = None\n",
      "    getitem_32 = head_mask[8]\n",
      "    layers_8_self_attn_layer_norm = getattr(self.layers, \"8\").self_attn_layer_norm(add_16)\n",
      "    size_16 = layers_8_self_attn_layer_norm.size()\n",
      "    getitem_33 = size_16[0]\n",
      "    getitem_34 = size_16[1]\n",
      "    getitem_35 = size_16[2];  size_16 = None\n",
      "    layers_8_self_attn_q_proj = getattr(self.layers, \"8\").self_attn.q_proj(layers_8_self_attn_layer_norm)\n",
      "    mul_16 = layers_8_self_attn_q_proj * 0.125;  layers_8_self_attn_q_proj = None\n",
      "    layers_8_self_attn_k_proj = getattr(self.layers, \"8\").self_attn.k_proj(layers_8_self_attn_layer_norm)\n",
      "    view_40 = layers_8_self_attn_k_proj.view(getitem_33, -1, 12, 64);  layers_8_self_attn_k_proj = None\n",
      "    transpose_40 = view_40.transpose(1, 2);  view_40 = None\n",
      "    contiguous_24 = transpose_40.contiguous();  transpose_40 = None\n",
      "    layers_8_self_attn_v_proj = getattr(self.layers, \"8\").self_attn.v_proj(layers_8_self_attn_layer_norm);  layers_8_self_attn_layer_norm = None\n",
      "    view_41 = layers_8_self_attn_v_proj.view(getitem_33, -1, 12, 64);  layers_8_self_attn_v_proj = None\n",
      "    transpose_41 = view_41.transpose(1, 2);  view_41 = None\n",
      "    contiguous_25 = transpose_41.contiguous();  transpose_41 = None\n",
      "    mul_17 = getitem_33 * 12\n",
      "    view_42 = mul_16.view(getitem_33, getitem_34, 12, 64);  mul_16 = None\n",
      "    transpose_42 = view_42.transpose(1, 2);  view_42 = None\n",
      "    contiguous_26 = transpose_42.contiguous();  transpose_42 = None\n",
      "    view_43 = contiguous_26.view(mul_17, -1, 64);  contiguous_26 = None\n",
      "    reshape_24 = contiguous_24.reshape(mul_17, -1, 64);  contiguous_24 = None\n",
      "    reshape_25 = contiguous_25.reshape(mul_17, -1, 64);  contiguous_25 = mul_17 = None\n",
      "    size_17 = reshape_24.size(1)\n",
      "    transpose_43 = reshape_24.transpose(1, 2);  reshape_24 = None\n",
      "    bmm_16 = torch.bmm(view_43, transpose_43);  view_43 = transpose_43 = None\n",
      "    softmax_8 = torch.nn.functional.softmax(bmm_16, dim = -1, _stacklevel = 3, dtype = None);  bmm_16 = None\n",
      "    dropout_33 = torch.nn.functional.dropout(softmax_8, p = 0.0, training = False, inplace = False);  softmax_8 = None\n",
      "    bmm_17 = torch.bmm(dropout_33, reshape_25);  dropout_33 = reshape_25 = None\n",
      "    view_44 = bmm_17.view(getitem_33, 12, getitem_34, 64);  bmm_17 = None\n",
      "    transpose_44 = view_44.transpose(1, 2);  view_44 = None\n",
      "    reshape_26 = transpose_44.reshape(getitem_33, getitem_34, 768);  transpose_44 = getitem_33 = getitem_34 = None\n",
      "    layers_8_self_attn_out_proj = getattr(self.layers, \"8\").self_attn.out_proj(reshape_26);  reshape_26 = None\n",
      "    dropout_34 = torch.nn.functional.dropout(layers_8_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_8_self_attn_out_proj = None\n",
      "    add_17 = add_16 + dropout_34;  add_16 = dropout_34 = None\n",
      "    layers_8_final_layer_norm = getattr(self.layers, \"8\").final_layer_norm(add_17)\n",
      "    layers_8_fc1 = getattr(self.layers, \"8\").fc1(layers_8_final_layer_norm);  layers_8_final_layer_norm = None\n",
      "    gelu_10 = torch._C._nn.gelu(layers_8_fc1);  layers_8_fc1 = None\n",
      "    dropout_35 = torch.nn.functional.dropout(gelu_10, p = 0.0, training = False, inplace = False);  gelu_10 = None\n",
      "    layers_8_fc2 = getattr(self.layers, \"8\").fc2(dropout_35);  dropout_35 = None\n",
      "    dropout_36 = torch.nn.functional.dropout(layers_8_fc2, p = 0.0, training = False, inplace = False);  layers_8_fc2 = None\n",
      "    add_18 = add_17 + dropout_36;  add_17 = dropout_36 = None\n",
      "    getitem_36 = head_mask[9]\n",
      "    layers_9_self_attn_layer_norm = getattr(self.layers, \"9\").self_attn_layer_norm(add_18)\n",
      "    size_18 = layers_9_self_attn_layer_norm.size()\n",
      "    getitem_37 = size_18[0]\n",
      "    getitem_38 = size_18[1]\n",
      "    getitem_39 = size_18[2];  size_18 = None\n",
      "    layers_9_self_attn_q_proj = getattr(self.layers, \"9\").self_attn.q_proj(layers_9_self_attn_layer_norm)\n",
      "    mul_18 = layers_9_self_attn_q_proj * 0.125;  layers_9_self_attn_q_proj = None\n",
      "    layers_9_self_attn_k_proj = getattr(self.layers, \"9\").self_attn.k_proj(layers_9_self_attn_layer_norm)\n",
      "    view_45 = layers_9_self_attn_k_proj.view(getitem_37, -1, 12, 64);  layers_9_self_attn_k_proj = None\n",
      "    transpose_45 = view_45.transpose(1, 2);  view_45 = None\n",
      "    contiguous_27 = transpose_45.contiguous();  transpose_45 = None\n",
      "    layers_9_self_attn_v_proj = getattr(self.layers, \"9\").self_attn.v_proj(layers_9_self_attn_layer_norm);  layers_9_self_attn_layer_norm = None\n",
      "    view_46 = layers_9_self_attn_v_proj.view(getitem_37, -1, 12, 64);  layers_9_self_attn_v_proj = None\n",
      "    transpose_46 = view_46.transpose(1, 2);  view_46 = None\n",
      "    contiguous_28 = transpose_46.contiguous();  transpose_46 = None\n",
      "    mul_19 = getitem_37 * 12\n",
      "    view_47 = mul_18.view(getitem_37, getitem_38, 12, 64);  mul_18 = None\n",
      "    transpose_47 = view_47.transpose(1, 2);  view_47 = None\n",
      "    contiguous_29 = transpose_47.contiguous();  transpose_47 = None\n",
      "    view_48 = contiguous_29.view(mul_19, -1, 64);  contiguous_29 = None\n",
      "    reshape_27 = contiguous_27.reshape(mul_19, -1, 64);  contiguous_27 = None\n",
      "    reshape_28 = contiguous_28.reshape(mul_19, -1, 64);  contiguous_28 = mul_19 = None\n",
      "    size_19 = reshape_27.size(1)\n",
      "    transpose_48 = reshape_27.transpose(1, 2);  reshape_27 = None\n",
      "    bmm_18 = torch.bmm(view_48, transpose_48);  view_48 = transpose_48 = None\n",
      "    softmax_9 = torch.nn.functional.softmax(bmm_18, dim = -1, _stacklevel = 3, dtype = None);  bmm_18 = None\n",
      "    dropout_37 = torch.nn.functional.dropout(softmax_9, p = 0.0, training = False, inplace = False);  softmax_9 = None\n",
      "    bmm_19 = torch.bmm(dropout_37, reshape_28);  dropout_37 = reshape_28 = None\n",
      "    view_49 = bmm_19.view(getitem_37, 12, getitem_38, 64);  bmm_19 = None\n",
      "    transpose_49 = view_49.transpose(1, 2);  view_49 = None\n",
      "    reshape_29 = transpose_49.reshape(getitem_37, getitem_38, 768);  transpose_49 = getitem_37 = getitem_38 = None\n",
      "    layers_9_self_attn_out_proj = getattr(self.layers, \"9\").self_attn.out_proj(reshape_29);  reshape_29 = None\n",
      "    dropout_38 = torch.nn.functional.dropout(layers_9_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_9_self_attn_out_proj = None\n",
      "    add_19 = add_18 + dropout_38;  add_18 = dropout_38 = None\n",
      "    layers_9_final_layer_norm = getattr(self.layers, \"9\").final_layer_norm(add_19)\n",
      "    layers_9_fc1 = getattr(self.layers, \"9\").fc1(layers_9_final_layer_norm);  layers_9_final_layer_norm = None\n",
      "    gelu_11 = torch._C._nn.gelu(layers_9_fc1);  layers_9_fc1 = None\n",
      "    dropout_39 = torch.nn.functional.dropout(gelu_11, p = 0.0, training = False, inplace = False);  gelu_11 = None\n",
      "    layers_9_fc2 = getattr(self.layers, \"9\").fc2(dropout_39);  dropout_39 = None\n",
      "    dropout_40 = torch.nn.functional.dropout(layers_9_fc2, p = 0.0, training = False, inplace = False);  layers_9_fc2 = None\n",
      "    add_20 = add_19 + dropout_40;  add_19 = dropout_40 = None\n",
      "    getitem_40 = head_mask[10]\n",
      "    layers_10_self_attn_layer_norm = getattr(self.layers, \"10\").self_attn_layer_norm(add_20)\n",
      "    size_20 = layers_10_self_attn_layer_norm.size()\n",
      "    getitem_41 = size_20[0]\n",
      "    getitem_42 = size_20[1]\n",
      "    getitem_43 = size_20[2];  size_20 = None\n",
      "    layers_10_self_attn_q_proj = getattr(self.layers, \"10\").self_attn.q_proj(layers_10_self_attn_layer_norm)\n",
      "    mul_20 = layers_10_self_attn_q_proj * 0.125;  layers_10_self_attn_q_proj = None\n",
      "    layers_10_self_attn_k_proj = getattr(self.layers, \"10\").self_attn.k_proj(layers_10_self_attn_layer_norm)\n",
      "    view_50 = layers_10_self_attn_k_proj.view(getitem_41, -1, 12, 64);  layers_10_self_attn_k_proj = None\n",
      "    transpose_50 = view_50.transpose(1, 2);  view_50 = None\n",
      "    contiguous_30 = transpose_50.contiguous();  transpose_50 = None\n",
      "    layers_10_self_attn_v_proj = getattr(self.layers, \"10\").self_attn.v_proj(layers_10_self_attn_layer_norm);  layers_10_self_attn_layer_norm = None\n",
      "    view_51 = layers_10_self_attn_v_proj.view(getitem_41, -1, 12, 64);  layers_10_self_attn_v_proj = None\n",
      "    transpose_51 = view_51.transpose(1, 2);  view_51 = None\n",
      "    contiguous_31 = transpose_51.contiguous();  transpose_51 = None\n",
      "    mul_21 = getitem_41 * 12\n",
      "    view_52 = mul_20.view(getitem_41, getitem_42, 12, 64);  mul_20 = None\n",
      "    transpose_52 = view_52.transpose(1, 2);  view_52 = None\n",
      "    contiguous_32 = transpose_52.contiguous();  transpose_52 = None\n",
      "    view_53 = contiguous_32.view(mul_21, -1, 64);  contiguous_32 = None\n",
      "    reshape_30 = contiguous_30.reshape(mul_21, -1, 64);  contiguous_30 = None\n",
      "    reshape_31 = contiguous_31.reshape(mul_21, -1, 64);  contiguous_31 = mul_21 = None\n",
      "    size_21 = reshape_30.size(1)\n",
      "    transpose_53 = reshape_30.transpose(1, 2);  reshape_30 = None\n",
      "    bmm_20 = torch.bmm(view_53, transpose_53);  view_53 = transpose_53 = None\n",
      "    softmax_10 = torch.nn.functional.softmax(bmm_20, dim = -1, _stacklevel = 3, dtype = None);  bmm_20 = None\n",
      "    dropout_41 = torch.nn.functional.dropout(softmax_10, p = 0.0, training = False, inplace = False);  softmax_10 = None\n",
      "    bmm_21 = torch.bmm(dropout_41, reshape_31);  dropout_41 = reshape_31 = None\n",
      "    view_54 = bmm_21.view(getitem_41, 12, getitem_42, 64);  bmm_21 = None\n",
      "    transpose_54 = view_54.transpose(1, 2);  view_54 = None\n",
      "    reshape_32 = transpose_54.reshape(getitem_41, getitem_42, 768);  transpose_54 = getitem_41 = getitem_42 = None\n",
      "    layers_10_self_attn_out_proj = getattr(self.layers, \"10\").self_attn.out_proj(reshape_32);  reshape_32 = None\n",
      "    dropout_42 = torch.nn.functional.dropout(layers_10_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_10_self_attn_out_proj = None\n",
      "    add_21 = add_20 + dropout_42;  add_20 = dropout_42 = None\n",
      "    layers_10_final_layer_norm = getattr(self.layers, \"10\").final_layer_norm(add_21)\n",
      "    layers_10_fc1 = getattr(self.layers, \"10\").fc1(layers_10_final_layer_norm);  layers_10_final_layer_norm = None\n",
      "    gelu_12 = torch._C._nn.gelu(layers_10_fc1);  layers_10_fc1 = None\n",
      "    dropout_43 = torch.nn.functional.dropout(gelu_12, p = 0.0, training = False, inplace = False);  gelu_12 = None\n",
      "    layers_10_fc2 = getattr(self.layers, \"10\").fc2(dropout_43);  dropout_43 = None\n",
      "    dropout_44 = torch.nn.functional.dropout(layers_10_fc2, p = 0.0, training = False, inplace = False);  layers_10_fc2 = None\n",
      "    add_22 = add_21 + dropout_44;  add_21 = dropout_44 = None\n",
      "    getitem_44 = head_mask[11];  head_mask = None\n",
      "    layers_11_self_attn_layer_norm = getattr(self.layers, \"11\").self_attn_layer_norm(add_22)\n",
      "    size_22 = layers_11_self_attn_layer_norm.size()\n",
      "    getitem_45 = size_22[0]\n",
      "    getitem_46 = size_22[1]\n",
      "    getitem_47 = size_22[2];  size_22 = None\n",
      "    layers_11_self_attn_q_proj = getattr(self.layers, \"11\").self_attn.q_proj(layers_11_self_attn_layer_norm)\n",
      "    mul_22 = layers_11_self_attn_q_proj * 0.125;  layers_11_self_attn_q_proj = None\n",
      "    layers_11_self_attn_k_proj = getattr(self.layers, \"11\").self_attn.k_proj(layers_11_self_attn_layer_norm)\n",
      "    view_55 = layers_11_self_attn_k_proj.view(getitem_45, -1, 12, 64);  layers_11_self_attn_k_proj = None\n",
      "    transpose_55 = view_55.transpose(1, 2);  view_55 = None\n",
      "    contiguous_33 = transpose_55.contiguous();  transpose_55 = None\n",
      "    layers_11_self_attn_v_proj = getattr(self.layers, \"11\").self_attn.v_proj(layers_11_self_attn_layer_norm);  layers_11_self_attn_layer_norm = None\n",
      "    view_56 = layers_11_self_attn_v_proj.view(getitem_45, -1, 12, 64);  layers_11_self_attn_v_proj = None\n",
      "    transpose_56 = view_56.transpose(1, 2);  view_56 = None\n",
      "    contiguous_34 = transpose_56.contiguous();  transpose_56 = None\n",
      "    mul_23 = getitem_45 * 12\n",
      "    view_57 = mul_22.view(getitem_45, getitem_46, 12, 64);  mul_22 = None\n",
      "    transpose_57 = view_57.transpose(1, 2);  view_57 = None\n",
      "    contiguous_35 = transpose_57.contiguous();  transpose_57 = None\n",
      "    view_58 = contiguous_35.view(mul_23, -1, 64);  contiguous_35 = None\n",
      "    reshape_33 = contiguous_33.reshape(mul_23, -1, 64);  contiguous_33 = None\n",
      "    reshape_34 = contiguous_34.reshape(mul_23, -1, 64);  contiguous_34 = mul_23 = None\n",
      "    size_23 = reshape_33.size(1)\n",
      "    transpose_58 = reshape_33.transpose(1, 2);  reshape_33 = None\n",
      "    bmm_22 = torch.bmm(view_58, transpose_58);  view_58 = transpose_58 = None\n",
      "    softmax_11 = torch.nn.functional.softmax(bmm_22, dim = -1, _stacklevel = 3, dtype = None);  bmm_22 = None\n",
      "    dropout_45 = torch.nn.functional.dropout(softmax_11, p = 0.0, training = False, inplace = False);  softmax_11 = None\n",
      "    bmm_23 = torch.bmm(dropout_45, reshape_34);  dropout_45 = reshape_34 = None\n",
      "    view_59 = bmm_23.view(getitem_45, 12, getitem_46, 64);  bmm_23 = None\n",
      "    transpose_59 = view_59.transpose(1, 2);  view_59 = None\n",
      "    reshape_35 = transpose_59.reshape(getitem_45, getitem_46, 768);  transpose_59 = getitem_45 = getitem_46 = None\n",
      "    layers_11_self_attn_out_proj = getattr(self.layers, \"11\").self_attn.out_proj(reshape_35);  reshape_35 = None\n",
      "    dropout_46 = torch.nn.functional.dropout(layers_11_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_11_self_attn_out_proj = None\n",
      "    add_23 = add_22 + dropout_46;  add_22 = dropout_46 = None\n",
      "    layers_11_final_layer_norm = getattr(self.layers, \"11\").final_layer_norm(add_23)\n",
      "    layers_11_fc1 = getattr(self.layers, \"11\").fc1(layers_11_final_layer_norm);  layers_11_final_layer_norm = None\n",
      "    gelu_13 = torch._C._nn.gelu(layers_11_fc1);  layers_11_fc1 = None\n",
      "    dropout_47 = torch.nn.functional.dropout(gelu_13, p = 0.0, training = False, inplace = False);  gelu_13 = None\n",
      "    layers_11_fc2 = getattr(self.layers, \"11\").fc2(dropout_47);  dropout_47 = None\n",
      "    dropout_48 = torch.nn.functional.dropout(layers_11_fc2, p = 0.0, training = False, inplace = False);  layers_11_fc2 = None\n",
      "    add_24 = add_23 + dropout_48;  add_23 = dropout_48 = None\n",
      "    layer_norm = self.layer_norm(add_24);  add_24 = None\n",
      "    return {'last_hidden_state': layer_norm}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(symbolic_traced.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48955666",
   "metadata": {},
   "outputs": [
    {
     "ename": "TraceError",
     "evalue": "symbolically traced variables cannot be used as inputs to control flow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTraceError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m symbolic_traced : torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule \u001b[38;5;241m=\u001b[39m \u001b[43msymbolic_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:1109\u001b[0m, in \u001b[0;36msymbolic_trace\u001b[0;34m(root, concrete_args)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03mSymbolic tracing API\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03m    GraphModule: a Module created from the recorded operations from ``root``.\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m tracer \u001b[38;5;241m=\u001b[39m Tracer()\n\u001b[0;32m-> 1109\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1110\u001b[0m name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1111\u001b[0m     root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(root, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m root\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1112\u001b[0m )\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m GraphModule(tracer\u001b[38;5;241m.\u001b[39mroot, graph, name)\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:778\u001b[0m, in \u001b[0;36mTracer.trace\u001b[0;34m(self, root, concrete_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_search:\n\u001b[1;32m    772\u001b[0m             _autowrap_check(\n\u001b[1;32m    773\u001b[0m                 patcher, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids\n\u001b[1;32m    774\u001b[0m             )\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_node(\n\u001b[1;32m    776\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    777\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 778\u001b[0m             (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[1;32m    779\u001b[0m             {},\n\u001b[1;32m    780\u001b[0m             type_expr\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    781\u001b[0m         )\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmodule_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1505\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1501\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1502\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1503\u001b[0m         )\n\u001b[0;32m-> 1505\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m### 1\u001b[39;49;00m\n\u001b[1;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1523\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:756\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _orig_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    751\u001b[0m _autowrap_check(\n\u001b[1;32m    752\u001b[0m     patcher,\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    755\u001b[0m )\n\u001b[0;32m--> 756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:467\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_stack[_scope\u001b[38;5;241m.\u001b[39mmodule_path] \u001b[38;5;241m=\u001b[39m _scope\u001b[38;5;241m.\u001b[39mmodule_type\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 467\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:749\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1377\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1355\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;66;03m#if encoder_outputs is None:  ## not required for inference (without generate)\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m     \u001b[38;5;66;03m#input_features = self._mask_input_features(input_features, attention_mask=attention_mask)\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#inputs_embeds=decoder_inputs_embeds,  # disabled for tracing, input_ids used for inference\u001b[39;49;00m\n\u001b[1;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;66;03m#if not return_dict:  # True for inference\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m     \u001b[38;5;66;03m#return decoder_outputs + encoder_outputs\u001b[39;00m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Seq2SeqModelOutput(\n\u001b[1;32m   1395\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mdecoder_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state,\n\u001b[1;32m   1396\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mdecoder_outputs\u001b[38;5;241m.\u001b[39mpast_key_values,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1402\u001b[0m     encoder_attentions\u001b[38;5;241m=\u001b[39mencoder_outputs\u001b[38;5;241m.\u001b[39mattentions,\n\u001b[1;32m   1403\u001b[0m )\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:756\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _orig_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    751\u001b[0m _autowrap_check(\n\u001b[1;32m    752\u001b[0m     patcher,\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    755\u001b[0m )\n\u001b[0;32m--> 756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:467\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_stack[_scope\u001b[38;5;241m.\u001b[39mmodule_path] \u001b[38;5;241m=\u001b[39m _scope\u001b[38;5;241m.\u001b[39mmodule_type\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 467\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:749\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1193\u001b[0m, in \u001b[0;36mWhisperDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1171\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[idx] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;66;03m#if self.gradient_checkpointing and self.training:\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;66;03m#def create_custom_forward(module):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;66;03m#else:\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m## else ends\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;66;03m#if use_cache:  ## True\u001b[39;00m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:756\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _orig_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    751\u001b[0m _autowrap_check(\n\u001b[1;32m    752\u001b[0m     patcher,\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    755\u001b[0m )\n\u001b[0;32m--> 756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:467\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_stack[_scope\u001b[38;5;241m.\u001b[39mmodule_path] \u001b[38;5;241m=\u001b[39m _scope\u001b[38;5;241m.\u001b[39mmodule_type\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 467\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:749\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:622\u001b[0m, in \u001b[0;36mWhisperDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;66;03m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m    621\u001b[0m cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    631\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:756\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _orig_module_call(mod, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    751\u001b[0m _autowrap_check(\n\u001b[1;32m    752\u001b[0m     patcher,\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, mod), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__globals__\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}),\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autowrap_function_ids,\n\u001b[1;32m    755\u001b[0m )\n\u001b[0;32m--> 756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:467\u001b[0m, in \u001b[0;36mTracer.call_module\u001b[0;34m(self, m, forward, args, kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_stack[_scope\u001b[38;5;241m.\u001b[39mmodule_path] \u001b[38;5;241m=\u001b[39m _scope\u001b[38;5;241m.\u001b[39mmodule_type\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_leaf_module(m, module_qualified_name):\n\u001b[0;32m--> 467\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     ret_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, module_qualified_name, args, kwargs)\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:749\u001b[0m, in \u001b[0;36mTracer.trace.<locals>.module_call_wrapper.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_module_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:372\u001b[0m, in \u001b[0;36mWhisperAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    364\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# get key, value proj\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# `past_key_value[0].shape[2] == key_value_states.shape[1]`\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# is checking that the `sequence_length` of the `past_key_value` is the same as\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# the provided `key_value_states` to support prefix tuning\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    370\u001b[0m     is_cross_attention\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m past_key_value[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m key_value_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    373\u001b[0m ):\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;66;03m# reuse k,v, cross_attentions\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    376\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/proxy.py:413\u001b[0m, in \u001b[0;36mProxy.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mcreate_proxy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m'\u001b[39m, assert_fn, (\u001b[38;5;28mself\u001b[39m,), {})\n\u001b[1;32m    411\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bool\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/fx/proxy.py:276\u001b[0m, in \u001b[0;36mTracerBase.to_bool\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;129m@compatibility\u001b[39m(is_backward_compatible\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_bool\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProxy\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called when a proxy object is being converted to a boolean, such as\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m    when used in control flow.  Normally we don't know what to do because\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    we don't know the value of the proxy, but a custom tracer can attach more\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    information to the graph node using create_node and can choose to return a value.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TraceError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msymbolically traced variables cannot be used as inputs to control flow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTraceError\u001b[0m: symbolically traced variables cannot be used as inputs to control flow"
     ]
    }
   ],
   "source": [
    "symbolic_traced : torch.fx.GraphModule = symbolic_trace(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bdb99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
