{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd662fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ujan/anaconda3/envs/asr/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda112.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ujan/anaconda3/envs/asr/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 112\n",
      "CUDA SETUP: Loading binary /home/ujan/anaconda3/envs/asr/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda112.so...\n"
     ]
    }
   ],
   "source": [
    "#from transformers.utils.fx import symbolic_trace\n",
    "from transformers import AutoConfig, AutoFeatureExtractor, AutoTokenizer, AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from modeling_whisper_traceable import WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.fx import symbolic_trace\n",
    "import torch.fx as fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de1f6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers.utils.fx import symbolic_trace\n",
    "from transformers import AutoConfig, AutoFeatureExtractor, AutoTokenizer, AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from modeling_whisper_traceable import WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name_or_path = 'openai/whisper-small'\n",
    "data_dir = 'mozilla-foundation/common_voice_11_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93fa02cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset from mozilla-foundation/common_voice_11_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ujan/speech-processing/experiments/notebooks/modeling_whisper_traceable.py:1467: UserWarning: disabled check : \n",
      "                attn_weights.size() == (bsz * self.num_heads, tgt_len, src_len) \n",
      "            in WhisperAttention.forward\n",
      "  warnings.warn(\n",
      "/home/ujan/speech-processing/experiments/notebooks/modeling_whisper_traceable.py:1472: UserWarning: disabled check :\n",
      "                attn_output.size() == (bsz * self.num_heads, tgt_len, self.head_dim)\n",
      "            in WhisperAttention.forward\n",
      "  warnings.warn(\n",
      "/home/ujan/speech-processing/experiments/notebooks/modeling_whisper_traceable.py:1477: UserWarning: if passing in a tuple for encoder_outputs, wrap it in a BaseModelOutput when return_dict=True\n",
      "                before passing through model. As :\n",
      "                    encoder_outputs = BaseModelOutput(\n",
      "                    last_hidden_state=encoder_outputs[0],\n",
      "                    hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
      "                    attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
      "            )\n",
      "  warnings.warn(\n",
      "/home/ujan/speech-processing/experiments/notebooks/modeling_whisper_traceable.py:1486: UserWarning: clamping disabled in WhisperEncoderLayer.forward\n",
      "  warnings.warn('clamping disabled in WhisperEncoderLayer.forward')\n",
      "/home/ujan/speech-processing/experiments/notebooks/modeling_whisper_traceable.py:1487: UserWarning: disabled check : \n",
      "                if head_mask/cross_attn_head_mask has a correct number of layers specified\n",
      "            in WhisperDecoder.forward\n",
      "  warnings.warn(\n",
      "/home/ujan/speech-processing/experiments/notebooks/modeling_whisper_traceable.py:1492: UserWarning: disabled check : \n",
      "                attention_mask.size() == (bsz, 1, tgt_len, src_len))\n",
      "            in WhisperAttention.forward\n",
      "  warnings.warn(\n",
      "/home/ujan/speech-processing/experiments/notebooks/modeling_whisper_traceable.py:1497: UserWarning: disabled check : \n",
      "                layer_head_mask.size() != (self.num_heads,)\n",
      "            in WhisperAttention.forward\n",
      "  warnings.warn(\n",
      "/home/ujan/speech-processing/experiments/notebooks/modeling_whisper_traceable.py:1502: UserWarning: prefix tuning disabled in WhisperAttention.forward\n",
      "  warnings.warn('prefix tuning disabled in WhisperAttention.forward')\n",
      "Reading metadata...: 10581it [00:02, 3761.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "print('loading dataset from {}'.format(data_dir))\n",
    "\n",
    "raw_datasets = load_dataset(data_dir, \"zh-CN\", split=\"test\", streaming=True)\n",
    "text_column_name = 'sentence'\n",
    "\n",
    "\n",
    "# model, tokenizer, feature extractor, processor\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    #cache_dir=args.cache_dir,\n",
    "    #revision=args.model_revision,\n",
    "    #use_auth_token=True if args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "model_config.update({\"forced_decoder_ids\": [], \"suppress_tokens\": []})\n",
    "\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    #cache_dir=args.cache_dir,\n",
    "    #revision=args.model_revision,\n",
    "    #use_auth_token=True if args.use_auth_token else None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    #cache_dir=args.cache_dir,\n",
    "    #use_fast=model_args.use_fast_tokenizer,\n",
    "    #revision=model_args.model_revision,\n",
    "    #use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "tokenizer.set_prefix_tokens(language='chinese', task='transcribe')\n",
    "\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    config=model_config,\n",
    "    #cache_dir=args.cache_dir,\n",
    "    #revision=args.model_revision,\n",
    "    #use_auth_token=True if args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "    \n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language='chinese', task='transcribe')\n",
    "\n",
    "\n",
    "dataset = raw_datasets\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "sample = next(iter(dataset))\n",
    "\n",
    "\n",
    "inputs = processor(\n",
    "    sample['audio'][\"array\"],\n",
    "    sampling_rate=feature_extractor.sampling_rate,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\")\n",
    "                \n",
    "input_features = inputs.input_features\n",
    "attention_mask = inputs.attention_mask\n",
    "decoder_input_ids = torch.tensor([model.config.decoder_start_token_id]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a7a8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "265c4cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496383a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_concrete_args={\n",
    "                'output_hidden_states': False,\n",
    "                'output_attentions': False,\n",
    "                'head_mask': None,\n",
    "                'return_dict': True,\n",
    "              }\n",
    "\n",
    "decoder_concrete_args={\n",
    "                'output_hidden_states': False,\n",
    "                'output_attentions': False,\n",
    "                'return_dict': True,\n",
    "                'use_cache': True,\n",
    "                'decoder_inputs_embeds': None,\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c669c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_traced : torch.fx.GraphModule = symbolic_trace(encoder, concrete_args=encoder_concrete_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "056376e1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, input_features, attention_mask = None, head_mask_1 = None, output_attentions_1 = None, output_hidden_states_1 = None, return_dict_1 = None):\n",
      "    _assert_is_none = torch.fx._symbolic_trace._assert_is_none(head_mask_1, 'head_mask has been specialized to have value None but got another value');  head_mask_1 = None\n",
      "    eq = output_attentions_1 == False;  output_attentions_1 = None\n",
      "    _assert = torch._assert(eq, 'output_attentions has been specialized to have value False but got another value');  eq = None\n",
      "    eq_1 = output_hidden_states_1 == False;  output_hidden_states_1 = None\n",
      "    _assert_1 = torch._assert(eq_1, 'output_hidden_states has been specialized to have value False but got another value');  eq_1 = None\n",
      "    eq_2 = return_dict_1 == True;  return_dict_1 = None\n",
      "    _assert_2 = torch._assert(eq_2, 'return_dict has been specialized to have value True but got another value');  eq_2 = None\n",
      "    conv1 = self.conv1(input_features);  input_features = None\n",
      "    gelu = torch._C._nn.gelu(conv1);  conv1 = None\n",
      "    conv2 = self.conv2(gelu);  gelu = None\n",
      "    gelu_1 = torch._C._nn.gelu(conv2);  conv2 = None\n",
      "    permute = gelu_1.permute(0, 2, 1);  gelu_1 = None\n",
      "    embed_positions_weight = self.embed_positions.weight\n",
      "    add = permute + embed_positions_weight;  permute = embed_positions_weight = None\n",
      "    dropout = torch.nn.functional.dropout(add, p = 0.0, training = False, inplace = False);  add = None\n",
      "    layers_0_self_attn_layer_norm = getattr(self.layers, \"0\").self_attn_layer_norm(dropout)\n",
      "    size = layers_0_self_attn_layer_norm.size()\n",
      "    getitem = size[0]\n",
      "    getitem_1 = size[1]\n",
      "    getitem_2 = size[2];  size = None\n",
      "    layers_0_self_attn_q_proj = getattr(self.layers, \"0\").self_attn.q_proj(layers_0_self_attn_layer_norm)\n",
      "    mul = layers_0_self_attn_q_proj * 0.125;  layers_0_self_attn_q_proj = None\n",
      "    layers_0_self_attn_k_proj = getattr(self.layers, \"0\").self_attn.k_proj(layers_0_self_attn_layer_norm)\n",
      "    view = layers_0_self_attn_k_proj.view(getitem, -1, 12, 64);  layers_0_self_attn_k_proj = None\n",
      "    transpose = view.transpose(1, 2);  view = None\n",
      "    contiguous = transpose.contiguous();  transpose = None\n",
      "    layers_0_self_attn_v_proj = getattr(self.layers, \"0\").self_attn.v_proj(layers_0_self_attn_layer_norm);  layers_0_self_attn_layer_norm = None\n",
      "    view_1 = layers_0_self_attn_v_proj.view(getitem, -1, 12, 64);  layers_0_self_attn_v_proj = None\n",
      "    transpose_1 = view_1.transpose(1, 2);  view_1 = None\n",
      "    contiguous_1 = transpose_1.contiguous();  transpose_1 = None\n",
      "    mul_1 = getitem * 12\n",
      "    view_2 = mul.view(getitem, getitem_1, 12, 64);  mul = None\n",
      "    transpose_2 = view_2.transpose(1, 2);  view_2 = None\n",
      "    contiguous_2 = transpose_2.contiguous();  transpose_2 = None\n",
      "    view_3 = contiguous_2.view(mul_1, -1, 64);  contiguous_2 = None\n",
      "    reshape = contiguous.reshape(mul_1, -1, 64);  contiguous = None\n",
      "    reshape_1 = contiguous_1.reshape(mul_1, -1, 64);  contiguous_1 = mul_1 = None\n",
      "    size_1 = reshape.size(1)\n",
      "    transpose_3 = reshape.transpose(1, 2);  reshape = None\n",
      "    bmm = torch.bmm(view_3, transpose_3);  view_3 = transpose_3 = None\n",
      "    softmax = torch.nn.functional.softmax(bmm, dim = -1, _stacklevel = 3, dtype = None);  bmm = None\n",
      "    dropout_1 = torch.nn.functional.dropout(softmax, p = 0.0, training = False, inplace = False);  softmax = None\n",
      "    bmm_1 = torch.bmm(dropout_1, reshape_1);  dropout_1 = reshape_1 = None\n",
      "    view_4 = bmm_1.view(getitem, 12, getitem_1, 64);  bmm_1 = None\n",
      "    transpose_4 = view_4.transpose(1, 2);  view_4 = None\n",
      "    reshape_2 = transpose_4.reshape(getitem, getitem_1, 768);  transpose_4 = getitem = getitem_1 = None\n",
      "    layers_0_self_attn_out_proj = getattr(self.layers, \"0\").self_attn.out_proj(reshape_2);  reshape_2 = None\n",
      "    dropout_2 = torch.nn.functional.dropout(layers_0_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_0_self_attn_out_proj = None\n",
      "    add_1 = dropout + dropout_2;  dropout = dropout_2 = None\n",
      "    layers_0_final_layer_norm = getattr(self.layers, \"0\").final_layer_norm(add_1)\n",
      "    layers_0_fc1 = getattr(self.layers, \"0\").fc1(layers_0_final_layer_norm);  layers_0_final_layer_norm = None\n",
      "    gelu_2 = torch._C._nn.gelu(layers_0_fc1);  layers_0_fc1 = None\n",
      "    dropout_3 = torch.nn.functional.dropout(gelu_2, p = 0.0, training = False, inplace = False);  gelu_2 = None\n",
      "    layers_0_fc2 = getattr(self.layers, \"0\").fc2(dropout_3);  dropout_3 = None\n",
      "    dropout_4 = torch.nn.functional.dropout(layers_0_fc2, p = 0.0, training = False, inplace = False);  layers_0_fc2 = None\n",
      "    add_2 = add_1 + dropout_4;  add_1 = dropout_4 = None\n",
      "    layers_1_self_attn_layer_norm = getattr(self.layers, \"1\").self_attn_layer_norm(add_2)\n",
      "    size_2 = layers_1_self_attn_layer_norm.size()\n",
      "    getitem_3 = size_2[0]\n",
      "    getitem_4 = size_2[1]\n",
      "    getitem_5 = size_2[2];  size_2 = None\n",
      "    layers_1_self_attn_q_proj = getattr(self.layers, \"1\").self_attn.q_proj(layers_1_self_attn_layer_norm)\n",
      "    mul_2 = layers_1_self_attn_q_proj * 0.125;  layers_1_self_attn_q_proj = None\n",
      "    layers_1_self_attn_k_proj = getattr(self.layers, \"1\").self_attn.k_proj(layers_1_self_attn_layer_norm)\n",
      "    view_5 = layers_1_self_attn_k_proj.view(getitem_3, -1, 12, 64);  layers_1_self_attn_k_proj = None\n",
      "    transpose_5 = view_5.transpose(1, 2);  view_5 = None\n",
      "    contiguous_3 = transpose_5.contiguous();  transpose_5 = None\n",
      "    layers_1_self_attn_v_proj = getattr(self.layers, \"1\").self_attn.v_proj(layers_1_self_attn_layer_norm);  layers_1_self_attn_layer_norm = None\n",
      "    view_6 = layers_1_self_attn_v_proj.view(getitem_3, -1, 12, 64);  layers_1_self_attn_v_proj = None\n",
      "    transpose_6 = view_6.transpose(1, 2);  view_6 = None\n",
      "    contiguous_4 = transpose_6.contiguous();  transpose_6 = None\n",
      "    mul_3 = getitem_3 * 12\n",
      "    view_7 = mul_2.view(getitem_3, getitem_4, 12, 64);  mul_2 = None\n",
      "    transpose_7 = view_7.transpose(1, 2);  view_7 = None\n",
      "    contiguous_5 = transpose_7.contiguous();  transpose_7 = None\n",
      "    view_8 = contiguous_5.view(mul_3, -1, 64);  contiguous_5 = None\n",
      "    reshape_3 = contiguous_3.reshape(mul_3, -1, 64);  contiguous_3 = None\n",
      "    reshape_4 = contiguous_4.reshape(mul_3, -1, 64);  contiguous_4 = mul_3 = None\n",
      "    size_3 = reshape_3.size(1)\n",
      "    transpose_8 = reshape_3.transpose(1, 2);  reshape_3 = None\n",
      "    bmm_2 = torch.bmm(view_8, transpose_8);  view_8 = transpose_8 = None\n",
      "    softmax_1 = torch.nn.functional.softmax(bmm_2, dim = -1, _stacklevel = 3, dtype = None);  bmm_2 = None\n",
      "    dropout_5 = torch.nn.functional.dropout(softmax_1, p = 0.0, training = False, inplace = False);  softmax_1 = None\n",
      "    bmm_3 = torch.bmm(dropout_5, reshape_4);  dropout_5 = reshape_4 = None\n",
      "    view_9 = bmm_3.view(getitem_3, 12, getitem_4, 64);  bmm_3 = None\n",
      "    transpose_9 = view_9.transpose(1, 2);  view_9 = None\n",
      "    reshape_5 = transpose_9.reshape(getitem_3, getitem_4, 768);  transpose_9 = getitem_3 = getitem_4 = None\n",
      "    layers_1_self_attn_out_proj = getattr(self.layers, \"1\").self_attn.out_proj(reshape_5);  reshape_5 = None\n",
      "    dropout_6 = torch.nn.functional.dropout(layers_1_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_1_self_attn_out_proj = None\n",
      "    add_3 = add_2 + dropout_6;  add_2 = dropout_6 = None\n",
      "    layers_1_final_layer_norm = getattr(self.layers, \"1\").final_layer_norm(add_3)\n",
      "    layers_1_fc1 = getattr(self.layers, \"1\").fc1(layers_1_final_layer_norm);  layers_1_final_layer_norm = None\n",
      "    gelu_3 = torch._C._nn.gelu(layers_1_fc1);  layers_1_fc1 = None\n",
      "    dropout_7 = torch.nn.functional.dropout(gelu_3, p = 0.0, training = False, inplace = False);  gelu_3 = None\n",
      "    layers_1_fc2 = getattr(self.layers, \"1\").fc2(dropout_7);  dropout_7 = None\n",
      "    dropout_8 = torch.nn.functional.dropout(layers_1_fc2, p = 0.0, training = False, inplace = False);  layers_1_fc2 = None\n",
      "    add_4 = add_3 + dropout_8;  add_3 = dropout_8 = None\n",
      "    layers_2_self_attn_layer_norm = getattr(self.layers, \"2\").self_attn_layer_norm(add_4)\n",
      "    size_4 = layers_2_self_attn_layer_norm.size()\n",
      "    getitem_6 = size_4[0]\n",
      "    getitem_7 = size_4[1]\n",
      "    getitem_8 = size_4[2];  size_4 = None\n",
      "    layers_2_self_attn_q_proj = getattr(self.layers, \"2\").self_attn.q_proj(layers_2_self_attn_layer_norm)\n",
      "    mul_4 = layers_2_self_attn_q_proj * 0.125;  layers_2_self_attn_q_proj = None\n",
      "    layers_2_self_attn_k_proj = getattr(self.layers, \"2\").self_attn.k_proj(layers_2_self_attn_layer_norm)\n",
      "    view_10 = layers_2_self_attn_k_proj.view(getitem_6, -1, 12, 64);  layers_2_self_attn_k_proj = None\n",
      "    transpose_10 = view_10.transpose(1, 2);  view_10 = None\n",
      "    contiguous_6 = transpose_10.contiguous();  transpose_10 = None\n",
      "    layers_2_self_attn_v_proj = getattr(self.layers, \"2\").self_attn.v_proj(layers_2_self_attn_layer_norm);  layers_2_self_attn_layer_norm = None\n",
      "    view_11 = layers_2_self_attn_v_proj.view(getitem_6, -1, 12, 64);  layers_2_self_attn_v_proj = None\n",
      "    transpose_11 = view_11.transpose(1, 2);  view_11 = None\n",
      "    contiguous_7 = transpose_11.contiguous();  transpose_11 = None\n",
      "    mul_5 = getitem_6 * 12\n",
      "    view_12 = mul_4.view(getitem_6, getitem_7, 12, 64);  mul_4 = None\n",
      "    transpose_12 = view_12.transpose(1, 2);  view_12 = None\n",
      "    contiguous_8 = transpose_12.contiguous();  transpose_12 = None\n",
      "    view_13 = contiguous_8.view(mul_5, -1, 64);  contiguous_8 = None\n",
      "    reshape_6 = contiguous_6.reshape(mul_5, -1, 64);  contiguous_6 = None\n",
      "    reshape_7 = contiguous_7.reshape(mul_5, -1, 64);  contiguous_7 = mul_5 = None\n",
      "    size_5 = reshape_6.size(1)\n",
      "    transpose_13 = reshape_6.transpose(1, 2);  reshape_6 = None\n",
      "    bmm_4 = torch.bmm(view_13, transpose_13);  view_13 = transpose_13 = None\n",
      "    softmax_2 = torch.nn.functional.softmax(bmm_4, dim = -1, _stacklevel = 3, dtype = None);  bmm_4 = None\n",
      "    dropout_9 = torch.nn.functional.dropout(softmax_2, p = 0.0, training = False, inplace = False);  softmax_2 = None\n",
      "    bmm_5 = torch.bmm(dropout_9, reshape_7);  dropout_9 = reshape_7 = None\n",
      "    view_14 = bmm_5.view(getitem_6, 12, getitem_7, 64);  bmm_5 = None\n",
      "    transpose_14 = view_14.transpose(1, 2);  view_14 = None\n",
      "    reshape_8 = transpose_14.reshape(getitem_6, getitem_7, 768);  transpose_14 = getitem_6 = getitem_7 = None\n",
      "    layers_2_self_attn_out_proj = getattr(self.layers, \"2\").self_attn.out_proj(reshape_8);  reshape_8 = None\n",
      "    dropout_10 = torch.nn.functional.dropout(layers_2_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_2_self_attn_out_proj = None\n",
      "    add_5 = add_4 + dropout_10;  add_4 = dropout_10 = None\n",
      "    layers_2_final_layer_norm = getattr(self.layers, \"2\").final_layer_norm(add_5)\n",
      "    layers_2_fc1 = getattr(self.layers, \"2\").fc1(layers_2_final_layer_norm);  layers_2_final_layer_norm = None\n",
      "    gelu_4 = torch._C._nn.gelu(layers_2_fc1);  layers_2_fc1 = None\n",
      "    dropout_11 = torch.nn.functional.dropout(gelu_4, p = 0.0, training = False, inplace = False);  gelu_4 = None\n",
      "    layers_2_fc2 = getattr(self.layers, \"2\").fc2(dropout_11);  dropout_11 = None\n",
      "    dropout_12 = torch.nn.functional.dropout(layers_2_fc2, p = 0.0, training = False, inplace = False);  layers_2_fc2 = None\n",
      "    add_6 = add_5 + dropout_12;  add_5 = dropout_12 = None\n",
      "    layers_3_self_attn_layer_norm = getattr(self.layers, \"3\").self_attn_layer_norm(add_6)\n",
      "    size_6 = layers_3_self_attn_layer_norm.size()\n",
      "    getitem_9 = size_6[0]\n",
      "    getitem_10 = size_6[1]\n",
      "    getitem_11 = size_6[2];  size_6 = None\n",
      "    layers_3_self_attn_q_proj = getattr(self.layers, \"3\").self_attn.q_proj(layers_3_self_attn_layer_norm)\n",
      "    mul_6 = layers_3_self_attn_q_proj * 0.125;  layers_3_self_attn_q_proj = None\n",
      "    layers_3_self_attn_k_proj = getattr(self.layers, \"3\").self_attn.k_proj(layers_3_self_attn_layer_norm)\n",
      "    view_15 = layers_3_self_attn_k_proj.view(getitem_9, -1, 12, 64);  layers_3_self_attn_k_proj = None\n",
      "    transpose_15 = view_15.transpose(1, 2);  view_15 = None\n",
      "    contiguous_9 = transpose_15.contiguous();  transpose_15 = None\n",
      "    layers_3_self_attn_v_proj = getattr(self.layers, \"3\").self_attn.v_proj(layers_3_self_attn_layer_norm);  layers_3_self_attn_layer_norm = None\n",
      "    view_16 = layers_3_self_attn_v_proj.view(getitem_9, -1, 12, 64);  layers_3_self_attn_v_proj = None\n",
      "    transpose_16 = view_16.transpose(1, 2);  view_16 = None\n",
      "    contiguous_10 = transpose_16.contiguous();  transpose_16 = None\n",
      "    mul_7 = getitem_9 * 12\n",
      "    view_17 = mul_6.view(getitem_9, getitem_10, 12, 64);  mul_6 = None\n",
      "    transpose_17 = view_17.transpose(1, 2);  view_17 = None\n",
      "    contiguous_11 = transpose_17.contiguous();  transpose_17 = None\n",
      "    view_18 = contiguous_11.view(mul_7, -1, 64);  contiguous_11 = None\n",
      "    reshape_9 = contiguous_9.reshape(mul_7, -1, 64);  contiguous_9 = None\n",
      "    reshape_10 = contiguous_10.reshape(mul_7, -1, 64);  contiguous_10 = mul_7 = None\n",
      "    size_7 = reshape_9.size(1)\n",
      "    transpose_18 = reshape_9.transpose(1, 2);  reshape_9 = None\n",
      "    bmm_6 = torch.bmm(view_18, transpose_18);  view_18 = transpose_18 = None\n",
      "    softmax_3 = torch.nn.functional.softmax(bmm_6, dim = -1, _stacklevel = 3, dtype = None);  bmm_6 = None\n",
      "    dropout_13 = torch.nn.functional.dropout(softmax_3, p = 0.0, training = False, inplace = False);  softmax_3 = None\n",
      "    bmm_7 = torch.bmm(dropout_13, reshape_10);  dropout_13 = reshape_10 = None\n",
      "    view_19 = bmm_7.view(getitem_9, 12, getitem_10, 64);  bmm_7 = None\n",
      "    transpose_19 = view_19.transpose(1, 2);  view_19 = None\n",
      "    reshape_11 = transpose_19.reshape(getitem_9, getitem_10, 768);  transpose_19 = getitem_9 = getitem_10 = None\n",
      "    layers_3_self_attn_out_proj = getattr(self.layers, \"3\").self_attn.out_proj(reshape_11);  reshape_11 = None\n",
      "    dropout_14 = torch.nn.functional.dropout(layers_3_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_3_self_attn_out_proj = None\n",
      "    add_7 = add_6 + dropout_14;  add_6 = dropout_14 = None\n",
      "    layers_3_final_layer_norm = getattr(self.layers, \"3\").final_layer_norm(add_7)\n",
      "    layers_3_fc1 = getattr(self.layers, \"3\").fc1(layers_3_final_layer_norm);  layers_3_final_layer_norm = None\n",
      "    gelu_5 = torch._C._nn.gelu(layers_3_fc1);  layers_3_fc1 = None\n",
      "    dropout_15 = torch.nn.functional.dropout(gelu_5, p = 0.0, training = False, inplace = False);  gelu_5 = None\n",
      "    layers_3_fc2 = getattr(self.layers, \"3\").fc2(dropout_15);  dropout_15 = None\n",
      "    dropout_16 = torch.nn.functional.dropout(layers_3_fc2, p = 0.0, training = False, inplace = False);  layers_3_fc2 = None\n",
      "    add_8 = add_7 + dropout_16;  add_7 = dropout_16 = None\n",
      "    layers_4_self_attn_layer_norm = getattr(self.layers, \"4\").self_attn_layer_norm(add_8)\n",
      "    size_8 = layers_4_self_attn_layer_norm.size()\n",
      "    getitem_12 = size_8[0]\n",
      "    getitem_13 = size_8[1]\n",
      "    getitem_14 = size_8[2];  size_8 = None\n",
      "    layers_4_self_attn_q_proj = getattr(self.layers, \"4\").self_attn.q_proj(layers_4_self_attn_layer_norm)\n",
      "    mul_8 = layers_4_self_attn_q_proj * 0.125;  layers_4_self_attn_q_proj = None\n",
      "    layers_4_self_attn_k_proj = getattr(self.layers, \"4\").self_attn.k_proj(layers_4_self_attn_layer_norm)\n",
      "    view_20 = layers_4_self_attn_k_proj.view(getitem_12, -1, 12, 64);  layers_4_self_attn_k_proj = None\n",
      "    transpose_20 = view_20.transpose(1, 2);  view_20 = None\n",
      "    contiguous_12 = transpose_20.contiguous();  transpose_20 = None\n",
      "    layers_4_self_attn_v_proj = getattr(self.layers, \"4\").self_attn.v_proj(layers_4_self_attn_layer_norm);  layers_4_self_attn_layer_norm = None\n",
      "    view_21 = layers_4_self_attn_v_proj.view(getitem_12, -1, 12, 64);  layers_4_self_attn_v_proj = None\n",
      "    transpose_21 = view_21.transpose(1, 2);  view_21 = None\n",
      "    contiguous_13 = transpose_21.contiguous();  transpose_21 = None\n",
      "    mul_9 = getitem_12 * 12\n",
      "    view_22 = mul_8.view(getitem_12, getitem_13, 12, 64);  mul_8 = None\n",
      "    transpose_22 = view_22.transpose(1, 2);  view_22 = None\n",
      "    contiguous_14 = transpose_22.contiguous();  transpose_22 = None\n",
      "    view_23 = contiguous_14.view(mul_9, -1, 64);  contiguous_14 = None\n",
      "    reshape_12 = contiguous_12.reshape(mul_9, -1, 64);  contiguous_12 = None\n",
      "    reshape_13 = contiguous_13.reshape(mul_9, -1, 64);  contiguous_13 = mul_9 = None\n",
      "    size_9 = reshape_12.size(1)\n",
      "    transpose_23 = reshape_12.transpose(1, 2);  reshape_12 = None\n",
      "    bmm_8 = torch.bmm(view_23, transpose_23);  view_23 = transpose_23 = None\n",
      "    softmax_4 = torch.nn.functional.softmax(bmm_8, dim = -1, _stacklevel = 3, dtype = None);  bmm_8 = None\n",
      "    dropout_17 = torch.nn.functional.dropout(softmax_4, p = 0.0, training = False, inplace = False);  softmax_4 = None\n",
      "    bmm_9 = torch.bmm(dropout_17, reshape_13);  dropout_17 = reshape_13 = None\n",
      "    view_24 = bmm_9.view(getitem_12, 12, getitem_13, 64);  bmm_9 = None\n",
      "    transpose_24 = view_24.transpose(1, 2);  view_24 = None\n",
      "    reshape_14 = transpose_24.reshape(getitem_12, getitem_13, 768);  transpose_24 = getitem_12 = getitem_13 = None\n",
      "    layers_4_self_attn_out_proj = getattr(self.layers, \"4\").self_attn.out_proj(reshape_14);  reshape_14 = None\n",
      "    dropout_18 = torch.nn.functional.dropout(layers_4_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_4_self_attn_out_proj = None\n",
      "    add_9 = add_8 + dropout_18;  add_8 = dropout_18 = None\n",
      "    layers_4_final_layer_norm = getattr(self.layers, \"4\").final_layer_norm(add_9)\n",
      "    layers_4_fc1 = getattr(self.layers, \"4\").fc1(layers_4_final_layer_norm);  layers_4_final_layer_norm = None\n",
      "    gelu_6 = torch._C._nn.gelu(layers_4_fc1);  layers_4_fc1 = None\n",
      "    dropout_19 = torch.nn.functional.dropout(gelu_6, p = 0.0, training = False, inplace = False);  gelu_6 = None\n",
      "    layers_4_fc2 = getattr(self.layers, \"4\").fc2(dropout_19);  dropout_19 = None\n",
      "    dropout_20 = torch.nn.functional.dropout(layers_4_fc2, p = 0.0, training = False, inplace = False);  layers_4_fc2 = None\n",
      "    add_10 = add_9 + dropout_20;  add_9 = dropout_20 = None\n",
      "    layers_5_self_attn_layer_norm = getattr(self.layers, \"5\").self_attn_layer_norm(add_10)\n",
      "    size_10 = layers_5_self_attn_layer_norm.size()\n",
      "    getitem_15 = size_10[0]\n",
      "    getitem_16 = size_10[1]\n",
      "    getitem_17 = size_10[2];  size_10 = None\n",
      "    layers_5_self_attn_q_proj = getattr(self.layers, \"5\").self_attn.q_proj(layers_5_self_attn_layer_norm)\n",
      "    mul_10 = layers_5_self_attn_q_proj * 0.125;  layers_5_self_attn_q_proj = None\n",
      "    layers_5_self_attn_k_proj = getattr(self.layers, \"5\").self_attn.k_proj(layers_5_self_attn_layer_norm)\n",
      "    view_25 = layers_5_self_attn_k_proj.view(getitem_15, -1, 12, 64);  layers_5_self_attn_k_proj = None\n",
      "    transpose_25 = view_25.transpose(1, 2);  view_25 = None\n",
      "    contiguous_15 = transpose_25.contiguous();  transpose_25 = None\n",
      "    layers_5_self_attn_v_proj = getattr(self.layers, \"5\").self_attn.v_proj(layers_5_self_attn_layer_norm);  layers_5_self_attn_layer_norm = None\n",
      "    view_26 = layers_5_self_attn_v_proj.view(getitem_15, -1, 12, 64);  layers_5_self_attn_v_proj = None\n",
      "    transpose_26 = view_26.transpose(1, 2);  view_26 = None\n",
      "    contiguous_16 = transpose_26.contiguous();  transpose_26 = None\n",
      "    mul_11 = getitem_15 * 12\n",
      "    view_27 = mul_10.view(getitem_15, getitem_16, 12, 64);  mul_10 = None\n",
      "    transpose_27 = view_27.transpose(1, 2);  view_27 = None\n",
      "    contiguous_17 = transpose_27.contiguous();  transpose_27 = None\n",
      "    view_28 = contiguous_17.view(mul_11, -1, 64);  contiguous_17 = None\n",
      "    reshape_15 = contiguous_15.reshape(mul_11, -1, 64);  contiguous_15 = None\n",
      "    reshape_16 = contiguous_16.reshape(mul_11, -1, 64);  contiguous_16 = mul_11 = None\n",
      "    size_11 = reshape_15.size(1)\n",
      "    transpose_28 = reshape_15.transpose(1, 2);  reshape_15 = None\n",
      "    bmm_10 = torch.bmm(view_28, transpose_28);  view_28 = transpose_28 = None\n",
      "    softmax_5 = torch.nn.functional.softmax(bmm_10, dim = -1, _stacklevel = 3, dtype = None);  bmm_10 = None\n",
      "    dropout_21 = torch.nn.functional.dropout(softmax_5, p = 0.0, training = False, inplace = False);  softmax_5 = None\n",
      "    bmm_11 = torch.bmm(dropout_21, reshape_16);  dropout_21 = reshape_16 = None\n",
      "    view_29 = bmm_11.view(getitem_15, 12, getitem_16, 64);  bmm_11 = None\n",
      "    transpose_29 = view_29.transpose(1, 2);  view_29 = None\n",
      "    reshape_17 = transpose_29.reshape(getitem_15, getitem_16, 768);  transpose_29 = getitem_15 = getitem_16 = None\n",
      "    layers_5_self_attn_out_proj = getattr(self.layers, \"5\").self_attn.out_proj(reshape_17);  reshape_17 = None\n",
      "    dropout_22 = torch.nn.functional.dropout(layers_5_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_5_self_attn_out_proj = None\n",
      "    add_11 = add_10 + dropout_22;  add_10 = dropout_22 = None\n",
      "    layers_5_final_layer_norm = getattr(self.layers, \"5\").final_layer_norm(add_11)\n",
      "    layers_5_fc1 = getattr(self.layers, \"5\").fc1(layers_5_final_layer_norm);  layers_5_final_layer_norm = None\n",
      "    gelu_7 = torch._C._nn.gelu(layers_5_fc1);  layers_5_fc1 = None\n",
      "    dropout_23 = torch.nn.functional.dropout(gelu_7, p = 0.0, training = False, inplace = False);  gelu_7 = None\n",
      "    layers_5_fc2 = getattr(self.layers, \"5\").fc2(dropout_23);  dropout_23 = None\n",
      "    dropout_24 = torch.nn.functional.dropout(layers_5_fc2, p = 0.0, training = False, inplace = False);  layers_5_fc2 = None\n",
      "    add_12 = add_11 + dropout_24;  add_11 = dropout_24 = None\n",
      "    layers_6_self_attn_layer_norm = getattr(self.layers, \"6\").self_attn_layer_norm(add_12)\n",
      "    size_12 = layers_6_self_attn_layer_norm.size()\n",
      "    getitem_18 = size_12[0]\n",
      "    getitem_19 = size_12[1]\n",
      "    getitem_20 = size_12[2];  size_12 = None\n",
      "    layers_6_self_attn_q_proj = getattr(self.layers, \"6\").self_attn.q_proj(layers_6_self_attn_layer_norm)\n",
      "    mul_12 = layers_6_self_attn_q_proj * 0.125;  layers_6_self_attn_q_proj = None\n",
      "    layers_6_self_attn_k_proj = getattr(self.layers, \"6\").self_attn.k_proj(layers_6_self_attn_layer_norm)\n",
      "    view_30 = layers_6_self_attn_k_proj.view(getitem_18, -1, 12, 64);  layers_6_self_attn_k_proj = None\n",
      "    transpose_30 = view_30.transpose(1, 2);  view_30 = None\n",
      "    contiguous_18 = transpose_30.contiguous();  transpose_30 = None\n",
      "    layers_6_self_attn_v_proj = getattr(self.layers, \"6\").self_attn.v_proj(layers_6_self_attn_layer_norm);  layers_6_self_attn_layer_norm = None\n",
      "    view_31 = layers_6_self_attn_v_proj.view(getitem_18, -1, 12, 64);  layers_6_self_attn_v_proj = None\n",
      "    transpose_31 = view_31.transpose(1, 2);  view_31 = None\n",
      "    contiguous_19 = transpose_31.contiguous();  transpose_31 = None\n",
      "    mul_13 = getitem_18 * 12\n",
      "    view_32 = mul_12.view(getitem_18, getitem_19, 12, 64);  mul_12 = None\n",
      "    transpose_32 = view_32.transpose(1, 2);  view_32 = None\n",
      "    contiguous_20 = transpose_32.contiguous();  transpose_32 = None\n",
      "    view_33 = contiguous_20.view(mul_13, -1, 64);  contiguous_20 = None\n",
      "    reshape_18 = contiguous_18.reshape(mul_13, -1, 64);  contiguous_18 = None\n",
      "    reshape_19 = contiguous_19.reshape(mul_13, -1, 64);  contiguous_19 = mul_13 = None\n",
      "    size_13 = reshape_18.size(1)\n",
      "    transpose_33 = reshape_18.transpose(1, 2);  reshape_18 = None\n",
      "    bmm_12 = torch.bmm(view_33, transpose_33);  view_33 = transpose_33 = None\n",
      "    softmax_6 = torch.nn.functional.softmax(bmm_12, dim = -1, _stacklevel = 3, dtype = None);  bmm_12 = None\n",
      "    dropout_25 = torch.nn.functional.dropout(softmax_6, p = 0.0, training = False, inplace = False);  softmax_6 = None\n",
      "    bmm_13 = torch.bmm(dropout_25, reshape_19);  dropout_25 = reshape_19 = None\n",
      "    view_34 = bmm_13.view(getitem_18, 12, getitem_19, 64);  bmm_13 = None\n",
      "    transpose_34 = view_34.transpose(1, 2);  view_34 = None\n",
      "    reshape_20 = transpose_34.reshape(getitem_18, getitem_19, 768);  transpose_34 = getitem_18 = getitem_19 = None\n",
      "    layers_6_self_attn_out_proj = getattr(self.layers, \"6\").self_attn.out_proj(reshape_20);  reshape_20 = None\n",
      "    dropout_26 = torch.nn.functional.dropout(layers_6_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_6_self_attn_out_proj = None\n",
      "    add_13 = add_12 + dropout_26;  add_12 = dropout_26 = None\n",
      "    layers_6_final_layer_norm = getattr(self.layers, \"6\").final_layer_norm(add_13)\n",
      "    layers_6_fc1 = getattr(self.layers, \"6\").fc1(layers_6_final_layer_norm);  layers_6_final_layer_norm = None\n",
      "    gelu_8 = torch._C._nn.gelu(layers_6_fc1);  layers_6_fc1 = None\n",
      "    dropout_27 = torch.nn.functional.dropout(gelu_8, p = 0.0, training = False, inplace = False);  gelu_8 = None\n",
      "    layers_6_fc2 = getattr(self.layers, \"6\").fc2(dropout_27);  dropout_27 = None\n",
      "    dropout_28 = torch.nn.functional.dropout(layers_6_fc2, p = 0.0, training = False, inplace = False);  layers_6_fc2 = None\n",
      "    add_14 = add_13 + dropout_28;  add_13 = dropout_28 = None\n",
      "    layers_7_self_attn_layer_norm = getattr(self.layers, \"7\").self_attn_layer_norm(add_14)\n",
      "    size_14 = layers_7_self_attn_layer_norm.size()\n",
      "    getitem_21 = size_14[0]\n",
      "    getitem_22 = size_14[1]\n",
      "    getitem_23 = size_14[2];  size_14 = None\n",
      "    layers_7_self_attn_q_proj = getattr(self.layers, \"7\").self_attn.q_proj(layers_7_self_attn_layer_norm)\n",
      "    mul_14 = layers_7_self_attn_q_proj * 0.125;  layers_7_self_attn_q_proj = None\n",
      "    layers_7_self_attn_k_proj = getattr(self.layers, \"7\").self_attn.k_proj(layers_7_self_attn_layer_norm)\n",
      "    view_35 = layers_7_self_attn_k_proj.view(getitem_21, -1, 12, 64);  layers_7_self_attn_k_proj = None\n",
      "    transpose_35 = view_35.transpose(1, 2);  view_35 = None\n",
      "    contiguous_21 = transpose_35.contiguous();  transpose_35 = None\n",
      "    layers_7_self_attn_v_proj = getattr(self.layers, \"7\").self_attn.v_proj(layers_7_self_attn_layer_norm);  layers_7_self_attn_layer_norm = None\n",
      "    view_36 = layers_7_self_attn_v_proj.view(getitem_21, -1, 12, 64);  layers_7_self_attn_v_proj = None\n",
      "    transpose_36 = view_36.transpose(1, 2);  view_36 = None\n",
      "    contiguous_22 = transpose_36.contiguous();  transpose_36 = None\n",
      "    mul_15 = getitem_21 * 12\n",
      "    view_37 = mul_14.view(getitem_21, getitem_22, 12, 64);  mul_14 = None\n",
      "    transpose_37 = view_37.transpose(1, 2);  view_37 = None\n",
      "    contiguous_23 = transpose_37.contiguous();  transpose_37 = None\n",
      "    view_38 = contiguous_23.view(mul_15, -1, 64);  contiguous_23 = None\n",
      "    reshape_21 = contiguous_21.reshape(mul_15, -1, 64);  contiguous_21 = None\n",
      "    reshape_22 = contiguous_22.reshape(mul_15, -1, 64);  contiguous_22 = mul_15 = None\n",
      "    size_15 = reshape_21.size(1)\n",
      "    transpose_38 = reshape_21.transpose(1, 2);  reshape_21 = None\n",
      "    bmm_14 = torch.bmm(view_38, transpose_38);  view_38 = transpose_38 = None\n",
      "    softmax_7 = torch.nn.functional.softmax(bmm_14, dim = -1, _stacklevel = 3, dtype = None);  bmm_14 = None\n",
      "    dropout_29 = torch.nn.functional.dropout(softmax_7, p = 0.0, training = False, inplace = False);  softmax_7 = None\n",
      "    bmm_15 = torch.bmm(dropout_29, reshape_22);  dropout_29 = reshape_22 = None\n",
      "    view_39 = bmm_15.view(getitem_21, 12, getitem_22, 64);  bmm_15 = None\n",
      "    transpose_39 = view_39.transpose(1, 2);  view_39 = None\n",
      "    reshape_23 = transpose_39.reshape(getitem_21, getitem_22, 768);  transpose_39 = getitem_21 = getitem_22 = None\n",
      "    layers_7_self_attn_out_proj = getattr(self.layers, \"7\").self_attn.out_proj(reshape_23);  reshape_23 = None\n",
      "    dropout_30 = torch.nn.functional.dropout(layers_7_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_7_self_attn_out_proj = None\n",
      "    add_15 = add_14 + dropout_30;  add_14 = dropout_30 = None\n",
      "    layers_7_final_layer_norm = getattr(self.layers, \"7\").final_layer_norm(add_15)\n",
      "    layers_7_fc1 = getattr(self.layers, \"7\").fc1(layers_7_final_layer_norm);  layers_7_final_layer_norm = None\n",
      "    gelu_9 = torch._C._nn.gelu(layers_7_fc1);  layers_7_fc1 = None\n",
      "    dropout_31 = torch.nn.functional.dropout(gelu_9, p = 0.0, training = False, inplace = False);  gelu_9 = None\n",
      "    layers_7_fc2 = getattr(self.layers, \"7\").fc2(dropout_31);  dropout_31 = None\n",
      "    dropout_32 = torch.nn.functional.dropout(layers_7_fc2, p = 0.0, training = False, inplace = False);  layers_7_fc2 = None\n",
      "    add_16 = add_15 + dropout_32;  add_15 = dropout_32 = None\n",
      "    layers_8_self_attn_layer_norm = getattr(self.layers, \"8\").self_attn_layer_norm(add_16)\n",
      "    size_16 = layers_8_self_attn_layer_norm.size()\n",
      "    getitem_24 = size_16[0]\n",
      "    getitem_25 = size_16[1]\n",
      "    getitem_26 = size_16[2];  size_16 = None\n",
      "    layers_8_self_attn_q_proj = getattr(self.layers, \"8\").self_attn.q_proj(layers_8_self_attn_layer_norm)\n",
      "    mul_16 = layers_8_self_attn_q_proj * 0.125;  layers_8_self_attn_q_proj = None\n",
      "    layers_8_self_attn_k_proj = getattr(self.layers, \"8\").self_attn.k_proj(layers_8_self_attn_layer_norm)\n",
      "    view_40 = layers_8_self_attn_k_proj.view(getitem_24, -1, 12, 64);  layers_8_self_attn_k_proj = None\n",
      "    transpose_40 = view_40.transpose(1, 2);  view_40 = None\n",
      "    contiguous_24 = transpose_40.contiguous();  transpose_40 = None\n",
      "    layers_8_self_attn_v_proj = getattr(self.layers, \"8\").self_attn.v_proj(layers_8_self_attn_layer_norm);  layers_8_self_attn_layer_norm = None\n",
      "    view_41 = layers_8_self_attn_v_proj.view(getitem_24, -1, 12, 64);  layers_8_self_attn_v_proj = None\n",
      "    transpose_41 = view_41.transpose(1, 2);  view_41 = None\n",
      "    contiguous_25 = transpose_41.contiguous();  transpose_41 = None\n",
      "    mul_17 = getitem_24 * 12\n",
      "    view_42 = mul_16.view(getitem_24, getitem_25, 12, 64);  mul_16 = None\n",
      "    transpose_42 = view_42.transpose(1, 2);  view_42 = None\n",
      "    contiguous_26 = transpose_42.contiguous();  transpose_42 = None\n",
      "    view_43 = contiguous_26.view(mul_17, -1, 64);  contiguous_26 = None\n",
      "    reshape_24 = contiguous_24.reshape(mul_17, -1, 64);  contiguous_24 = None\n",
      "    reshape_25 = contiguous_25.reshape(mul_17, -1, 64);  contiguous_25 = mul_17 = None\n",
      "    size_17 = reshape_24.size(1)\n",
      "    transpose_43 = reshape_24.transpose(1, 2);  reshape_24 = None\n",
      "    bmm_16 = torch.bmm(view_43, transpose_43);  view_43 = transpose_43 = None\n",
      "    softmax_8 = torch.nn.functional.softmax(bmm_16, dim = -1, _stacklevel = 3, dtype = None);  bmm_16 = None\n",
      "    dropout_33 = torch.nn.functional.dropout(softmax_8, p = 0.0, training = False, inplace = False);  softmax_8 = None\n",
      "    bmm_17 = torch.bmm(dropout_33, reshape_25);  dropout_33 = reshape_25 = None\n",
      "    view_44 = bmm_17.view(getitem_24, 12, getitem_25, 64);  bmm_17 = None\n",
      "    transpose_44 = view_44.transpose(1, 2);  view_44 = None\n",
      "    reshape_26 = transpose_44.reshape(getitem_24, getitem_25, 768);  transpose_44 = getitem_24 = getitem_25 = None\n",
      "    layers_8_self_attn_out_proj = getattr(self.layers, \"8\").self_attn.out_proj(reshape_26);  reshape_26 = None\n",
      "    dropout_34 = torch.nn.functional.dropout(layers_8_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_8_self_attn_out_proj = None\n",
      "    add_17 = add_16 + dropout_34;  add_16 = dropout_34 = None\n",
      "    layers_8_final_layer_norm = getattr(self.layers, \"8\").final_layer_norm(add_17)\n",
      "    layers_8_fc1 = getattr(self.layers, \"8\").fc1(layers_8_final_layer_norm);  layers_8_final_layer_norm = None\n",
      "    gelu_10 = torch._C._nn.gelu(layers_8_fc1);  layers_8_fc1 = None\n",
      "    dropout_35 = torch.nn.functional.dropout(gelu_10, p = 0.0, training = False, inplace = False);  gelu_10 = None\n",
      "    layers_8_fc2 = getattr(self.layers, \"8\").fc2(dropout_35);  dropout_35 = None\n",
      "    dropout_36 = torch.nn.functional.dropout(layers_8_fc2, p = 0.0, training = False, inplace = False);  layers_8_fc2 = None\n",
      "    add_18 = add_17 + dropout_36;  add_17 = dropout_36 = None\n",
      "    layers_9_self_attn_layer_norm = getattr(self.layers, \"9\").self_attn_layer_norm(add_18)\n",
      "    size_18 = layers_9_self_attn_layer_norm.size()\n",
      "    getitem_27 = size_18[0]\n",
      "    getitem_28 = size_18[1]\n",
      "    getitem_29 = size_18[2];  size_18 = None\n",
      "    layers_9_self_attn_q_proj = getattr(self.layers, \"9\").self_attn.q_proj(layers_9_self_attn_layer_norm)\n",
      "    mul_18 = layers_9_self_attn_q_proj * 0.125;  layers_9_self_attn_q_proj = None\n",
      "    layers_9_self_attn_k_proj = getattr(self.layers, \"9\").self_attn.k_proj(layers_9_self_attn_layer_norm)\n",
      "    view_45 = layers_9_self_attn_k_proj.view(getitem_27, -1, 12, 64);  layers_9_self_attn_k_proj = None\n",
      "    transpose_45 = view_45.transpose(1, 2);  view_45 = None\n",
      "    contiguous_27 = transpose_45.contiguous();  transpose_45 = None\n",
      "    layers_9_self_attn_v_proj = getattr(self.layers, \"9\").self_attn.v_proj(layers_9_self_attn_layer_norm);  layers_9_self_attn_layer_norm = None\n",
      "    view_46 = layers_9_self_attn_v_proj.view(getitem_27, -1, 12, 64);  layers_9_self_attn_v_proj = None\n",
      "    transpose_46 = view_46.transpose(1, 2);  view_46 = None\n",
      "    contiguous_28 = transpose_46.contiguous();  transpose_46 = None\n",
      "    mul_19 = getitem_27 * 12\n",
      "    view_47 = mul_18.view(getitem_27, getitem_28, 12, 64);  mul_18 = None\n",
      "    transpose_47 = view_47.transpose(1, 2);  view_47 = None\n",
      "    contiguous_29 = transpose_47.contiguous();  transpose_47 = None\n",
      "    view_48 = contiguous_29.view(mul_19, -1, 64);  contiguous_29 = None\n",
      "    reshape_27 = contiguous_27.reshape(mul_19, -1, 64);  contiguous_27 = None\n",
      "    reshape_28 = contiguous_28.reshape(mul_19, -1, 64);  contiguous_28 = mul_19 = None\n",
      "    size_19 = reshape_27.size(1)\n",
      "    transpose_48 = reshape_27.transpose(1, 2);  reshape_27 = None\n",
      "    bmm_18 = torch.bmm(view_48, transpose_48);  view_48 = transpose_48 = None\n",
      "    softmax_9 = torch.nn.functional.softmax(bmm_18, dim = -1, _stacklevel = 3, dtype = None);  bmm_18 = None\n",
      "    dropout_37 = torch.nn.functional.dropout(softmax_9, p = 0.0, training = False, inplace = False);  softmax_9 = None\n",
      "    bmm_19 = torch.bmm(dropout_37, reshape_28);  dropout_37 = reshape_28 = None\n",
      "    view_49 = bmm_19.view(getitem_27, 12, getitem_28, 64);  bmm_19 = None\n",
      "    transpose_49 = view_49.transpose(1, 2);  view_49 = None\n",
      "    reshape_29 = transpose_49.reshape(getitem_27, getitem_28, 768);  transpose_49 = getitem_27 = getitem_28 = None\n",
      "    layers_9_self_attn_out_proj = getattr(self.layers, \"9\").self_attn.out_proj(reshape_29);  reshape_29 = None\n",
      "    dropout_38 = torch.nn.functional.dropout(layers_9_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_9_self_attn_out_proj = None\n",
      "    add_19 = add_18 + dropout_38;  add_18 = dropout_38 = None\n",
      "    layers_9_final_layer_norm = getattr(self.layers, \"9\").final_layer_norm(add_19)\n",
      "    layers_9_fc1 = getattr(self.layers, \"9\").fc1(layers_9_final_layer_norm);  layers_9_final_layer_norm = None\n",
      "    gelu_11 = torch._C._nn.gelu(layers_9_fc1);  layers_9_fc1 = None\n",
      "    dropout_39 = torch.nn.functional.dropout(gelu_11, p = 0.0, training = False, inplace = False);  gelu_11 = None\n",
      "    layers_9_fc2 = getattr(self.layers, \"9\").fc2(dropout_39);  dropout_39 = None\n",
      "    dropout_40 = torch.nn.functional.dropout(layers_9_fc2, p = 0.0, training = False, inplace = False);  layers_9_fc2 = None\n",
      "    add_20 = add_19 + dropout_40;  add_19 = dropout_40 = None\n",
      "    layers_10_self_attn_layer_norm = getattr(self.layers, \"10\").self_attn_layer_norm(add_20)\n",
      "    size_20 = layers_10_self_attn_layer_norm.size()\n",
      "    getitem_30 = size_20[0]\n",
      "    getitem_31 = size_20[1]\n",
      "    getitem_32 = size_20[2];  size_20 = None\n",
      "    layers_10_self_attn_q_proj = getattr(self.layers, \"10\").self_attn.q_proj(layers_10_self_attn_layer_norm)\n",
      "    mul_20 = layers_10_self_attn_q_proj * 0.125;  layers_10_self_attn_q_proj = None\n",
      "    layers_10_self_attn_k_proj = getattr(self.layers, \"10\").self_attn.k_proj(layers_10_self_attn_layer_norm)\n",
      "    view_50 = layers_10_self_attn_k_proj.view(getitem_30, -1, 12, 64);  layers_10_self_attn_k_proj = None\n",
      "    transpose_50 = view_50.transpose(1, 2);  view_50 = None\n",
      "    contiguous_30 = transpose_50.contiguous();  transpose_50 = None\n",
      "    layers_10_self_attn_v_proj = getattr(self.layers, \"10\").self_attn.v_proj(layers_10_self_attn_layer_norm);  layers_10_self_attn_layer_norm = None\n",
      "    view_51 = layers_10_self_attn_v_proj.view(getitem_30, -1, 12, 64);  layers_10_self_attn_v_proj = None\n",
      "    transpose_51 = view_51.transpose(1, 2);  view_51 = None\n",
      "    contiguous_31 = transpose_51.contiguous();  transpose_51 = None\n",
      "    mul_21 = getitem_30 * 12\n",
      "    view_52 = mul_20.view(getitem_30, getitem_31, 12, 64);  mul_20 = None\n",
      "    transpose_52 = view_52.transpose(1, 2);  view_52 = None\n",
      "    contiguous_32 = transpose_52.contiguous();  transpose_52 = None\n",
      "    view_53 = contiguous_32.view(mul_21, -1, 64);  contiguous_32 = None\n",
      "    reshape_30 = contiguous_30.reshape(mul_21, -1, 64);  contiguous_30 = None\n",
      "    reshape_31 = contiguous_31.reshape(mul_21, -1, 64);  contiguous_31 = mul_21 = None\n",
      "    size_21 = reshape_30.size(1)\n",
      "    transpose_53 = reshape_30.transpose(1, 2);  reshape_30 = None\n",
      "    bmm_20 = torch.bmm(view_53, transpose_53);  view_53 = transpose_53 = None\n",
      "    softmax_10 = torch.nn.functional.softmax(bmm_20, dim = -1, _stacklevel = 3, dtype = None);  bmm_20 = None\n",
      "    dropout_41 = torch.nn.functional.dropout(softmax_10, p = 0.0, training = False, inplace = False);  softmax_10 = None\n",
      "    bmm_21 = torch.bmm(dropout_41, reshape_31);  dropout_41 = reshape_31 = None\n",
      "    view_54 = bmm_21.view(getitem_30, 12, getitem_31, 64);  bmm_21 = None\n",
      "    transpose_54 = view_54.transpose(1, 2);  view_54 = None\n",
      "    reshape_32 = transpose_54.reshape(getitem_30, getitem_31, 768);  transpose_54 = getitem_30 = getitem_31 = None\n",
      "    layers_10_self_attn_out_proj = getattr(self.layers, \"10\").self_attn.out_proj(reshape_32);  reshape_32 = None\n",
      "    dropout_42 = torch.nn.functional.dropout(layers_10_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_10_self_attn_out_proj = None\n",
      "    add_21 = add_20 + dropout_42;  add_20 = dropout_42 = None\n",
      "    layers_10_final_layer_norm = getattr(self.layers, \"10\").final_layer_norm(add_21)\n",
      "    layers_10_fc1 = getattr(self.layers, \"10\").fc1(layers_10_final_layer_norm);  layers_10_final_layer_norm = None\n",
      "    gelu_12 = torch._C._nn.gelu(layers_10_fc1);  layers_10_fc1 = None\n",
      "    dropout_43 = torch.nn.functional.dropout(gelu_12, p = 0.0, training = False, inplace = False);  gelu_12 = None\n",
      "    layers_10_fc2 = getattr(self.layers, \"10\").fc2(dropout_43);  dropout_43 = None\n",
      "    dropout_44 = torch.nn.functional.dropout(layers_10_fc2, p = 0.0, training = False, inplace = False);  layers_10_fc2 = None\n",
      "    add_22 = add_21 + dropout_44;  add_21 = dropout_44 = None\n",
      "    layers_11_self_attn_layer_norm = getattr(self.layers, \"11\").self_attn_layer_norm(add_22)\n",
      "    size_22 = layers_11_self_attn_layer_norm.size()\n",
      "    getitem_33 = size_22[0]\n",
      "    getitem_34 = size_22[1]\n",
      "    getitem_35 = size_22[2];  size_22 = None\n",
      "    layers_11_self_attn_q_proj = getattr(self.layers, \"11\").self_attn.q_proj(layers_11_self_attn_layer_norm)\n",
      "    mul_22 = layers_11_self_attn_q_proj * 0.125;  layers_11_self_attn_q_proj = None\n",
      "    layers_11_self_attn_k_proj = getattr(self.layers, \"11\").self_attn.k_proj(layers_11_self_attn_layer_norm)\n",
      "    view_55 = layers_11_self_attn_k_proj.view(getitem_33, -1, 12, 64);  layers_11_self_attn_k_proj = None\n",
      "    transpose_55 = view_55.transpose(1, 2);  view_55 = None\n",
      "    contiguous_33 = transpose_55.contiguous();  transpose_55 = None\n",
      "    layers_11_self_attn_v_proj = getattr(self.layers, \"11\").self_attn.v_proj(layers_11_self_attn_layer_norm);  layers_11_self_attn_layer_norm = None\n",
      "    view_56 = layers_11_self_attn_v_proj.view(getitem_33, -1, 12, 64);  layers_11_self_attn_v_proj = None\n",
      "    transpose_56 = view_56.transpose(1, 2);  view_56 = None\n",
      "    contiguous_34 = transpose_56.contiguous();  transpose_56 = None\n",
      "    mul_23 = getitem_33 * 12\n",
      "    view_57 = mul_22.view(getitem_33, getitem_34, 12, 64);  mul_22 = None\n",
      "    transpose_57 = view_57.transpose(1, 2);  view_57 = None\n",
      "    contiguous_35 = transpose_57.contiguous();  transpose_57 = None\n",
      "    view_58 = contiguous_35.view(mul_23, -1, 64);  contiguous_35 = None\n",
      "    reshape_33 = contiguous_33.reshape(mul_23, -1, 64);  contiguous_33 = None\n",
      "    reshape_34 = contiguous_34.reshape(mul_23, -1, 64);  contiguous_34 = mul_23 = None\n",
      "    size_23 = reshape_33.size(1)\n",
      "    transpose_58 = reshape_33.transpose(1, 2);  reshape_33 = None\n",
      "    bmm_22 = torch.bmm(view_58, transpose_58);  view_58 = transpose_58 = None\n",
      "    softmax_11 = torch.nn.functional.softmax(bmm_22, dim = -1, _stacklevel = 3, dtype = None);  bmm_22 = None\n",
      "    dropout_45 = torch.nn.functional.dropout(softmax_11, p = 0.0, training = False, inplace = False);  softmax_11 = None\n",
      "    bmm_23 = torch.bmm(dropout_45, reshape_34);  dropout_45 = reshape_34 = None\n",
      "    view_59 = bmm_23.view(getitem_33, 12, getitem_34, 64);  bmm_23 = None\n",
      "    transpose_59 = view_59.transpose(1, 2);  view_59 = None\n",
      "    reshape_35 = transpose_59.reshape(getitem_33, getitem_34, 768);  transpose_59 = getitem_33 = getitem_34 = None\n",
      "    layers_11_self_attn_out_proj = getattr(self.layers, \"11\").self_attn.out_proj(reshape_35);  reshape_35 = None\n",
      "    dropout_46 = torch.nn.functional.dropout(layers_11_self_attn_out_proj, p = 0.0, training = False, inplace = False);  layers_11_self_attn_out_proj = None\n",
      "    add_23 = add_22 + dropout_46;  add_22 = dropout_46 = None\n",
      "    layers_11_final_layer_norm = getattr(self.layers, \"11\").final_layer_norm(add_23)\n",
      "    layers_11_fc1 = getattr(self.layers, \"11\").fc1(layers_11_final_layer_norm);  layers_11_final_layer_norm = None\n",
      "    gelu_13 = torch._C._nn.gelu(layers_11_fc1);  layers_11_fc1 = None\n",
      "    dropout_47 = torch.nn.functional.dropout(gelu_13, p = 0.0, training = False, inplace = False);  gelu_13 = None\n",
      "    layers_11_fc2 = getattr(self.layers, \"11\").fc2(dropout_47);  dropout_47 = None\n",
      "    dropout_48 = torch.nn.functional.dropout(layers_11_fc2, p = 0.0, training = False, inplace = False);  layers_11_fc2 = None\n",
      "    add_24 = add_23 + dropout_48;  add_23 = dropout_48 = None\n",
      "    layer_norm = self.layer_norm(add_24);  add_24 = None\n",
      "    return {'last_hidden_state': layer_norm}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(symbolic_traced.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48955666",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_traced : torch.fx.GraphModule = symbolic_trace(model, concrete_args=decoder_concrete_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d98bdb99",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.fx._symbolic_trace.wrap(\"modeling_whisper_traceable__prepare_decoder_attention_mask\")\n",
      "\n",
      "def forward(self, input_features : typing_Union[torch.FloatTensor,NoneType] = None, attention_mask : typing_Union[torch.LongTensor,NoneType] = None, decoder_input_ids : typing_Union[torch.LongTensor,NoneType] = None, decoder_attention_mask : typing_Union[torch.LongTensor,NoneType] = None, head_mask : typing_Union[torch.Tensor,NoneType] = None, decoder_head_mask : typing_Union[torch.Tensor,NoneType] = None, cross_attn_head_mask : typing_Union[torch.Tensor,NoneType] = None, encoder_outputs : typing_Union[typing_Tuple[typing_Tuple[torch.FloatTensor]],NoneType] = None, past_key_values : typing_Union[typing_Tuple[typing_Tuple[torch.FloatTensor]],NoneType] = None, decoder_inputs_embeds_1 = None, labels : typing_Union[torch.LongTensor,NoneType] = None, use_cache_1 = None, output_attentions_1 = None, output_hidden_states_1 = None, return_dict_1 = None) -> typing_Union[typing_Tuple[torch.Tensor],transformers_modeling_outputs_Seq2SeqLMOutput]:\n",
      "    _assert_is_none = torch.fx._symbolic_trace._assert_is_none(decoder_inputs_embeds_1, 'decoder_inputs_embeds has been specialized to have value None but got another value');  decoder_inputs_embeds_1 = None\n",
      "    eq = use_cache_1 == True;  use_cache_1 = None\n",
      "    _assert = torch._assert(eq, 'use_cache has been specialized to have value True but got another value');  eq = None\n",
      "    eq_1 = output_attentions_1 == False;  output_attentions_1 = None\n",
      "    _assert_1 = torch._assert(eq_1, 'output_attentions has been specialized to have value False but got another value');  eq_1 = None\n",
      "    eq_2 = output_hidden_states_1 == False;  output_hidden_states_1 = None\n",
      "    _assert_2 = torch._assert(eq_2, 'output_hidden_states has been specialized to have value False but got another value');  eq_2 = None\n",
      "    eq_3 = return_dict_1 == True;  return_dict_1 = None\n",
      "    _assert_3 = torch._assert(eq_3, 'return_dict has been specialized to have value True but got another value');  eq_3 = None\n",
      "    getitem = encoder_outputs[0]\n",
      "    size = decoder_input_ids.size()\n",
      "    getitem_1 = size[-1]\n",
      "    view = decoder_input_ids.view(-1, getitem_1);  decoder_input_ids = getitem_1 = None\n",
      "    getitem_2 = past_key_values[0]\n",
      "    getitem_3 = getitem_2[0];  getitem_2 = None\n",
      "    getattr_1 = getitem_3.shape;  getitem_3 = None\n",
      "    getitem_4 = getattr_1[2];  getattr_1 = None\n",
      "    model_decoder_embed_tokens = self.model.decoder.embed_tokens(view)\n",
      "    _prepare_decoder_attention_mask = modeling_whisper_traceable__prepare_decoder_attention_mask(decoder_attention_mask, size, model_decoder_embed_tokens, getitem_4);  decoder_attention_mask = size = None\n",
      "    model_decoder_embed_positions_weight = self.model.decoder.embed_positions.weight\n",
      "    getattr_2 = view.shape;  view = None\n",
      "    getitem_5 = getattr_2[1];  getattr_2 = None\n",
      "    add = getitem_4 + getitem_5;  getitem_5 = None\n",
      "    getitem_6 = model_decoder_embed_positions_weight[slice(getitem_4, add, None)];  model_decoder_embed_positions_weight = getitem_4 = add = None\n",
      "    add_1 = model_decoder_embed_tokens + getitem_6;  model_decoder_embed_tokens = getitem_6 = None\n",
      "    dropout = torch.nn.functional.dropout(add_1, p = 0.0, training = False, inplace = False);  add_1 = None\n",
      "    getitem_7 = past_key_values[0]\n",
      "    getitem_8 = decoder_head_mask[0]\n",
      "    getitem_9 = cross_attn_head_mask[0]\n",
      "    model_decoder_layers_0_self_attn_layer_norm = getattr(self.model.decoder.layers, \"0\").self_attn_layer_norm(dropout)\n",
      "    getitem_10 = getitem_7[slice(None, 2, None)]\n",
      "    size_1 = model_decoder_layers_0_self_attn_layer_norm.size()\n",
      "    getitem_11 = size_1[0]\n",
      "    getitem_12 = size_1[1]\n",
      "    getitem_13 = size_1[2];  size_1 = None\n",
      "    model_decoder_layers_0_self_attn_q_proj = getattr(self.model.decoder.layers, \"0\").self_attn.q_proj(model_decoder_layers_0_self_attn_layer_norm)\n",
      "    mul = model_decoder_layers_0_self_attn_q_proj * 0.125;  model_decoder_layers_0_self_attn_q_proj = None\n",
      "    model_decoder_layers_0_self_attn_k_proj = getattr(self.model.decoder.layers, \"0\").self_attn.k_proj(model_decoder_layers_0_self_attn_layer_norm)\n",
      "    view_1 = model_decoder_layers_0_self_attn_k_proj.view(getitem_11, -1, 12, 64);  model_decoder_layers_0_self_attn_k_proj = None\n",
      "    transpose = view_1.transpose(1, 2);  view_1 = None\n",
      "    contiguous = transpose.contiguous();  transpose = None\n",
      "    model_decoder_layers_0_self_attn_v_proj = getattr(self.model.decoder.layers, \"0\").self_attn.v_proj(model_decoder_layers_0_self_attn_layer_norm);  model_decoder_layers_0_self_attn_layer_norm = None\n",
      "    view_2 = model_decoder_layers_0_self_attn_v_proj.view(getitem_11, -1, 12, 64);  model_decoder_layers_0_self_attn_v_proj = None\n",
      "    transpose_1 = view_2.transpose(1, 2);  view_2 = None\n",
      "    contiguous_1 = transpose_1.contiguous();  transpose_1 = None\n",
      "    getitem_14 = getitem_10[0]\n",
      "    cat = torch.cat([getitem_14, contiguous], dim = 2);  getitem_14 = contiguous = None\n",
      "    getitem_15 = getitem_10[1];  getitem_10 = None\n",
      "    cat_1 = torch.cat([getitem_15, contiguous_1], dim = 2);  getitem_15 = contiguous_1 = None\n",
      "    mul_1 = getitem_11 * 12\n",
      "    view_3 = mul.view(getitem_11, getitem_12, 12, 64);  mul = None\n",
      "    transpose_2 = view_3.transpose(1, 2);  view_3 = None\n",
      "    contiguous_2 = transpose_2.contiguous();  transpose_2 = None\n",
      "    view_4 = contiguous_2.view(mul_1, -1, 64);  contiguous_2 = None\n",
      "    reshape = cat.reshape(mul_1, -1, 64)\n",
      "    reshape_1 = cat_1.reshape(mul_1, -1, 64);  mul_1 = None\n",
      "    size_2 = reshape.size(1)\n",
      "    transpose_3 = reshape.transpose(1, 2);  reshape = None\n",
      "    bmm = torch.bmm(view_4, transpose_3);  view_4 = transpose_3 = None\n",
      "    view_5 = bmm.view(getitem_11, 12, getitem_12, size_2);  bmm = None\n",
      "    add_2 = view_5 + _prepare_decoder_attention_mask;  view_5 = None\n",
      "    mul_2 = getitem_11 * 12\n",
      "    view_6 = add_2.view(mul_2, getitem_12, size_2);  add_2 = mul_2 = None\n",
      "    softmax = torch.nn.functional.softmax(view_6, dim = -1, _stacklevel = 3, dtype = None);  view_6 = None\n",
      "    view_7 = getitem_8.view(1, -1, 1, 1);  getitem_8 = None\n",
      "    view_8 = softmax.view(getitem_11, 12, getitem_12, size_2);  softmax = None\n",
      "    mul_3 = view_7 * view_8;  view_7 = view_8 = None\n",
      "    mul_4 = getitem_11 * 12\n",
      "    view_9 = mul_3.view(mul_4, getitem_12, size_2);  mul_3 = mul_4 = size_2 = None\n",
      "    dropout_1 = torch.nn.functional.dropout(view_9, p = 0.0, training = False, inplace = False);  view_9 = None\n",
      "    bmm_1 = torch.bmm(dropout_1, reshape_1);  dropout_1 = reshape_1 = None\n",
      "    view_10 = bmm_1.view(getitem_11, 12, getitem_12, 64);  bmm_1 = None\n",
      "    transpose_4 = view_10.transpose(1, 2);  view_10 = None\n",
      "    reshape_2 = transpose_4.reshape(getitem_11, getitem_12, 768);  transpose_4 = getitem_11 = getitem_12 = None\n",
      "    model_decoder_layers_0_self_attn_out_proj = getattr(self.model.decoder.layers, \"0\").self_attn.out_proj(reshape_2);  reshape_2 = None\n",
      "    dropout_2 = torch.nn.functional.dropout(model_decoder_layers_0_self_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_0_self_attn_out_proj = None\n",
      "    add_3 = dropout + dropout_2;  dropout = dropout_2 = None\n",
      "    model_decoder_layers_0_encoder_attn_layer_norm = getattr(self.model.decoder.layers, \"0\").encoder_attn_layer_norm(add_3)\n",
      "    getitem_16 = getitem_7[slice(-2, None, None)];  getitem_7 = None\n",
      "    size_3 = model_decoder_layers_0_encoder_attn_layer_norm.size()\n",
      "    getitem_17 = size_3[0]\n",
      "    getitem_18 = size_3[1]\n",
      "    getitem_19 = size_3[2];  size_3 = None\n",
      "    model_decoder_layers_0_encoder_attn_q_proj = getattr(self.model.decoder.layers, \"0\").encoder_attn.q_proj(model_decoder_layers_0_encoder_attn_layer_norm);  model_decoder_layers_0_encoder_attn_layer_norm = None\n",
      "    mul_5 = model_decoder_layers_0_encoder_attn_q_proj * 0.125;  model_decoder_layers_0_encoder_attn_q_proj = None\n",
      "    getitem_20 = getitem_16[0]\n",
      "    getitem_21 = getitem_16[1];  getitem_16 = None\n",
      "    mul_6 = getitem_17 * 12\n",
      "    view_11 = mul_5.view(getitem_17, getitem_18, 12, 64);  mul_5 = None\n",
      "    transpose_5 = view_11.transpose(1, 2);  view_11 = None\n",
      "    contiguous_3 = transpose_5.contiguous();  transpose_5 = None\n",
      "    view_12 = contiguous_3.view(mul_6, -1, 64);  contiguous_3 = None\n",
      "    reshape_3 = getitem_20.reshape(mul_6, -1, 64)\n",
      "    reshape_4 = getitem_21.reshape(mul_6, -1, 64);  mul_6 = None\n",
      "    size_4 = reshape_3.size(1)\n",
      "    transpose_6 = reshape_3.transpose(1, 2);  reshape_3 = None\n",
      "    bmm_2 = torch.bmm(view_12, transpose_6);  view_12 = transpose_6 = None\n",
      "    softmax_1 = torch.nn.functional.softmax(bmm_2, dim = -1, _stacklevel = 3, dtype = None);  bmm_2 = None\n",
      "    view_13 = getitem_9.view(1, -1, 1, 1);  getitem_9 = None\n",
      "    view_14 = softmax_1.view(getitem_17, 12, getitem_18, size_4);  softmax_1 = None\n",
      "    mul_7 = view_13 * view_14;  view_13 = view_14 = None\n",
      "    mul_8 = getitem_17 * 12\n",
      "    view_15 = mul_7.view(mul_8, getitem_18, size_4);  mul_7 = mul_8 = size_4 = None\n",
      "    dropout_3 = torch.nn.functional.dropout(view_15, p = 0.0, training = False, inplace = False);  view_15 = None\n",
      "    bmm_3 = torch.bmm(dropout_3, reshape_4);  dropout_3 = reshape_4 = None\n",
      "    view_16 = bmm_3.view(getitem_17, 12, getitem_18, 64);  bmm_3 = None\n",
      "    transpose_7 = view_16.transpose(1, 2);  view_16 = None\n",
      "    reshape_5 = transpose_7.reshape(getitem_17, getitem_18, 768);  transpose_7 = getitem_17 = getitem_18 = None\n",
      "    model_decoder_layers_0_encoder_attn_out_proj = getattr(self.model.decoder.layers, \"0\").encoder_attn.out_proj(reshape_5);  reshape_5 = None\n",
      "    dropout_4 = torch.nn.functional.dropout(model_decoder_layers_0_encoder_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_0_encoder_attn_out_proj = None\n",
      "    add_4 = add_3 + dropout_4;  add_3 = dropout_4 = None\n",
      "    model_decoder_layers_0_final_layer_norm = getattr(self.model.decoder.layers, \"0\").final_layer_norm(add_4)\n",
      "    model_decoder_layers_0_fc1 = getattr(self.model.decoder.layers, \"0\").fc1(model_decoder_layers_0_final_layer_norm);  model_decoder_layers_0_final_layer_norm = None\n",
      "    gelu = torch._C._nn.gelu(model_decoder_layers_0_fc1);  model_decoder_layers_0_fc1 = None\n",
      "    dropout_5 = torch.nn.functional.dropout(gelu, p = 0.0, training = False, inplace = False);  gelu = None\n",
      "    model_decoder_layers_0_fc2 = getattr(self.model.decoder.layers, \"0\").fc2(dropout_5);  dropout_5 = None\n",
      "    dropout_6 = torch.nn.functional.dropout(model_decoder_layers_0_fc2, p = 0.0, training = False, inplace = False);  model_decoder_layers_0_fc2 = None\n",
      "    add_5 = add_4 + dropout_6;  add_4 = dropout_6 = None\n",
      "    getitem_22 = past_key_values[1]\n",
      "    getitem_23 = decoder_head_mask[1]\n",
      "    getitem_24 = cross_attn_head_mask[1]\n",
      "    model_decoder_layers_1_self_attn_layer_norm = getattr(self.model.decoder.layers, \"1\").self_attn_layer_norm(add_5)\n",
      "    getitem_25 = getitem_22[slice(None, 2, None)]\n",
      "    size_5 = model_decoder_layers_1_self_attn_layer_norm.size()\n",
      "    getitem_26 = size_5[0]\n",
      "    getitem_27 = size_5[1]\n",
      "    getitem_28 = size_5[2];  size_5 = None\n",
      "    model_decoder_layers_1_self_attn_q_proj = getattr(self.model.decoder.layers, \"1\").self_attn.q_proj(model_decoder_layers_1_self_attn_layer_norm)\n",
      "    mul_9 = model_decoder_layers_1_self_attn_q_proj * 0.125;  model_decoder_layers_1_self_attn_q_proj = None\n",
      "    model_decoder_layers_1_self_attn_k_proj = getattr(self.model.decoder.layers, \"1\").self_attn.k_proj(model_decoder_layers_1_self_attn_layer_norm)\n",
      "    view_17 = model_decoder_layers_1_self_attn_k_proj.view(getitem_26, -1, 12, 64);  model_decoder_layers_1_self_attn_k_proj = None\n",
      "    transpose_8 = view_17.transpose(1, 2);  view_17 = None\n",
      "    contiguous_4 = transpose_8.contiguous();  transpose_8 = None\n",
      "    model_decoder_layers_1_self_attn_v_proj = getattr(self.model.decoder.layers, \"1\").self_attn.v_proj(model_decoder_layers_1_self_attn_layer_norm);  model_decoder_layers_1_self_attn_layer_norm = None\n",
      "    view_18 = model_decoder_layers_1_self_attn_v_proj.view(getitem_26, -1, 12, 64);  model_decoder_layers_1_self_attn_v_proj = None\n",
      "    transpose_9 = view_18.transpose(1, 2);  view_18 = None\n",
      "    contiguous_5 = transpose_9.contiguous();  transpose_9 = None\n",
      "    getitem_29 = getitem_25[0]\n",
      "    cat_2 = torch.cat([getitem_29, contiguous_4], dim = 2);  getitem_29 = contiguous_4 = None\n",
      "    getitem_30 = getitem_25[1];  getitem_25 = None\n",
      "    cat_3 = torch.cat([getitem_30, contiguous_5], dim = 2);  getitem_30 = contiguous_5 = None\n",
      "    mul_10 = getitem_26 * 12\n",
      "    view_19 = mul_9.view(getitem_26, getitem_27, 12, 64);  mul_9 = None\n",
      "    transpose_10 = view_19.transpose(1, 2);  view_19 = None\n",
      "    contiguous_6 = transpose_10.contiguous();  transpose_10 = None\n",
      "    view_20 = contiguous_6.view(mul_10, -1, 64);  contiguous_6 = None\n",
      "    reshape_6 = cat_2.reshape(mul_10, -1, 64)\n",
      "    reshape_7 = cat_3.reshape(mul_10, -1, 64);  mul_10 = None\n",
      "    size_6 = reshape_6.size(1)\n",
      "    transpose_11 = reshape_6.transpose(1, 2);  reshape_6 = None\n",
      "    bmm_4 = torch.bmm(view_20, transpose_11);  view_20 = transpose_11 = None\n",
      "    view_21 = bmm_4.view(getitem_26, 12, getitem_27, size_6);  bmm_4 = None\n",
      "    add_6 = view_21 + _prepare_decoder_attention_mask;  view_21 = None\n",
      "    mul_11 = getitem_26 * 12\n",
      "    view_22 = add_6.view(mul_11, getitem_27, size_6);  add_6 = mul_11 = None\n",
      "    softmax_2 = torch.nn.functional.softmax(view_22, dim = -1, _stacklevel = 3, dtype = None);  view_22 = None\n",
      "    view_23 = getitem_23.view(1, -1, 1, 1);  getitem_23 = None\n",
      "    view_24 = softmax_2.view(getitem_26, 12, getitem_27, size_6);  softmax_2 = None\n",
      "    mul_12 = view_23 * view_24;  view_23 = view_24 = None\n",
      "    mul_13 = getitem_26 * 12\n",
      "    view_25 = mul_12.view(mul_13, getitem_27, size_6);  mul_12 = mul_13 = size_6 = None\n",
      "    dropout_7 = torch.nn.functional.dropout(view_25, p = 0.0, training = False, inplace = False);  view_25 = None\n",
      "    bmm_5 = torch.bmm(dropout_7, reshape_7);  dropout_7 = reshape_7 = None\n",
      "    view_26 = bmm_5.view(getitem_26, 12, getitem_27, 64);  bmm_5 = None\n",
      "    transpose_12 = view_26.transpose(1, 2);  view_26 = None\n",
      "    reshape_8 = transpose_12.reshape(getitem_26, getitem_27, 768);  transpose_12 = getitem_26 = getitem_27 = None\n",
      "    model_decoder_layers_1_self_attn_out_proj = getattr(self.model.decoder.layers, \"1\").self_attn.out_proj(reshape_8);  reshape_8 = None\n",
      "    dropout_8 = torch.nn.functional.dropout(model_decoder_layers_1_self_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_1_self_attn_out_proj = None\n",
      "    add_7 = add_5 + dropout_8;  add_5 = dropout_8 = None\n",
      "    model_decoder_layers_1_encoder_attn_layer_norm = getattr(self.model.decoder.layers, \"1\").encoder_attn_layer_norm(add_7)\n",
      "    getitem_31 = getitem_22[slice(-2, None, None)];  getitem_22 = None\n",
      "    size_7 = model_decoder_layers_1_encoder_attn_layer_norm.size()\n",
      "    getitem_32 = size_7[0]\n",
      "    getitem_33 = size_7[1]\n",
      "    getitem_34 = size_7[2];  size_7 = None\n",
      "    model_decoder_layers_1_encoder_attn_q_proj = getattr(self.model.decoder.layers, \"1\").encoder_attn.q_proj(model_decoder_layers_1_encoder_attn_layer_norm);  model_decoder_layers_1_encoder_attn_layer_norm = None\n",
      "    mul_14 = model_decoder_layers_1_encoder_attn_q_proj * 0.125;  model_decoder_layers_1_encoder_attn_q_proj = None\n",
      "    getitem_35 = getitem_31[0]\n",
      "    getitem_36 = getitem_31[1];  getitem_31 = None\n",
      "    mul_15 = getitem_32 * 12\n",
      "    view_27 = mul_14.view(getitem_32, getitem_33, 12, 64);  mul_14 = None\n",
      "    transpose_13 = view_27.transpose(1, 2);  view_27 = None\n",
      "    contiguous_7 = transpose_13.contiguous();  transpose_13 = None\n",
      "    view_28 = contiguous_7.view(mul_15, -1, 64);  contiguous_7 = None\n",
      "    reshape_9 = getitem_35.reshape(mul_15, -1, 64)\n",
      "    reshape_10 = getitem_36.reshape(mul_15, -1, 64);  mul_15 = None\n",
      "    size_8 = reshape_9.size(1)\n",
      "    transpose_14 = reshape_9.transpose(1, 2);  reshape_9 = None\n",
      "    bmm_6 = torch.bmm(view_28, transpose_14);  view_28 = transpose_14 = None\n",
      "    softmax_3 = torch.nn.functional.softmax(bmm_6, dim = -1, _stacklevel = 3, dtype = None);  bmm_6 = None\n",
      "    view_29 = getitem_24.view(1, -1, 1, 1);  getitem_24 = None\n",
      "    view_30 = softmax_3.view(getitem_32, 12, getitem_33, size_8);  softmax_3 = None\n",
      "    mul_16 = view_29 * view_30;  view_29 = view_30 = None\n",
      "    mul_17 = getitem_32 * 12\n",
      "    view_31 = mul_16.view(mul_17, getitem_33, size_8);  mul_16 = mul_17 = size_8 = None\n",
      "    dropout_9 = torch.nn.functional.dropout(view_31, p = 0.0, training = False, inplace = False);  view_31 = None\n",
      "    bmm_7 = torch.bmm(dropout_9, reshape_10);  dropout_9 = reshape_10 = None\n",
      "    view_32 = bmm_7.view(getitem_32, 12, getitem_33, 64);  bmm_7 = None\n",
      "    transpose_15 = view_32.transpose(1, 2);  view_32 = None\n",
      "    reshape_11 = transpose_15.reshape(getitem_32, getitem_33, 768);  transpose_15 = getitem_32 = getitem_33 = None\n",
      "    model_decoder_layers_1_encoder_attn_out_proj = getattr(self.model.decoder.layers, \"1\").encoder_attn.out_proj(reshape_11);  reshape_11 = None\n",
      "    dropout_10 = torch.nn.functional.dropout(model_decoder_layers_1_encoder_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_1_encoder_attn_out_proj = None\n",
      "    add_8 = add_7 + dropout_10;  add_7 = dropout_10 = None\n",
      "    model_decoder_layers_1_final_layer_norm = getattr(self.model.decoder.layers, \"1\").final_layer_norm(add_8)\n",
      "    model_decoder_layers_1_fc1 = getattr(self.model.decoder.layers, \"1\").fc1(model_decoder_layers_1_final_layer_norm);  model_decoder_layers_1_final_layer_norm = None\n",
      "    gelu_1 = torch._C._nn.gelu(model_decoder_layers_1_fc1);  model_decoder_layers_1_fc1 = None\n",
      "    dropout_11 = torch.nn.functional.dropout(gelu_1, p = 0.0, training = False, inplace = False);  gelu_1 = None\n",
      "    model_decoder_layers_1_fc2 = getattr(self.model.decoder.layers, \"1\").fc2(dropout_11);  dropout_11 = None\n",
      "    dropout_12 = torch.nn.functional.dropout(model_decoder_layers_1_fc2, p = 0.0, training = False, inplace = False);  model_decoder_layers_1_fc2 = None\n",
      "    add_9 = add_8 + dropout_12;  add_8 = dropout_12 = None\n",
      "    getitem_37 = past_key_values[2]\n",
      "    getitem_38 = decoder_head_mask[2]\n",
      "    getitem_39 = cross_attn_head_mask[2]\n",
      "    model_decoder_layers_2_self_attn_layer_norm = getattr(self.model.decoder.layers, \"2\").self_attn_layer_norm(add_9)\n",
      "    getitem_40 = getitem_37[slice(None, 2, None)]\n",
      "    size_9 = model_decoder_layers_2_self_attn_layer_norm.size()\n",
      "    getitem_41 = size_9[0]\n",
      "    getitem_42 = size_9[1]\n",
      "    getitem_43 = size_9[2];  size_9 = None\n",
      "    model_decoder_layers_2_self_attn_q_proj = getattr(self.model.decoder.layers, \"2\").self_attn.q_proj(model_decoder_layers_2_self_attn_layer_norm)\n",
      "    mul_18 = model_decoder_layers_2_self_attn_q_proj * 0.125;  model_decoder_layers_2_self_attn_q_proj = None\n",
      "    model_decoder_layers_2_self_attn_k_proj = getattr(self.model.decoder.layers, \"2\").self_attn.k_proj(model_decoder_layers_2_self_attn_layer_norm)\n",
      "    view_33 = model_decoder_layers_2_self_attn_k_proj.view(getitem_41, -1, 12, 64);  model_decoder_layers_2_self_attn_k_proj = None\n",
      "    transpose_16 = view_33.transpose(1, 2);  view_33 = None\n",
      "    contiguous_8 = transpose_16.contiguous();  transpose_16 = None\n",
      "    model_decoder_layers_2_self_attn_v_proj = getattr(self.model.decoder.layers, \"2\").self_attn.v_proj(model_decoder_layers_2_self_attn_layer_norm);  model_decoder_layers_2_self_attn_layer_norm = None\n",
      "    view_34 = model_decoder_layers_2_self_attn_v_proj.view(getitem_41, -1, 12, 64);  model_decoder_layers_2_self_attn_v_proj = None\n",
      "    transpose_17 = view_34.transpose(1, 2);  view_34 = None\n",
      "    contiguous_9 = transpose_17.contiguous();  transpose_17 = None\n",
      "    getitem_44 = getitem_40[0]\n",
      "    cat_4 = torch.cat([getitem_44, contiguous_8], dim = 2);  getitem_44 = contiguous_8 = None\n",
      "    getitem_45 = getitem_40[1];  getitem_40 = None\n",
      "    cat_5 = torch.cat([getitem_45, contiguous_9], dim = 2);  getitem_45 = contiguous_9 = None\n",
      "    mul_19 = getitem_41 * 12\n",
      "    view_35 = mul_18.view(getitem_41, getitem_42, 12, 64);  mul_18 = None\n",
      "    transpose_18 = view_35.transpose(1, 2);  view_35 = None\n",
      "    contiguous_10 = transpose_18.contiguous();  transpose_18 = None\n",
      "    view_36 = contiguous_10.view(mul_19, -1, 64);  contiguous_10 = None\n",
      "    reshape_12 = cat_4.reshape(mul_19, -1, 64)\n",
      "    reshape_13 = cat_5.reshape(mul_19, -1, 64);  mul_19 = None\n",
      "    size_10 = reshape_12.size(1)\n",
      "    transpose_19 = reshape_12.transpose(1, 2);  reshape_12 = None\n",
      "    bmm_8 = torch.bmm(view_36, transpose_19);  view_36 = transpose_19 = None\n",
      "    view_37 = bmm_8.view(getitem_41, 12, getitem_42, size_10);  bmm_8 = None\n",
      "    add_10 = view_37 + _prepare_decoder_attention_mask;  view_37 = None\n",
      "    mul_20 = getitem_41 * 12\n",
      "    view_38 = add_10.view(mul_20, getitem_42, size_10);  add_10 = mul_20 = None\n",
      "    softmax_4 = torch.nn.functional.softmax(view_38, dim = -1, _stacklevel = 3, dtype = None);  view_38 = None\n",
      "    view_39 = getitem_38.view(1, -1, 1, 1);  getitem_38 = None\n",
      "    view_40 = softmax_4.view(getitem_41, 12, getitem_42, size_10);  softmax_4 = None\n",
      "    mul_21 = view_39 * view_40;  view_39 = view_40 = None\n",
      "    mul_22 = getitem_41 * 12\n",
      "    view_41 = mul_21.view(mul_22, getitem_42, size_10);  mul_21 = mul_22 = size_10 = None\n",
      "    dropout_13 = torch.nn.functional.dropout(view_41, p = 0.0, training = False, inplace = False);  view_41 = None\n",
      "    bmm_9 = torch.bmm(dropout_13, reshape_13);  dropout_13 = reshape_13 = None\n",
      "    view_42 = bmm_9.view(getitem_41, 12, getitem_42, 64);  bmm_9 = None\n",
      "    transpose_20 = view_42.transpose(1, 2);  view_42 = None\n",
      "    reshape_14 = transpose_20.reshape(getitem_41, getitem_42, 768);  transpose_20 = getitem_41 = getitem_42 = None\n",
      "    model_decoder_layers_2_self_attn_out_proj = getattr(self.model.decoder.layers, \"2\").self_attn.out_proj(reshape_14);  reshape_14 = None\n",
      "    dropout_14 = torch.nn.functional.dropout(model_decoder_layers_2_self_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_2_self_attn_out_proj = None\n",
      "    add_11 = add_9 + dropout_14;  add_9 = dropout_14 = None\n",
      "    model_decoder_layers_2_encoder_attn_layer_norm = getattr(self.model.decoder.layers, \"2\").encoder_attn_layer_norm(add_11)\n",
      "    getitem_46 = getitem_37[slice(-2, None, None)];  getitem_37 = None\n",
      "    size_11 = model_decoder_layers_2_encoder_attn_layer_norm.size()\n",
      "    getitem_47 = size_11[0]\n",
      "    getitem_48 = size_11[1]\n",
      "    getitem_49 = size_11[2];  size_11 = None\n",
      "    model_decoder_layers_2_encoder_attn_q_proj = getattr(self.model.decoder.layers, \"2\").encoder_attn.q_proj(model_decoder_layers_2_encoder_attn_layer_norm);  model_decoder_layers_2_encoder_attn_layer_norm = None\n",
      "    mul_23 = model_decoder_layers_2_encoder_attn_q_proj * 0.125;  model_decoder_layers_2_encoder_attn_q_proj = None\n",
      "    getitem_50 = getitem_46[0]\n",
      "    getitem_51 = getitem_46[1];  getitem_46 = None\n",
      "    mul_24 = getitem_47 * 12\n",
      "    view_43 = mul_23.view(getitem_47, getitem_48, 12, 64);  mul_23 = None\n",
      "    transpose_21 = view_43.transpose(1, 2);  view_43 = None\n",
      "    contiguous_11 = transpose_21.contiguous();  transpose_21 = None\n",
      "    view_44 = contiguous_11.view(mul_24, -1, 64);  contiguous_11 = None\n",
      "    reshape_15 = getitem_50.reshape(mul_24, -1, 64)\n",
      "    reshape_16 = getitem_51.reshape(mul_24, -1, 64);  mul_24 = None\n",
      "    size_12 = reshape_15.size(1)\n",
      "    transpose_22 = reshape_15.transpose(1, 2);  reshape_15 = None\n",
      "    bmm_10 = torch.bmm(view_44, transpose_22);  view_44 = transpose_22 = None\n",
      "    softmax_5 = torch.nn.functional.softmax(bmm_10, dim = -1, _stacklevel = 3, dtype = None);  bmm_10 = None\n",
      "    view_45 = getitem_39.view(1, -1, 1, 1);  getitem_39 = None\n",
      "    view_46 = softmax_5.view(getitem_47, 12, getitem_48, size_12);  softmax_5 = None\n",
      "    mul_25 = view_45 * view_46;  view_45 = view_46 = None\n",
      "    mul_26 = getitem_47 * 12\n",
      "    view_47 = mul_25.view(mul_26, getitem_48, size_12);  mul_25 = mul_26 = size_12 = None\n",
      "    dropout_15 = torch.nn.functional.dropout(view_47, p = 0.0, training = False, inplace = False);  view_47 = None\n",
      "    bmm_11 = torch.bmm(dropout_15, reshape_16);  dropout_15 = reshape_16 = None\n",
      "    view_48 = bmm_11.view(getitem_47, 12, getitem_48, 64);  bmm_11 = None\n",
      "    transpose_23 = view_48.transpose(1, 2);  view_48 = None\n",
      "    reshape_17 = transpose_23.reshape(getitem_47, getitem_48, 768);  transpose_23 = getitem_47 = getitem_48 = None\n",
      "    model_decoder_layers_2_encoder_attn_out_proj = getattr(self.model.decoder.layers, \"2\").encoder_attn.out_proj(reshape_17);  reshape_17 = None\n",
      "    dropout_16 = torch.nn.functional.dropout(model_decoder_layers_2_encoder_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_2_encoder_attn_out_proj = None\n",
      "    add_12 = add_11 + dropout_16;  add_11 = dropout_16 = None\n",
      "    model_decoder_layers_2_final_layer_norm = getattr(self.model.decoder.layers, \"2\").final_layer_norm(add_12)\n",
      "    model_decoder_layers_2_fc1 = getattr(self.model.decoder.layers, \"2\").fc1(model_decoder_layers_2_final_layer_norm);  model_decoder_layers_2_final_layer_norm = None\n",
      "    gelu_2 = torch._C._nn.gelu(model_decoder_layers_2_fc1);  model_decoder_layers_2_fc1 = None\n",
      "    dropout_17 = torch.nn.functional.dropout(gelu_2, p = 0.0, training = False, inplace = False);  gelu_2 = None\n",
      "    model_decoder_layers_2_fc2 = getattr(self.model.decoder.layers, \"2\").fc2(dropout_17);  dropout_17 = None\n",
      "    dropout_18 = torch.nn.functional.dropout(model_decoder_layers_2_fc2, p = 0.0, training = False, inplace = False);  model_decoder_layers_2_fc2 = None\n",
      "    add_13 = add_12 + dropout_18;  add_12 = dropout_18 = None\n",
      "    getitem_52 = past_key_values[3]\n",
      "    getitem_53 = decoder_head_mask[3]\n",
      "    getitem_54 = cross_attn_head_mask[3]\n",
      "    model_decoder_layers_3_self_attn_layer_norm = getattr(self.model.decoder.layers, \"3\").self_attn_layer_norm(add_13)\n",
      "    getitem_55 = getitem_52[slice(None, 2, None)]\n",
      "    size_13 = model_decoder_layers_3_self_attn_layer_norm.size()\n",
      "    getitem_56 = size_13[0]\n",
      "    getitem_57 = size_13[1]\n",
      "    getitem_58 = size_13[2];  size_13 = None\n",
      "    model_decoder_layers_3_self_attn_q_proj = getattr(self.model.decoder.layers, \"3\").self_attn.q_proj(model_decoder_layers_3_self_attn_layer_norm)\n",
      "    mul_27 = model_decoder_layers_3_self_attn_q_proj * 0.125;  model_decoder_layers_3_self_attn_q_proj = None\n",
      "    model_decoder_layers_3_self_attn_k_proj = getattr(self.model.decoder.layers, \"3\").self_attn.k_proj(model_decoder_layers_3_self_attn_layer_norm)\n",
      "    view_49 = model_decoder_layers_3_self_attn_k_proj.view(getitem_56, -1, 12, 64);  model_decoder_layers_3_self_attn_k_proj = None\n",
      "    transpose_24 = view_49.transpose(1, 2);  view_49 = None\n",
      "    contiguous_12 = transpose_24.contiguous();  transpose_24 = None\n",
      "    model_decoder_layers_3_self_attn_v_proj = getattr(self.model.decoder.layers, \"3\").self_attn.v_proj(model_decoder_layers_3_self_attn_layer_norm);  model_decoder_layers_3_self_attn_layer_norm = None\n",
      "    view_50 = model_decoder_layers_3_self_attn_v_proj.view(getitem_56, -1, 12, 64);  model_decoder_layers_3_self_attn_v_proj = None\n",
      "    transpose_25 = view_50.transpose(1, 2);  view_50 = None\n",
      "    contiguous_13 = transpose_25.contiguous();  transpose_25 = None\n",
      "    getitem_59 = getitem_55[0]\n",
      "    cat_6 = torch.cat([getitem_59, contiguous_12], dim = 2);  getitem_59 = contiguous_12 = None\n",
      "    getitem_60 = getitem_55[1];  getitem_55 = None\n",
      "    cat_7 = torch.cat([getitem_60, contiguous_13], dim = 2);  getitem_60 = contiguous_13 = None\n",
      "    mul_28 = getitem_56 * 12\n",
      "    view_51 = mul_27.view(getitem_56, getitem_57, 12, 64);  mul_27 = None\n",
      "    transpose_26 = view_51.transpose(1, 2);  view_51 = None\n",
      "    contiguous_14 = transpose_26.contiguous();  transpose_26 = None\n",
      "    view_52 = contiguous_14.view(mul_28, -1, 64);  contiguous_14 = None\n",
      "    reshape_18 = cat_6.reshape(mul_28, -1, 64)\n",
      "    reshape_19 = cat_7.reshape(mul_28, -1, 64);  mul_28 = None\n",
      "    size_14 = reshape_18.size(1)\n",
      "    transpose_27 = reshape_18.transpose(1, 2);  reshape_18 = None\n",
      "    bmm_12 = torch.bmm(view_52, transpose_27);  view_52 = transpose_27 = None\n",
      "    view_53 = bmm_12.view(getitem_56, 12, getitem_57, size_14);  bmm_12 = None\n",
      "    add_14 = view_53 + _prepare_decoder_attention_mask;  view_53 = None\n",
      "    mul_29 = getitem_56 * 12\n",
      "    view_54 = add_14.view(mul_29, getitem_57, size_14);  add_14 = mul_29 = None\n",
      "    softmax_6 = torch.nn.functional.softmax(view_54, dim = -1, _stacklevel = 3, dtype = None);  view_54 = None\n",
      "    view_55 = getitem_53.view(1, -1, 1, 1);  getitem_53 = None\n",
      "    view_56 = softmax_6.view(getitem_56, 12, getitem_57, size_14);  softmax_6 = None\n",
      "    mul_30 = view_55 * view_56;  view_55 = view_56 = None\n",
      "    mul_31 = getitem_56 * 12\n",
      "    view_57 = mul_30.view(mul_31, getitem_57, size_14);  mul_30 = mul_31 = size_14 = None\n",
      "    dropout_19 = torch.nn.functional.dropout(view_57, p = 0.0, training = False, inplace = False);  view_57 = None\n",
      "    bmm_13 = torch.bmm(dropout_19, reshape_19);  dropout_19 = reshape_19 = None\n",
      "    view_58 = bmm_13.view(getitem_56, 12, getitem_57, 64);  bmm_13 = None\n",
      "    transpose_28 = view_58.transpose(1, 2);  view_58 = None\n",
      "    reshape_20 = transpose_28.reshape(getitem_56, getitem_57, 768);  transpose_28 = getitem_56 = getitem_57 = None\n",
      "    model_decoder_layers_3_self_attn_out_proj = getattr(self.model.decoder.layers, \"3\").self_attn.out_proj(reshape_20);  reshape_20 = None\n",
      "    dropout_20 = torch.nn.functional.dropout(model_decoder_layers_3_self_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_3_self_attn_out_proj = None\n",
      "    add_15 = add_13 + dropout_20;  add_13 = dropout_20 = None\n",
      "    model_decoder_layers_3_encoder_attn_layer_norm = getattr(self.model.decoder.layers, \"3\").encoder_attn_layer_norm(add_15)\n",
      "    getitem_61 = getitem_52[slice(-2, None, None)];  getitem_52 = None\n",
      "    size_15 = model_decoder_layers_3_encoder_attn_layer_norm.size()\n",
      "    getitem_62 = size_15[0]\n",
      "    getitem_63 = size_15[1]\n",
      "    getitem_64 = size_15[2];  size_15 = None\n",
      "    model_decoder_layers_3_encoder_attn_q_proj = getattr(self.model.decoder.layers, \"3\").encoder_attn.q_proj(model_decoder_layers_3_encoder_attn_layer_norm);  model_decoder_layers_3_encoder_attn_layer_norm = None\n",
      "    mul_32 = model_decoder_layers_3_encoder_attn_q_proj * 0.125;  model_decoder_layers_3_encoder_attn_q_proj = None\n",
      "    getitem_65 = getitem_61[0]\n",
      "    getitem_66 = getitem_61[1];  getitem_61 = None\n",
      "    mul_33 = getitem_62 * 12\n",
      "    view_59 = mul_32.view(getitem_62, getitem_63, 12, 64);  mul_32 = None\n",
      "    transpose_29 = view_59.transpose(1, 2);  view_59 = None\n",
      "    contiguous_15 = transpose_29.contiguous();  transpose_29 = None\n",
      "    view_60 = contiguous_15.view(mul_33, -1, 64);  contiguous_15 = None\n",
      "    reshape_21 = getitem_65.reshape(mul_33, -1, 64)\n",
      "    reshape_22 = getitem_66.reshape(mul_33, -1, 64);  mul_33 = None\n",
      "    size_16 = reshape_21.size(1)\n",
      "    transpose_30 = reshape_21.transpose(1, 2);  reshape_21 = None\n",
      "    bmm_14 = torch.bmm(view_60, transpose_30);  view_60 = transpose_30 = None\n",
      "    softmax_7 = torch.nn.functional.softmax(bmm_14, dim = -1, _stacklevel = 3, dtype = None);  bmm_14 = None\n",
      "    view_61 = getitem_54.view(1, -1, 1, 1);  getitem_54 = None\n",
      "    view_62 = softmax_7.view(getitem_62, 12, getitem_63, size_16);  softmax_7 = None\n",
      "    mul_34 = view_61 * view_62;  view_61 = view_62 = None\n",
      "    mul_35 = getitem_62 * 12\n",
      "    view_63 = mul_34.view(mul_35, getitem_63, size_16);  mul_34 = mul_35 = size_16 = None\n",
      "    dropout_21 = torch.nn.functional.dropout(view_63, p = 0.0, training = False, inplace = False);  view_63 = None\n",
      "    bmm_15 = torch.bmm(dropout_21, reshape_22);  dropout_21 = reshape_22 = None\n",
      "    view_64 = bmm_15.view(getitem_62, 12, getitem_63, 64);  bmm_15 = None\n",
      "    transpose_31 = view_64.transpose(1, 2);  view_64 = None\n",
      "    reshape_23 = transpose_31.reshape(getitem_62, getitem_63, 768);  transpose_31 = getitem_62 = getitem_63 = None\n",
      "    model_decoder_layers_3_encoder_attn_out_proj = getattr(self.model.decoder.layers, \"3\").encoder_attn.out_proj(reshape_23);  reshape_23 = None\n",
      "    dropout_22 = torch.nn.functional.dropout(model_decoder_layers_3_encoder_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_3_encoder_attn_out_proj = None\n",
      "    add_16 = add_15 + dropout_22;  add_15 = dropout_22 = None\n",
      "    model_decoder_layers_3_final_layer_norm = getattr(self.model.decoder.layers, \"3\").final_layer_norm(add_16)\n",
      "    model_decoder_layers_3_fc1 = getattr(self.model.decoder.layers, \"3\").fc1(model_decoder_layers_3_final_layer_norm);  model_decoder_layers_3_final_layer_norm = None\n",
      "    gelu_3 = torch._C._nn.gelu(model_decoder_layers_3_fc1);  model_decoder_layers_3_fc1 = None\n",
      "    dropout_23 = torch.nn.functional.dropout(gelu_3, p = 0.0, training = False, inplace = False);  gelu_3 = None\n",
      "    model_decoder_layers_3_fc2 = getattr(self.model.decoder.layers, \"3\").fc2(dropout_23);  dropout_23 = None\n",
      "    dropout_24 = torch.nn.functional.dropout(model_decoder_layers_3_fc2, p = 0.0, training = False, inplace = False);  model_decoder_layers_3_fc2 = None\n",
      "    add_17 = add_16 + dropout_24;  add_16 = dropout_24 = None\n",
      "    getitem_67 = past_key_values[4]\n",
      "    getitem_68 = decoder_head_mask[4]\n",
      "    getitem_69 = cross_attn_head_mask[4]\n",
      "    model_decoder_layers_4_self_attn_layer_norm = getattr(self.model.decoder.layers, \"4\").self_attn_layer_norm(add_17)\n",
      "    getitem_70 = getitem_67[slice(None, 2, None)]\n",
      "    size_17 = model_decoder_layers_4_self_attn_layer_norm.size()\n",
      "    getitem_71 = size_17[0]\n",
      "    getitem_72 = size_17[1]\n",
      "    getitem_73 = size_17[2];  size_17 = None\n",
      "    model_decoder_layers_4_self_attn_q_proj = getattr(self.model.decoder.layers, \"4\").self_attn.q_proj(model_decoder_layers_4_self_attn_layer_norm)\n",
      "    mul_36 = model_decoder_layers_4_self_attn_q_proj * 0.125;  model_decoder_layers_4_self_attn_q_proj = None\n",
      "    model_decoder_layers_4_self_attn_k_proj = getattr(self.model.decoder.layers, \"4\").self_attn.k_proj(model_decoder_layers_4_self_attn_layer_norm)\n",
      "    view_65 = model_decoder_layers_4_self_attn_k_proj.view(getitem_71, -1, 12, 64);  model_decoder_layers_4_self_attn_k_proj = None\n",
      "    transpose_32 = view_65.transpose(1, 2);  view_65 = None\n",
      "    contiguous_16 = transpose_32.contiguous();  transpose_32 = None\n",
      "    model_decoder_layers_4_self_attn_v_proj = getattr(self.model.decoder.layers, \"4\").self_attn.v_proj(model_decoder_layers_4_self_attn_layer_norm);  model_decoder_layers_4_self_attn_layer_norm = None\n",
      "    view_66 = model_decoder_layers_4_self_attn_v_proj.view(getitem_71, -1, 12, 64);  model_decoder_layers_4_self_attn_v_proj = None\n",
      "    transpose_33 = view_66.transpose(1, 2);  view_66 = None\n",
      "    contiguous_17 = transpose_33.contiguous();  transpose_33 = None\n",
      "    getitem_74 = getitem_70[0]\n",
      "    cat_8 = torch.cat([getitem_74, contiguous_16], dim = 2);  getitem_74 = contiguous_16 = None\n",
      "    getitem_75 = getitem_70[1];  getitem_70 = None\n",
      "    cat_9 = torch.cat([getitem_75, contiguous_17], dim = 2);  getitem_75 = contiguous_17 = None\n",
      "    mul_37 = getitem_71 * 12\n",
      "    view_67 = mul_36.view(getitem_71, getitem_72, 12, 64);  mul_36 = None\n",
      "    transpose_34 = view_67.transpose(1, 2);  view_67 = None\n",
      "    contiguous_18 = transpose_34.contiguous();  transpose_34 = None\n",
      "    view_68 = contiguous_18.view(mul_37, -1, 64);  contiguous_18 = None\n",
      "    reshape_24 = cat_8.reshape(mul_37, -1, 64)\n",
      "    reshape_25 = cat_9.reshape(mul_37, -1, 64);  mul_37 = None\n",
      "    size_18 = reshape_24.size(1)\n",
      "    transpose_35 = reshape_24.transpose(1, 2);  reshape_24 = None\n",
      "    bmm_16 = torch.bmm(view_68, transpose_35);  view_68 = transpose_35 = None\n",
      "    view_69 = bmm_16.view(getitem_71, 12, getitem_72, size_18);  bmm_16 = None\n",
      "    add_18 = view_69 + _prepare_decoder_attention_mask;  view_69 = None\n",
      "    mul_38 = getitem_71 * 12\n",
      "    view_70 = add_18.view(mul_38, getitem_72, size_18);  add_18 = mul_38 = None\n",
      "    softmax_8 = torch.nn.functional.softmax(view_70, dim = -1, _stacklevel = 3, dtype = None);  view_70 = None\n",
      "    view_71 = getitem_68.view(1, -1, 1, 1);  getitem_68 = None\n",
      "    view_72 = softmax_8.view(getitem_71, 12, getitem_72, size_18);  softmax_8 = None\n",
      "    mul_39 = view_71 * view_72;  view_71 = view_72 = None\n",
      "    mul_40 = getitem_71 * 12\n",
      "    view_73 = mul_39.view(mul_40, getitem_72, size_18);  mul_39 = mul_40 = size_18 = None\n",
      "    dropout_25 = torch.nn.functional.dropout(view_73, p = 0.0, training = False, inplace = False);  view_73 = None\n",
      "    bmm_17 = torch.bmm(dropout_25, reshape_25);  dropout_25 = reshape_25 = None\n",
      "    view_74 = bmm_17.view(getitem_71, 12, getitem_72, 64);  bmm_17 = None\n",
      "    transpose_36 = view_74.transpose(1, 2);  view_74 = None\n",
      "    reshape_26 = transpose_36.reshape(getitem_71, getitem_72, 768);  transpose_36 = getitem_71 = getitem_72 = None\n",
      "    model_decoder_layers_4_self_attn_out_proj = getattr(self.model.decoder.layers, \"4\").self_attn.out_proj(reshape_26);  reshape_26 = None\n",
      "    dropout_26 = torch.nn.functional.dropout(model_decoder_layers_4_self_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_4_self_attn_out_proj = None\n",
      "    add_19 = add_17 + dropout_26;  add_17 = dropout_26 = None\n",
      "    model_decoder_layers_4_encoder_attn_layer_norm = getattr(self.model.decoder.layers, \"4\").encoder_attn_layer_norm(add_19)\n",
      "    getitem_76 = getitem_67[slice(-2, None, None)];  getitem_67 = None\n",
      "    size_19 = model_decoder_layers_4_encoder_attn_layer_norm.size()\n",
      "    getitem_77 = size_19[0]\n",
      "    getitem_78 = size_19[1]\n",
      "    getitem_79 = size_19[2];  size_19 = None\n",
      "    model_decoder_layers_4_encoder_attn_q_proj = getattr(self.model.decoder.layers, \"4\").encoder_attn.q_proj(model_decoder_layers_4_encoder_attn_layer_norm);  model_decoder_layers_4_encoder_attn_layer_norm = None\n",
      "    mul_41 = model_decoder_layers_4_encoder_attn_q_proj * 0.125;  model_decoder_layers_4_encoder_attn_q_proj = None\n",
      "    getitem_80 = getitem_76[0]\n",
      "    getitem_81 = getitem_76[1];  getitem_76 = None\n",
      "    mul_42 = getitem_77 * 12\n",
      "    view_75 = mul_41.view(getitem_77, getitem_78, 12, 64);  mul_41 = None\n",
      "    transpose_37 = view_75.transpose(1, 2);  view_75 = None\n",
      "    contiguous_19 = transpose_37.contiguous();  transpose_37 = None\n",
      "    view_76 = contiguous_19.view(mul_42, -1, 64);  contiguous_19 = None\n",
      "    reshape_27 = getitem_80.reshape(mul_42, -1, 64)\n",
      "    reshape_28 = getitem_81.reshape(mul_42, -1, 64);  mul_42 = None\n",
      "    size_20 = reshape_27.size(1)\n",
      "    transpose_38 = reshape_27.transpose(1, 2);  reshape_27 = None\n",
      "    bmm_18 = torch.bmm(view_76, transpose_38);  view_76 = transpose_38 = None\n",
      "    softmax_9 = torch.nn.functional.softmax(bmm_18, dim = -1, _stacklevel = 3, dtype = None);  bmm_18 = None\n",
      "    view_77 = getitem_69.view(1, -1, 1, 1);  getitem_69 = None\n",
      "    view_78 = softmax_9.view(getitem_77, 12, getitem_78, size_20);  softmax_9 = None\n",
      "    mul_43 = view_77 * view_78;  view_77 = view_78 = None\n",
      "    mul_44 = getitem_77 * 12\n",
      "    view_79 = mul_43.view(mul_44, getitem_78, size_20);  mul_43 = mul_44 = size_20 = None\n",
      "    dropout_27 = torch.nn.functional.dropout(view_79, p = 0.0, training = False, inplace = False);  view_79 = None\n",
      "    bmm_19 = torch.bmm(dropout_27, reshape_28);  dropout_27 = reshape_28 = None\n",
      "    view_80 = bmm_19.view(getitem_77, 12, getitem_78, 64);  bmm_19 = None\n",
      "    transpose_39 = view_80.transpose(1, 2);  view_80 = None\n",
      "    reshape_29 = transpose_39.reshape(getitem_77, getitem_78, 768);  transpose_39 = getitem_77 = getitem_78 = None\n",
      "    model_decoder_layers_4_encoder_attn_out_proj = getattr(self.model.decoder.layers, \"4\").encoder_attn.out_proj(reshape_29);  reshape_29 = None\n",
      "    dropout_28 = torch.nn.functional.dropout(model_decoder_layers_4_encoder_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_4_encoder_attn_out_proj = None\n",
      "    add_20 = add_19 + dropout_28;  add_19 = dropout_28 = None\n",
      "    model_decoder_layers_4_final_layer_norm = getattr(self.model.decoder.layers, \"4\").final_layer_norm(add_20)\n",
      "    model_decoder_layers_4_fc1 = getattr(self.model.decoder.layers, \"4\").fc1(model_decoder_layers_4_final_layer_norm);  model_decoder_layers_4_final_layer_norm = None\n",
      "    gelu_4 = torch._C._nn.gelu(model_decoder_layers_4_fc1);  model_decoder_layers_4_fc1 = None\n",
      "    dropout_29 = torch.nn.functional.dropout(gelu_4, p = 0.0, training = False, inplace = False);  gelu_4 = None\n",
      "    model_decoder_layers_4_fc2 = getattr(self.model.decoder.layers, \"4\").fc2(dropout_29);  dropout_29 = None\n",
      "    dropout_30 = torch.nn.functional.dropout(model_decoder_layers_4_fc2, p = 0.0, training = False, inplace = False);  model_decoder_layers_4_fc2 = None\n",
      "    add_21 = add_20 + dropout_30;  add_20 = dropout_30 = None\n",
      "    getitem_82 = past_key_values[5]\n",
      "    getitem_83 = decoder_head_mask[5]\n",
      "    getitem_84 = cross_attn_head_mask[5]\n",
      "    model_decoder_layers_5_self_attn_layer_norm = getattr(self.model.decoder.layers, \"5\").self_attn_layer_norm(add_21)\n",
      "    getitem_85 = getitem_82[slice(None, 2, None)]\n",
      "    size_21 = model_decoder_layers_5_self_attn_layer_norm.size()\n",
      "    getitem_86 = size_21[0]\n",
      "    getitem_87 = size_21[1]\n",
      "    getitem_88 = size_21[2];  size_21 = None\n",
      "    model_decoder_layers_5_self_attn_q_proj = getattr(self.model.decoder.layers, \"5\").self_attn.q_proj(model_decoder_layers_5_self_attn_layer_norm)\n",
      "    mul_45 = model_decoder_layers_5_self_attn_q_proj * 0.125;  model_decoder_layers_5_self_attn_q_proj = None\n",
      "    model_decoder_layers_5_self_attn_k_proj = getattr(self.model.decoder.layers, \"5\").self_attn.k_proj(model_decoder_layers_5_self_attn_layer_norm)\n",
      "    view_81 = model_decoder_layers_5_self_attn_k_proj.view(getitem_86, -1, 12, 64);  model_decoder_layers_5_self_attn_k_proj = None\n",
      "    transpose_40 = view_81.transpose(1, 2);  view_81 = None\n",
      "    contiguous_20 = transpose_40.contiguous();  transpose_40 = None\n",
      "    model_decoder_layers_5_self_attn_v_proj = getattr(self.model.decoder.layers, \"5\").self_attn.v_proj(model_decoder_layers_5_self_attn_layer_norm);  model_decoder_layers_5_self_attn_layer_norm = None\n",
      "    view_82 = model_decoder_layers_5_self_attn_v_proj.view(getitem_86, -1, 12, 64);  model_decoder_layers_5_self_attn_v_proj = None\n",
      "    transpose_41 = view_82.transpose(1, 2);  view_82 = None\n",
      "    contiguous_21 = transpose_41.contiguous();  transpose_41 = None\n",
      "    getitem_89 = getitem_85[0]\n",
      "    cat_10 = torch.cat([getitem_89, contiguous_20], dim = 2);  getitem_89 = contiguous_20 = None\n",
      "    getitem_90 = getitem_85[1];  getitem_85 = None\n",
      "    cat_11 = torch.cat([getitem_90, contiguous_21], dim = 2);  getitem_90 = contiguous_21 = None\n",
      "    mul_46 = getitem_86 * 12\n",
      "    view_83 = mul_45.view(getitem_86, getitem_87, 12, 64);  mul_45 = None\n",
      "    transpose_42 = view_83.transpose(1, 2);  view_83 = None\n",
      "    contiguous_22 = transpose_42.contiguous();  transpose_42 = None\n",
      "    view_84 = contiguous_22.view(mul_46, -1, 64);  contiguous_22 = None\n",
      "    reshape_30 = cat_10.reshape(mul_46, -1, 64)\n",
      "    reshape_31 = cat_11.reshape(mul_46, -1, 64);  mul_46 = None\n",
      "    size_22 = reshape_30.size(1)\n",
      "    transpose_43 = reshape_30.transpose(1, 2);  reshape_30 = None\n",
      "    bmm_20 = torch.bmm(view_84, transpose_43);  view_84 = transpose_43 = None\n",
      "    view_85 = bmm_20.view(getitem_86, 12, getitem_87, size_22);  bmm_20 = None\n",
      "    add_22 = view_85 + _prepare_decoder_attention_mask;  view_85 = None\n",
      "    mul_47 = getitem_86 * 12\n",
      "    view_86 = add_22.view(mul_47, getitem_87, size_22);  add_22 = mul_47 = None\n",
      "    softmax_10 = torch.nn.functional.softmax(view_86, dim = -1, _stacklevel = 3, dtype = None);  view_86 = None\n",
      "    view_87 = getitem_83.view(1, -1, 1, 1);  getitem_83 = None\n",
      "    view_88 = softmax_10.view(getitem_86, 12, getitem_87, size_22);  softmax_10 = None\n",
      "    mul_48 = view_87 * view_88;  view_87 = view_88 = None\n",
      "    mul_49 = getitem_86 * 12\n",
      "    view_89 = mul_48.view(mul_49, getitem_87, size_22);  mul_48 = mul_49 = size_22 = None\n",
      "    dropout_31 = torch.nn.functional.dropout(view_89, p = 0.0, training = False, inplace = False);  view_89 = None\n",
      "    bmm_21 = torch.bmm(dropout_31, reshape_31);  dropout_31 = reshape_31 = None\n",
      "    view_90 = bmm_21.view(getitem_86, 12, getitem_87, 64);  bmm_21 = None\n",
      "    transpose_44 = view_90.transpose(1, 2);  view_90 = None\n",
      "    reshape_32 = transpose_44.reshape(getitem_86, getitem_87, 768);  transpose_44 = getitem_86 = getitem_87 = None\n",
      "    model_decoder_layers_5_self_attn_out_proj = getattr(self.model.decoder.layers, \"5\").self_attn.out_proj(reshape_32);  reshape_32 = None\n",
      "    dropout_32 = torch.nn.functional.dropout(model_decoder_layers_5_self_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_5_self_attn_out_proj = None\n",
      "    add_23 = add_21 + dropout_32;  add_21 = dropout_32 = None\n",
      "    model_decoder_layers_5_encoder_attn_layer_norm = getattr(self.model.decoder.layers, \"5\").encoder_attn_layer_norm(add_23)\n",
      "    getitem_91 = getitem_82[slice(-2, None, None)];  getitem_82 = None\n",
      "    size_23 = model_decoder_layers_5_encoder_attn_layer_norm.size()\n",
      "    getitem_92 = size_23[0]\n",
      "    getitem_93 = size_23[1]\n",
      "    getitem_94 = size_23[2];  size_23 = None\n",
      "    model_decoder_layers_5_encoder_attn_q_proj = getattr(self.model.decoder.layers, \"5\").encoder_attn.q_proj(model_decoder_layers_5_encoder_attn_layer_norm);  model_decoder_layers_5_encoder_attn_layer_norm = None\n",
      "    mul_50 = model_decoder_layers_5_encoder_attn_q_proj * 0.125;  model_decoder_layers_5_encoder_attn_q_proj = None\n",
      "    getitem_95 = getitem_91[0]\n",
      "    getitem_96 = getitem_91[1];  getitem_91 = None\n",
      "    mul_51 = getitem_92 * 12\n",
      "    view_91 = mul_50.view(getitem_92, getitem_93, 12, 64);  mul_50 = None\n",
      "    transpose_45 = view_91.transpose(1, 2);  view_91 = None\n",
      "    contiguous_23 = transpose_45.contiguous();  transpose_45 = None\n",
      "    view_92 = contiguous_23.view(mul_51, -1, 64);  contiguous_23 = None\n",
      "    reshape_33 = getitem_95.reshape(mul_51, -1, 64)\n",
      "    reshape_34 = getitem_96.reshape(mul_51, -1, 64);  mul_51 = None\n",
      "    size_24 = reshape_33.size(1)\n",
      "    transpose_46 = reshape_33.transpose(1, 2);  reshape_33 = None\n",
      "    bmm_22 = torch.bmm(view_92, transpose_46);  view_92 = transpose_46 = None\n",
      "    softmax_11 = torch.nn.functional.softmax(bmm_22, dim = -1, _stacklevel = 3, dtype = None);  bmm_22 = None\n",
      "    view_93 = getitem_84.view(1, -1, 1, 1);  getitem_84 = None\n",
      "    view_94 = softmax_11.view(getitem_92, 12, getitem_93, size_24);  softmax_11 = None\n",
      "    mul_52 = view_93 * view_94;  view_93 = view_94 = None\n",
      "    mul_53 = getitem_92 * 12\n",
      "    view_95 = mul_52.view(mul_53, getitem_93, size_24);  mul_52 = mul_53 = size_24 = None\n",
      "    dropout_33 = torch.nn.functional.dropout(view_95, p = 0.0, training = False, inplace = False);  view_95 = None\n",
      "    bmm_23 = torch.bmm(dropout_33, reshape_34);  dropout_33 = reshape_34 = None\n",
      "    view_96 = bmm_23.view(getitem_92, 12, getitem_93, 64);  bmm_23 = None\n",
      "    transpose_47 = view_96.transpose(1, 2);  view_96 = None\n",
      "    reshape_35 = transpose_47.reshape(getitem_92, getitem_93, 768);  transpose_47 = getitem_92 = getitem_93 = None\n",
      "    model_decoder_layers_5_encoder_attn_out_proj = getattr(self.model.decoder.layers, \"5\").encoder_attn.out_proj(reshape_35);  reshape_35 = None\n",
      "    dropout_34 = torch.nn.functional.dropout(model_decoder_layers_5_encoder_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_5_encoder_attn_out_proj = None\n",
      "    add_24 = add_23 + dropout_34;  add_23 = dropout_34 = None\n",
      "    model_decoder_layers_5_final_layer_norm = getattr(self.model.decoder.layers, \"5\").final_layer_norm(add_24)\n",
      "    model_decoder_layers_5_fc1 = getattr(self.model.decoder.layers, \"5\").fc1(model_decoder_layers_5_final_layer_norm);  model_decoder_layers_5_final_layer_norm = None\n",
      "    gelu_5 = torch._C._nn.gelu(model_decoder_layers_5_fc1);  model_decoder_layers_5_fc1 = None\n",
      "    dropout_35 = torch.nn.functional.dropout(gelu_5, p = 0.0, training = False, inplace = False);  gelu_5 = None\n",
      "    model_decoder_layers_5_fc2 = getattr(self.model.decoder.layers, \"5\").fc2(dropout_35);  dropout_35 = None\n",
      "    dropout_36 = torch.nn.functional.dropout(model_decoder_layers_5_fc2, p = 0.0, training = False, inplace = False);  model_decoder_layers_5_fc2 = None\n",
      "    add_25 = add_24 + dropout_36;  add_24 = dropout_36 = None\n",
      "    getitem_97 = past_key_values[6]\n",
      "    getitem_98 = decoder_head_mask[6]\n",
      "    getitem_99 = cross_attn_head_mask[6]\n",
      "    model_decoder_layers_6_self_attn_layer_norm = getattr(self.model.decoder.layers, \"6\").self_attn_layer_norm(add_25)\n",
      "    getitem_100 = getitem_97[slice(None, 2, None)]\n",
      "    size_25 = model_decoder_layers_6_self_attn_layer_norm.size()\n",
      "    getitem_101 = size_25[0]\n",
      "    getitem_102 = size_25[1]\n",
      "    getitem_103 = size_25[2];  size_25 = None\n",
      "    model_decoder_layers_6_self_attn_q_proj = getattr(self.model.decoder.layers, \"6\").self_attn.q_proj(model_decoder_layers_6_self_attn_layer_norm)\n",
      "    mul_54 = model_decoder_layers_6_self_attn_q_proj * 0.125;  model_decoder_layers_6_self_attn_q_proj = None\n",
      "    model_decoder_layers_6_self_attn_k_proj = getattr(self.model.decoder.layers, \"6\").self_attn.k_proj(model_decoder_layers_6_self_attn_layer_norm)\n",
      "    view_97 = model_decoder_layers_6_self_attn_k_proj.view(getitem_101, -1, 12, 64);  model_decoder_layers_6_self_attn_k_proj = None\n",
      "    transpose_48 = view_97.transpose(1, 2);  view_97 = None\n",
      "    contiguous_24 = transpose_48.contiguous();  transpose_48 = None\n",
      "    model_decoder_layers_6_self_attn_v_proj = getattr(self.model.decoder.layers, \"6\").self_attn.v_proj(model_decoder_layers_6_self_attn_layer_norm);  model_decoder_layers_6_self_attn_layer_norm = None\n",
      "    view_98 = model_decoder_layers_6_self_attn_v_proj.view(getitem_101, -1, 12, 64);  model_decoder_layers_6_self_attn_v_proj = None\n",
      "    transpose_49 = view_98.transpose(1, 2);  view_98 = None\n",
      "    contiguous_25 = transpose_49.contiguous();  transpose_49 = None\n",
      "    getitem_104 = getitem_100[0]\n",
      "    cat_12 = torch.cat([getitem_104, contiguous_24], dim = 2);  getitem_104 = contiguous_24 = None\n",
      "    getitem_105 = getitem_100[1];  getitem_100 = None\n",
      "    cat_13 = torch.cat([getitem_105, contiguous_25], dim = 2);  getitem_105 = contiguous_25 = None\n",
      "    mul_55 = getitem_101 * 12\n",
      "    view_99 = mul_54.view(getitem_101, getitem_102, 12, 64);  mul_54 = None\n",
      "    transpose_50 = view_99.transpose(1, 2);  view_99 = None\n",
      "    contiguous_26 = transpose_50.contiguous();  transpose_50 = None\n",
      "    view_100 = contiguous_26.view(mul_55, -1, 64);  contiguous_26 = None\n",
      "    reshape_36 = cat_12.reshape(mul_55, -1, 64)\n",
      "    reshape_37 = cat_13.reshape(mul_55, -1, 64);  mul_55 = None\n",
      "    size_26 = reshape_36.size(1)\n",
      "    transpose_51 = reshape_36.transpose(1, 2);  reshape_36 = None\n",
      "    bmm_24 = torch.bmm(view_100, transpose_51);  view_100 = transpose_51 = None\n",
      "    view_101 = bmm_24.view(getitem_101, 12, getitem_102, size_26);  bmm_24 = None\n",
      "    add_26 = view_101 + _prepare_decoder_attention_mask;  view_101 = None\n",
      "    mul_56 = getitem_101 * 12\n",
      "    view_102 = add_26.view(mul_56, getitem_102, size_26);  add_26 = mul_56 = None\n",
      "    softmax_12 = torch.nn.functional.softmax(view_102, dim = -1, _stacklevel = 3, dtype = None);  view_102 = None\n",
      "    view_103 = getitem_98.view(1, -1, 1, 1);  getitem_98 = None\n",
      "    view_104 = softmax_12.view(getitem_101, 12, getitem_102, size_26);  softmax_12 = None\n",
      "    mul_57 = view_103 * view_104;  view_103 = view_104 = None\n",
      "    mul_58 = getitem_101 * 12\n",
      "    view_105 = mul_57.view(mul_58, getitem_102, size_26);  mul_57 = mul_58 = size_26 = None\n",
      "    dropout_37 = torch.nn.functional.dropout(view_105, p = 0.0, training = False, inplace = False);  view_105 = None\n",
      "    bmm_25 = torch.bmm(dropout_37, reshape_37);  dropout_37 = reshape_37 = None\n",
      "    view_106 = bmm_25.view(getitem_101, 12, getitem_102, 64);  bmm_25 = None\n",
      "    transpose_52 = view_106.transpose(1, 2);  view_106 = None\n",
      "    reshape_38 = transpose_52.reshape(getitem_101, getitem_102, 768);  transpose_52 = getitem_101 = getitem_102 = None\n",
      "    model_decoder_layers_6_self_attn_out_proj = getattr(self.model.decoder.layers, \"6\").self_attn.out_proj(reshape_38);  reshape_38 = None\n",
      "    dropout_38 = torch.nn.functional.dropout(model_decoder_layers_6_self_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_6_self_attn_out_proj = None\n",
      "    add_27 = add_25 + dropout_38;  add_25 = dropout_38 = None\n",
      "    model_decoder_layers_6_encoder_attn_layer_norm = getattr(self.model.decoder.layers, \"6\").encoder_attn_layer_norm(add_27)\n",
      "    getitem_106 = getitem_97[slice(-2, None, None)];  getitem_97 = None\n",
      "    size_27 = model_decoder_layers_6_encoder_attn_layer_norm.size()\n",
      "    getitem_107 = size_27[0]\n",
      "    getitem_108 = size_27[1]\n",
      "    getitem_109 = size_27[2];  size_27 = None\n",
      "    model_decoder_layers_6_encoder_attn_q_proj = getattr(self.model.decoder.layers, \"6\").encoder_attn.q_proj(model_decoder_layers_6_encoder_attn_layer_norm);  model_decoder_layers_6_encoder_attn_layer_norm = None\n",
      "    mul_59 = model_decoder_layers_6_encoder_attn_q_proj * 0.125;  model_decoder_layers_6_encoder_attn_q_proj = None\n",
      "    getitem_110 = getitem_106[0]\n",
      "    getitem_111 = getitem_106[1];  getitem_106 = None\n",
      "    mul_60 = getitem_107 * 12\n",
      "    view_107 = mul_59.view(getitem_107, getitem_108, 12, 64);  mul_59 = None\n",
      "    transpose_53 = view_107.transpose(1, 2);  view_107 = None\n",
      "    contiguous_27 = transpose_53.contiguous();  transpose_53 = None\n",
      "    view_108 = contiguous_27.view(mul_60, -1, 64);  contiguous_27 = None\n",
      "    reshape_39 = getitem_110.reshape(mul_60, -1, 64)\n",
      "    reshape_40 = getitem_111.reshape(mul_60, -1, 64);  mul_60 = None\n",
      "    size_28 = reshape_39.size(1)\n",
      "    transpose_54 = reshape_39.transpose(1, 2);  reshape_39 = None\n",
      "    bmm_26 = torch.bmm(view_108, transpose_54);  view_108 = transpose_54 = None\n",
      "    softmax_13 = torch.nn.functional.softmax(bmm_26, dim = -1, _stacklevel = 3, dtype = None);  bmm_26 = None\n",
      "    view_109 = getitem_99.view(1, -1, 1, 1);  getitem_99 = None\n",
      "    view_110 = softmax_13.view(getitem_107, 12, getitem_108, size_28);  softmax_13 = None\n",
      "    mul_61 = view_109 * view_110;  view_109 = view_110 = None\n",
      "    mul_62 = getitem_107 * 12\n",
      "    view_111 = mul_61.view(mul_62, getitem_108, size_28);  mul_61 = mul_62 = size_28 = None\n",
      "    dropout_39 = torch.nn.functional.dropout(view_111, p = 0.0, training = False, inplace = False);  view_111 = None\n",
      "    bmm_27 = torch.bmm(dropout_39, reshape_40);  dropout_39 = reshape_40 = None\n",
      "    view_112 = bmm_27.view(getitem_107, 12, getitem_108, 64);  bmm_27 = None\n",
      "    transpose_55 = view_112.transpose(1, 2);  view_112 = None\n",
      "    reshape_41 = transpose_55.reshape(getitem_107, getitem_108, 768);  transpose_55 = getitem_107 = getitem_108 = None\n",
      "    model_decoder_layers_6_encoder_attn_out_proj = getattr(self.model.decoder.layers, \"6\").encoder_attn.out_proj(reshape_41);  reshape_41 = None\n",
      "    dropout_40 = torch.nn.functional.dropout(model_decoder_layers_6_encoder_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_6_encoder_attn_out_proj = None\n",
      "    add_28 = add_27 + dropout_40;  add_27 = dropout_40 = None\n",
      "    model_decoder_layers_6_final_layer_norm = getattr(self.model.decoder.layers, \"6\").final_layer_norm(add_28)\n",
      "    model_decoder_layers_6_fc1 = getattr(self.model.decoder.layers, \"6\").fc1(model_decoder_layers_6_final_layer_norm);  model_decoder_layers_6_final_layer_norm = None\n",
      "    gelu_6 = torch._C._nn.gelu(model_decoder_layers_6_fc1);  model_decoder_layers_6_fc1 = None\n",
      "    dropout_41 = torch.nn.functional.dropout(gelu_6, p = 0.0, training = False, inplace = False);  gelu_6 = None\n",
      "    model_decoder_layers_6_fc2 = getattr(self.model.decoder.layers, \"6\").fc2(dropout_41);  dropout_41 = None\n",
      "    dropout_42 = torch.nn.functional.dropout(model_decoder_layers_6_fc2, p = 0.0, training = False, inplace = False);  model_decoder_layers_6_fc2 = None\n",
      "    add_29 = add_28 + dropout_42;  add_28 = dropout_42 = None\n",
      "    getitem_112 = past_key_values[7]\n",
      "    getitem_113 = decoder_head_mask[7]\n",
      "    getitem_114 = cross_attn_head_mask[7]\n",
      "    model_decoder_layers_7_self_attn_layer_norm = getattr(self.model.decoder.layers, \"7\").self_attn_layer_norm(add_29)\n",
      "    getitem_115 = getitem_112[slice(None, 2, None)]\n",
      "    size_29 = model_decoder_layers_7_self_attn_layer_norm.size()\n",
      "    getitem_116 = size_29[0]\n",
      "    getitem_117 = size_29[1]\n",
      "    getitem_118 = size_29[2];  size_29 = None\n",
      "    model_decoder_layers_7_self_attn_q_proj = getattr(self.model.decoder.layers, \"7\").self_attn.q_proj(model_decoder_layers_7_self_attn_layer_norm)\n",
      "    mul_63 = model_decoder_layers_7_self_attn_q_proj * 0.125;  model_decoder_layers_7_self_attn_q_proj = None\n",
      "    model_decoder_layers_7_self_attn_k_proj = getattr(self.model.decoder.layers, \"7\").self_attn.k_proj(model_decoder_layers_7_self_attn_layer_norm)\n",
      "    view_113 = model_decoder_layers_7_self_attn_k_proj.view(getitem_116, -1, 12, 64);  model_decoder_layers_7_self_attn_k_proj = None\n",
      "    transpose_56 = view_113.transpose(1, 2);  view_113 = None\n",
      "    contiguous_28 = transpose_56.contiguous();  transpose_56 = None\n",
      "    model_decoder_layers_7_self_attn_v_proj = getattr(self.model.decoder.layers, \"7\").self_attn.v_proj(model_decoder_layers_7_self_attn_layer_norm);  model_decoder_layers_7_self_attn_layer_norm = None\n",
      "    view_114 = model_decoder_layers_7_self_attn_v_proj.view(getitem_116, -1, 12, 64);  model_decoder_layers_7_self_attn_v_proj = None\n",
      "    transpose_57 = view_114.transpose(1, 2);  view_114 = None\n",
      "    contiguous_29 = transpose_57.contiguous();  transpose_57 = None\n",
      "    getitem_119 = getitem_115[0]\n",
      "    cat_14 = torch.cat([getitem_119, contiguous_28], dim = 2);  getitem_119 = contiguous_28 = None\n",
      "    getitem_120 = getitem_115[1];  getitem_115 = None\n",
      "    cat_15 = torch.cat([getitem_120, contiguous_29], dim = 2);  getitem_120 = contiguous_29 = None\n",
      "    mul_64 = getitem_116 * 12\n",
      "    view_115 = mul_63.view(getitem_116, getitem_117, 12, 64);  mul_63 = None\n",
      "    transpose_58 = view_115.transpose(1, 2);  view_115 = None\n",
      "    contiguous_30 = transpose_58.contiguous();  transpose_58 = None\n",
      "    view_116 = contiguous_30.view(mul_64, -1, 64);  contiguous_30 = None\n",
      "    reshape_42 = cat_14.reshape(mul_64, -1, 64)\n",
      "    reshape_43 = cat_15.reshape(mul_64, -1, 64);  mul_64 = None\n",
      "    size_30 = reshape_42.size(1)\n",
      "    transpose_59 = reshape_42.transpose(1, 2);  reshape_42 = None\n",
      "    bmm_28 = torch.bmm(view_116, transpose_59);  view_116 = transpose_59 = None\n",
      "    view_117 = bmm_28.view(getitem_116, 12, getitem_117, size_30);  bmm_28 = None\n",
      "    add_30 = view_117 + _prepare_decoder_attention_mask;  view_117 = None\n",
      "    mul_65 = getitem_116 * 12\n",
      "    view_118 = add_30.view(mul_65, getitem_117, size_30);  add_30 = mul_65 = None\n",
      "    softmax_14 = torch.nn.functional.softmax(view_118, dim = -1, _stacklevel = 3, dtype = None);  view_118 = None\n",
      "    view_119 = getitem_113.view(1, -1, 1, 1);  getitem_113 = None\n",
      "    view_120 = softmax_14.view(getitem_116, 12, getitem_117, size_30);  softmax_14 = None\n",
      "    mul_66 = view_119 * view_120;  view_119 = view_120 = None\n",
      "    mul_67 = getitem_116 * 12\n",
      "    view_121 = mul_66.view(mul_67, getitem_117, size_30);  mul_66 = mul_67 = size_30 = None\n",
      "    dropout_43 = torch.nn.functional.dropout(view_121, p = 0.0, training = False, inplace = False);  view_121 = None\n",
      "    bmm_29 = torch.bmm(dropout_43, reshape_43);  dropout_43 = reshape_43 = None\n",
      "    view_122 = bmm_29.view(getitem_116, 12, getitem_117, 64);  bmm_29 = None\n",
      "    transpose_60 = view_122.transpose(1, 2);  view_122 = None\n",
      "    reshape_44 = transpose_60.reshape(getitem_116, getitem_117, 768);  transpose_60 = getitem_116 = getitem_117 = None\n",
      "    model_decoder_layers_7_self_attn_out_proj = getattr(self.model.decoder.layers, \"7\").self_attn.out_proj(reshape_44);  reshape_44 = None\n",
      "    dropout_44 = torch.nn.functional.dropout(model_decoder_layers_7_self_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_7_self_attn_out_proj = None\n",
      "    add_31 = add_29 + dropout_44;  add_29 = dropout_44 = None\n",
      "    model_decoder_layers_7_encoder_attn_layer_norm = getattr(self.model.decoder.layers, \"7\").encoder_attn_layer_norm(add_31)\n",
      "    getitem_121 = getitem_112[slice(-2, None, None)];  getitem_112 = None\n",
      "    size_31 = model_decoder_layers_7_encoder_attn_layer_norm.size()\n",
      "    getitem_122 = size_31[0]\n",
      "    getitem_123 = size_31[1]\n",
      "    getitem_124 = size_31[2];  size_31 = None\n",
      "    model_decoder_layers_7_encoder_attn_q_proj = getattr(self.model.decoder.layers, \"7\").encoder_attn.q_proj(model_decoder_layers_7_encoder_attn_layer_norm);  model_decoder_layers_7_encoder_attn_layer_norm = None\n",
      "    mul_68 = model_decoder_layers_7_encoder_attn_q_proj * 0.125;  model_decoder_layers_7_encoder_attn_q_proj = None\n",
      "    getitem_125 = getitem_121[0]\n",
      "    getitem_126 = getitem_121[1];  getitem_121 = None\n",
      "    mul_69 = getitem_122 * 12\n",
      "    view_123 = mul_68.view(getitem_122, getitem_123, 12, 64);  mul_68 = None\n",
      "    transpose_61 = view_123.transpose(1, 2);  view_123 = None\n",
      "    contiguous_31 = transpose_61.contiguous();  transpose_61 = None\n",
      "    view_124 = contiguous_31.view(mul_69, -1, 64);  contiguous_31 = None\n",
      "    reshape_45 = getitem_125.reshape(mul_69, -1, 64)\n",
      "    reshape_46 = getitem_126.reshape(mul_69, -1, 64);  mul_69 = None\n",
      "    size_32 = reshape_45.size(1)\n",
      "    transpose_62 = reshape_45.transpose(1, 2);  reshape_45 = None\n",
      "    bmm_30 = torch.bmm(view_124, transpose_62);  view_124 = transpose_62 = None\n",
      "    softmax_15 = torch.nn.functional.softmax(bmm_30, dim = -1, _stacklevel = 3, dtype = None);  bmm_30 = None\n",
      "    view_125 = getitem_114.view(1, -1, 1, 1);  getitem_114 = None\n",
      "    view_126 = softmax_15.view(getitem_122, 12, getitem_123, size_32);  softmax_15 = None\n",
      "    mul_70 = view_125 * view_126;  view_125 = view_126 = None\n",
      "    mul_71 = getitem_122 * 12\n",
      "    view_127 = mul_70.view(mul_71, getitem_123, size_32);  mul_70 = mul_71 = size_32 = None\n",
      "    dropout_45 = torch.nn.functional.dropout(view_127, p = 0.0, training = False, inplace = False);  view_127 = None\n",
      "    bmm_31 = torch.bmm(dropout_45, reshape_46);  dropout_45 = reshape_46 = None\n",
      "    view_128 = bmm_31.view(getitem_122, 12, getitem_123, 64);  bmm_31 = None\n",
      "    transpose_63 = view_128.transpose(1, 2);  view_128 = None\n",
      "    reshape_47 = transpose_63.reshape(getitem_122, getitem_123, 768);  transpose_63 = getitem_122 = getitem_123 = None\n",
      "    model_decoder_layers_7_encoder_attn_out_proj = getattr(self.model.decoder.layers, \"7\").encoder_attn.out_proj(reshape_47);  reshape_47 = None\n",
      "    dropout_46 = torch.nn.functional.dropout(model_decoder_layers_7_encoder_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_7_encoder_attn_out_proj = None\n",
      "    add_32 = add_31 + dropout_46;  add_31 = dropout_46 = None\n",
      "    model_decoder_layers_7_final_layer_norm = getattr(self.model.decoder.layers, \"7\").final_layer_norm(add_32)\n",
      "    model_decoder_layers_7_fc1 = getattr(self.model.decoder.layers, \"7\").fc1(model_decoder_layers_7_final_layer_norm);  model_decoder_layers_7_final_layer_norm = None\n",
      "    gelu_7 = torch._C._nn.gelu(model_decoder_layers_7_fc1);  model_decoder_layers_7_fc1 = None\n",
      "    dropout_47 = torch.nn.functional.dropout(gelu_7, p = 0.0, training = False, inplace = False);  gelu_7 = None\n",
      "    model_decoder_layers_7_fc2 = getattr(self.model.decoder.layers, \"7\").fc2(dropout_47);  dropout_47 = None\n",
      "    dropout_48 = torch.nn.functional.dropout(model_decoder_layers_7_fc2, p = 0.0, training = False, inplace = False);  model_decoder_layers_7_fc2 = None\n",
      "    add_33 = add_32 + dropout_48;  add_32 = dropout_48 = None\n",
      "    getitem_127 = past_key_values[8]\n",
      "    getitem_128 = decoder_head_mask[8]\n",
      "    getitem_129 = cross_attn_head_mask[8]\n",
      "    model_decoder_layers_8_self_attn_layer_norm = getattr(self.model.decoder.layers, \"8\").self_attn_layer_norm(add_33)\n",
      "    getitem_130 = getitem_127[slice(None, 2, None)]\n",
      "    size_33 = model_decoder_layers_8_self_attn_layer_norm.size()\n",
      "    getitem_131 = size_33[0]\n",
      "    getitem_132 = size_33[1]\n",
      "    getitem_133 = size_33[2];  size_33 = None\n",
      "    model_decoder_layers_8_self_attn_q_proj = getattr(self.model.decoder.layers, \"8\").self_attn.q_proj(model_decoder_layers_8_self_attn_layer_norm)\n",
      "    mul_72 = model_decoder_layers_8_self_attn_q_proj * 0.125;  model_decoder_layers_8_self_attn_q_proj = None\n",
      "    model_decoder_layers_8_self_attn_k_proj = getattr(self.model.decoder.layers, \"8\").self_attn.k_proj(model_decoder_layers_8_self_attn_layer_norm)\n",
      "    view_129 = model_decoder_layers_8_self_attn_k_proj.view(getitem_131, -1, 12, 64);  model_decoder_layers_8_self_attn_k_proj = None\n",
      "    transpose_64 = view_129.transpose(1, 2);  view_129 = None\n",
      "    contiguous_32 = transpose_64.contiguous();  transpose_64 = None\n",
      "    model_decoder_layers_8_self_attn_v_proj = getattr(self.model.decoder.layers, \"8\").self_attn.v_proj(model_decoder_layers_8_self_attn_layer_norm);  model_decoder_layers_8_self_attn_layer_norm = None\n",
      "    view_130 = model_decoder_layers_8_self_attn_v_proj.view(getitem_131, -1, 12, 64);  model_decoder_layers_8_self_attn_v_proj = None\n",
      "    transpose_65 = view_130.transpose(1, 2);  view_130 = None\n",
      "    contiguous_33 = transpose_65.contiguous();  transpose_65 = None\n",
      "    getitem_134 = getitem_130[0]\n",
      "    cat_16 = torch.cat([getitem_134, contiguous_32], dim = 2);  getitem_134 = contiguous_32 = None\n",
      "    getitem_135 = getitem_130[1];  getitem_130 = None\n",
      "    cat_17 = torch.cat([getitem_135, contiguous_33], dim = 2);  getitem_135 = contiguous_33 = None\n",
      "    mul_73 = getitem_131 * 12\n",
      "    view_131 = mul_72.view(getitem_131, getitem_132, 12, 64);  mul_72 = None\n",
      "    transpose_66 = view_131.transpose(1, 2);  view_131 = None\n",
      "    contiguous_34 = transpose_66.contiguous();  transpose_66 = None\n",
      "    view_132 = contiguous_34.view(mul_73, -1, 64);  contiguous_34 = None\n",
      "    reshape_48 = cat_16.reshape(mul_73, -1, 64)\n",
      "    reshape_49 = cat_17.reshape(mul_73, -1, 64);  mul_73 = None\n",
      "    size_34 = reshape_48.size(1)\n",
      "    transpose_67 = reshape_48.transpose(1, 2);  reshape_48 = None\n",
      "    bmm_32 = torch.bmm(view_132, transpose_67);  view_132 = transpose_67 = None\n",
      "    view_133 = bmm_32.view(getitem_131, 12, getitem_132, size_34);  bmm_32 = None\n",
      "    add_34 = view_133 + _prepare_decoder_attention_mask;  view_133 = None\n",
      "    mul_74 = getitem_131 * 12\n",
      "    view_134 = add_34.view(mul_74, getitem_132, size_34);  add_34 = mul_74 = None\n",
      "    softmax_16 = torch.nn.functional.softmax(view_134, dim = -1, _stacklevel = 3, dtype = None);  view_134 = None\n",
      "    view_135 = getitem_128.view(1, -1, 1, 1);  getitem_128 = None\n",
      "    view_136 = softmax_16.view(getitem_131, 12, getitem_132, size_34);  softmax_16 = None\n",
      "    mul_75 = view_135 * view_136;  view_135 = view_136 = None\n",
      "    mul_76 = getitem_131 * 12\n",
      "    view_137 = mul_75.view(mul_76, getitem_132, size_34);  mul_75 = mul_76 = size_34 = None\n",
      "    dropout_49 = torch.nn.functional.dropout(view_137, p = 0.0, training = False, inplace = False);  view_137 = None\n",
      "    bmm_33 = torch.bmm(dropout_49, reshape_49);  dropout_49 = reshape_49 = None\n",
      "    view_138 = bmm_33.view(getitem_131, 12, getitem_132, 64);  bmm_33 = None\n",
      "    transpose_68 = view_138.transpose(1, 2);  view_138 = None\n",
      "    reshape_50 = transpose_68.reshape(getitem_131, getitem_132, 768);  transpose_68 = getitem_131 = getitem_132 = None\n",
      "    model_decoder_layers_8_self_attn_out_proj = getattr(self.model.decoder.layers, \"8\").self_attn.out_proj(reshape_50);  reshape_50 = None\n",
      "    dropout_50 = torch.nn.functional.dropout(model_decoder_layers_8_self_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_8_self_attn_out_proj = None\n",
      "    add_35 = add_33 + dropout_50;  add_33 = dropout_50 = None\n",
      "    model_decoder_layers_8_encoder_attn_layer_norm = getattr(self.model.decoder.layers, \"8\").encoder_attn_layer_norm(add_35)\n",
      "    getitem_136 = getitem_127[slice(-2, None, None)];  getitem_127 = None\n",
      "    size_35 = model_decoder_layers_8_encoder_attn_layer_norm.size()\n",
      "    getitem_137 = size_35[0]\n",
      "    getitem_138 = size_35[1]\n",
      "    getitem_139 = size_35[2];  size_35 = None\n",
      "    model_decoder_layers_8_encoder_attn_q_proj = getattr(self.model.decoder.layers, \"8\").encoder_attn.q_proj(model_decoder_layers_8_encoder_attn_layer_norm);  model_decoder_layers_8_encoder_attn_layer_norm = None\n",
      "    mul_77 = model_decoder_layers_8_encoder_attn_q_proj * 0.125;  model_decoder_layers_8_encoder_attn_q_proj = None\n",
      "    getitem_140 = getitem_136[0]\n",
      "    getitem_141 = getitem_136[1];  getitem_136 = None\n",
      "    mul_78 = getitem_137 * 12\n",
      "    view_139 = mul_77.view(getitem_137, getitem_138, 12, 64);  mul_77 = None\n",
      "    transpose_69 = view_139.transpose(1, 2);  view_139 = None\n",
      "    contiguous_35 = transpose_69.contiguous();  transpose_69 = None\n",
      "    view_140 = contiguous_35.view(mul_78, -1, 64);  contiguous_35 = None\n",
      "    reshape_51 = getitem_140.reshape(mul_78, -1, 64)\n",
      "    reshape_52 = getitem_141.reshape(mul_78, -1, 64);  mul_78 = None\n",
      "    size_36 = reshape_51.size(1)\n",
      "    transpose_70 = reshape_51.transpose(1, 2);  reshape_51 = None\n",
      "    bmm_34 = torch.bmm(view_140, transpose_70);  view_140 = transpose_70 = None\n",
      "    softmax_17 = torch.nn.functional.softmax(bmm_34, dim = -1, _stacklevel = 3, dtype = None);  bmm_34 = None\n",
      "    view_141 = getitem_129.view(1, -1, 1, 1);  getitem_129 = None\n",
      "    view_142 = softmax_17.view(getitem_137, 12, getitem_138, size_36);  softmax_17 = None\n",
      "    mul_79 = view_141 * view_142;  view_141 = view_142 = None\n",
      "    mul_80 = getitem_137 * 12\n",
      "    view_143 = mul_79.view(mul_80, getitem_138, size_36);  mul_79 = mul_80 = size_36 = None\n",
      "    dropout_51 = torch.nn.functional.dropout(view_143, p = 0.0, training = False, inplace = False);  view_143 = None\n",
      "    bmm_35 = torch.bmm(dropout_51, reshape_52);  dropout_51 = reshape_52 = None\n",
      "    view_144 = bmm_35.view(getitem_137, 12, getitem_138, 64);  bmm_35 = None\n",
      "    transpose_71 = view_144.transpose(1, 2);  view_144 = None\n",
      "    reshape_53 = transpose_71.reshape(getitem_137, getitem_138, 768);  transpose_71 = getitem_137 = getitem_138 = None\n",
      "    model_decoder_layers_8_encoder_attn_out_proj = getattr(self.model.decoder.layers, \"8\").encoder_attn.out_proj(reshape_53);  reshape_53 = None\n",
      "    dropout_52 = torch.nn.functional.dropout(model_decoder_layers_8_encoder_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_8_encoder_attn_out_proj = None\n",
      "    add_36 = add_35 + dropout_52;  add_35 = dropout_52 = None\n",
      "    model_decoder_layers_8_final_layer_norm = getattr(self.model.decoder.layers, \"8\").final_layer_norm(add_36)\n",
      "    model_decoder_layers_8_fc1 = getattr(self.model.decoder.layers, \"8\").fc1(model_decoder_layers_8_final_layer_norm);  model_decoder_layers_8_final_layer_norm = None\n",
      "    gelu_8 = torch._C._nn.gelu(model_decoder_layers_8_fc1);  model_decoder_layers_8_fc1 = None\n",
      "    dropout_53 = torch.nn.functional.dropout(gelu_8, p = 0.0, training = False, inplace = False);  gelu_8 = None\n",
      "    model_decoder_layers_8_fc2 = getattr(self.model.decoder.layers, \"8\").fc2(dropout_53);  dropout_53 = None\n",
      "    dropout_54 = torch.nn.functional.dropout(model_decoder_layers_8_fc2, p = 0.0, training = False, inplace = False);  model_decoder_layers_8_fc2 = None\n",
      "    add_37 = add_36 + dropout_54;  add_36 = dropout_54 = None\n",
      "    getitem_142 = past_key_values[9]\n",
      "    getitem_143 = decoder_head_mask[9]\n",
      "    getitem_144 = cross_attn_head_mask[9]\n",
      "    model_decoder_layers_9_self_attn_layer_norm = getattr(self.model.decoder.layers, \"9\").self_attn_layer_norm(add_37)\n",
      "    getitem_145 = getitem_142[slice(None, 2, None)]\n",
      "    size_37 = model_decoder_layers_9_self_attn_layer_norm.size()\n",
      "    getitem_146 = size_37[0]\n",
      "    getitem_147 = size_37[1]\n",
      "    getitem_148 = size_37[2];  size_37 = None\n",
      "    model_decoder_layers_9_self_attn_q_proj = getattr(self.model.decoder.layers, \"9\").self_attn.q_proj(model_decoder_layers_9_self_attn_layer_norm)\n",
      "    mul_81 = model_decoder_layers_9_self_attn_q_proj * 0.125;  model_decoder_layers_9_self_attn_q_proj = None\n",
      "    model_decoder_layers_9_self_attn_k_proj = getattr(self.model.decoder.layers, \"9\").self_attn.k_proj(model_decoder_layers_9_self_attn_layer_norm)\n",
      "    view_145 = model_decoder_layers_9_self_attn_k_proj.view(getitem_146, -1, 12, 64);  model_decoder_layers_9_self_attn_k_proj = None\n",
      "    transpose_72 = view_145.transpose(1, 2);  view_145 = None\n",
      "    contiguous_36 = transpose_72.contiguous();  transpose_72 = None\n",
      "    model_decoder_layers_9_self_attn_v_proj = getattr(self.model.decoder.layers, \"9\").self_attn.v_proj(model_decoder_layers_9_self_attn_layer_norm);  model_decoder_layers_9_self_attn_layer_norm = None\n",
      "    view_146 = model_decoder_layers_9_self_attn_v_proj.view(getitem_146, -1, 12, 64);  model_decoder_layers_9_self_attn_v_proj = None\n",
      "    transpose_73 = view_146.transpose(1, 2);  view_146 = None\n",
      "    contiguous_37 = transpose_73.contiguous();  transpose_73 = None\n",
      "    getitem_149 = getitem_145[0]\n",
      "    cat_18 = torch.cat([getitem_149, contiguous_36], dim = 2);  getitem_149 = contiguous_36 = None\n",
      "    getitem_150 = getitem_145[1];  getitem_145 = None\n",
      "    cat_19 = torch.cat([getitem_150, contiguous_37], dim = 2);  getitem_150 = contiguous_37 = None\n",
      "    mul_82 = getitem_146 * 12\n",
      "    view_147 = mul_81.view(getitem_146, getitem_147, 12, 64);  mul_81 = None\n",
      "    transpose_74 = view_147.transpose(1, 2);  view_147 = None\n",
      "    contiguous_38 = transpose_74.contiguous();  transpose_74 = None\n",
      "    view_148 = contiguous_38.view(mul_82, -1, 64);  contiguous_38 = None\n",
      "    reshape_54 = cat_18.reshape(mul_82, -1, 64)\n",
      "    reshape_55 = cat_19.reshape(mul_82, -1, 64);  mul_82 = None\n",
      "    size_38 = reshape_54.size(1)\n",
      "    transpose_75 = reshape_54.transpose(1, 2);  reshape_54 = None\n",
      "    bmm_36 = torch.bmm(view_148, transpose_75);  view_148 = transpose_75 = None\n",
      "    view_149 = bmm_36.view(getitem_146, 12, getitem_147, size_38);  bmm_36 = None\n",
      "    add_38 = view_149 + _prepare_decoder_attention_mask;  view_149 = None\n",
      "    mul_83 = getitem_146 * 12\n",
      "    view_150 = add_38.view(mul_83, getitem_147, size_38);  add_38 = mul_83 = None\n",
      "    softmax_18 = torch.nn.functional.softmax(view_150, dim = -1, _stacklevel = 3, dtype = None);  view_150 = None\n",
      "    view_151 = getitem_143.view(1, -1, 1, 1);  getitem_143 = None\n",
      "    view_152 = softmax_18.view(getitem_146, 12, getitem_147, size_38);  softmax_18 = None\n",
      "    mul_84 = view_151 * view_152;  view_151 = view_152 = None\n",
      "    mul_85 = getitem_146 * 12\n",
      "    view_153 = mul_84.view(mul_85, getitem_147, size_38);  mul_84 = mul_85 = size_38 = None\n",
      "    dropout_55 = torch.nn.functional.dropout(view_153, p = 0.0, training = False, inplace = False);  view_153 = None\n",
      "    bmm_37 = torch.bmm(dropout_55, reshape_55);  dropout_55 = reshape_55 = None\n",
      "    view_154 = bmm_37.view(getitem_146, 12, getitem_147, 64);  bmm_37 = None\n",
      "    transpose_76 = view_154.transpose(1, 2);  view_154 = None\n",
      "    reshape_56 = transpose_76.reshape(getitem_146, getitem_147, 768);  transpose_76 = getitem_146 = getitem_147 = None\n",
      "    model_decoder_layers_9_self_attn_out_proj = getattr(self.model.decoder.layers, \"9\").self_attn.out_proj(reshape_56);  reshape_56 = None\n",
      "    dropout_56 = torch.nn.functional.dropout(model_decoder_layers_9_self_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_9_self_attn_out_proj = None\n",
      "    add_39 = add_37 + dropout_56;  add_37 = dropout_56 = None\n",
      "    model_decoder_layers_9_encoder_attn_layer_norm = getattr(self.model.decoder.layers, \"9\").encoder_attn_layer_norm(add_39)\n",
      "    getitem_151 = getitem_142[slice(-2, None, None)];  getitem_142 = None\n",
      "    size_39 = model_decoder_layers_9_encoder_attn_layer_norm.size()\n",
      "    getitem_152 = size_39[0]\n",
      "    getitem_153 = size_39[1]\n",
      "    getitem_154 = size_39[2];  size_39 = None\n",
      "    model_decoder_layers_9_encoder_attn_q_proj = getattr(self.model.decoder.layers, \"9\").encoder_attn.q_proj(model_decoder_layers_9_encoder_attn_layer_norm);  model_decoder_layers_9_encoder_attn_layer_norm = None\n",
      "    mul_86 = model_decoder_layers_9_encoder_attn_q_proj * 0.125;  model_decoder_layers_9_encoder_attn_q_proj = None\n",
      "    getitem_155 = getitem_151[0]\n",
      "    getitem_156 = getitem_151[1];  getitem_151 = None\n",
      "    mul_87 = getitem_152 * 12\n",
      "    view_155 = mul_86.view(getitem_152, getitem_153, 12, 64);  mul_86 = None\n",
      "    transpose_77 = view_155.transpose(1, 2);  view_155 = None\n",
      "    contiguous_39 = transpose_77.contiguous();  transpose_77 = None\n",
      "    view_156 = contiguous_39.view(mul_87, -1, 64);  contiguous_39 = None\n",
      "    reshape_57 = getitem_155.reshape(mul_87, -1, 64)\n",
      "    reshape_58 = getitem_156.reshape(mul_87, -1, 64);  mul_87 = None\n",
      "    size_40 = reshape_57.size(1)\n",
      "    transpose_78 = reshape_57.transpose(1, 2);  reshape_57 = None\n",
      "    bmm_38 = torch.bmm(view_156, transpose_78);  view_156 = transpose_78 = None\n",
      "    softmax_19 = torch.nn.functional.softmax(bmm_38, dim = -1, _stacklevel = 3, dtype = None);  bmm_38 = None\n",
      "    view_157 = getitem_144.view(1, -1, 1, 1);  getitem_144 = None\n",
      "    view_158 = softmax_19.view(getitem_152, 12, getitem_153, size_40);  softmax_19 = None\n",
      "    mul_88 = view_157 * view_158;  view_157 = view_158 = None\n",
      "    mul_89 = getitem_152 * 12\n",
      "    view_159 = mul_88.view(mul_89, getitem_153, size_40);  mul_88 = mul_89 = size_40 = None\n",
      "    dropout_57 = torch.nn.functional.dropout(view_159, p = 0.0, training = False, inplace = False);  view_159 = None\n",
      "    bmm_39 = torch.bmm(dropout_57, reshape_58);  dropout_57 = reshape_58 = None\n",
      "    view_160 = bmm_39.view(getitem_152, 12, getitem_153, 64);  bmm_39 = None\n",
      "    transpose_79 = view_160.transpose(1, 2);  view_160 = None\n",
      "    reshape_59 = transpose_79.reshape(getitem_152, getitem_153, 768);  transpose_79 = getitem_152 = getitem_153 = None\n",
      "    model_decoder_layers_9_encoder_attn_out_proj = getattr(self.model.decoder.layers, \"9\").encoder_attn.out_proj(reshape_59);  reshape_59 = None\n",
      "    dropout_58 = torch.nn.functional.dropout(model_decoder_layers_9_encoder_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_9_encoder_attn_out_proj = None\n",
      "    add_40 = add_39 + dropout_58;  add_39 = dropout_58 = None\n",
      "    model_decoder_layers_9_final_layer_norm = getattr(self.model.decoder.layers, \"9\").final_layer_norm(add_40)\n",
      "    model_decoder_layers_9_fc1 = getattr(self.model.decoder.layers, \"9\").fc1(model_decoder_layers_9_final_layer_norm);  model_decoder_layers_9_final_layer_norm = None\n",
      "    gelu_9 = torch._C._nn.gelu(model_decoder_layers_9_fc1);  model_decoder_layers_9_fc1 = None\n",
      "    dropout_59 = torch.nn.functional.dropout(gelu_9, p = 0.0, training = False, inplace = False);  gelu_9 = None\n",
      "    model_decoder_layers_9_fc2 = getattr(self.model.decoder.layers, \"9\").fc2(dropout_59);  dropout_59 = None\n",
      "    dropout_60 = torch.nn.functional.dropout(model_decoder_layers_9_fc2, p = 0.0, training = False, inplace = False);  model_decoder_layers_9_fc2 = None\n",
      "    add_41 = add_40 + dropout_60;  add_40 = dropout_60 = None\n",
      "    getitem_157 = past_key_values[10]\n",
      "    getitem_158 = decoder_head_mask[10]\n",
      "    getitem_159 = cross_attn_head_mask[10]\n",
      "    model_decoder_layers_10_self_attn_layer_norm = getattr(self.model.decoder.layers, \"10\").self_attn_layer_norm(add_41)\n",
      "    getitem_160 = getitem_157[slice(None, 2, None)]\n",
      "    size_41 = model_decoder_layers_10_self_attn_layer_norm.size()\n",
      "    getitem_161 = size_41[0]\n",
      "    getitem_162 = size_41[1]\n",
      "    getitem_163 = size_41[2];  size_41 = None\n",
      "    model_decoder_layers_10_self_attn_q_proj = getattr(self.model.decoder.layers, \"10\").self_attn.q_proj(model_decoder_layers_10_self_attn_layer_norm)\n",
      "    mul_90 = model_decoder_layers_10_self_attn_q_proj * 0.125;  model_decoder_layers_10_self_attn_q_proj = None\n",
      "    model_decoder_layers_10_self_attn_k_proj = getattr(self.model.decoder.layers, \"10\").self_attn.k_proj(model_decoder_layers_10_self_attn_layer_norm)\n",
      "    view_161 = model_decoder_layers_10_self_attn_k_proj.view(getitem_161, -1, 12, 64);  model_decoder_layers_10_self_attn_k_proj = None\n",
      "    transpose_80 = view_161.transpose(1, 2);  view_161 = None\n",
      "    contiguous_40 = transpose_80.contiguous();  transpose_80 = None\n",
      "    model_decoder_layers_10_self_attn_v_proj = getattr(self.model.decoder.layers, \"10\").self_attn.v_proj(model_decoder_layers_10_self_attn_layer_norm);  model_decoder_layers_10_self_attn_layer_norm = None\n",
      "    view_162 = model_decoder_layers_10_self_attn_v_proj.view(getitem_161, -1, 12, 64);  model_decoder_layers_10_self_attn_v_proj = None\n",
      "    transpose_81 = view_162.transpose(1, 2);  view_162 = None\n",
      "    contiguous_41 = transpose_81.contiguous();  transpose_81 = None\n",
      "    getitem_164 = getitem_160[0]\n",
      "    cat_20 = torch.cat([getitem_164, contiguous_40], dim = 2);  getitem_164 = contiguous_40 = None\n",
      "    getitem_165 = getitem_160[1];  getitem_160 = None\n",
      "    cat_21 = torch.cat([getitem_165, contiguous_41], dim = 2);  getitem_165 = contiguous_41 = None\n",
      "    mul_91 = getitem_161 * 12\n",
      "    view_163 = mul_90.view(getitem_161, getitem_162, 12, 64);  mul_90 = None\n",
      "    transpose_82 = view_163.transpose(1, 2);  view_163 = None\n",
      "    contiguous_42 = transpose_82.contiguous();  transpose_82 = None\n",
      "    view_164 = contiguous_42.view(mul_91, -1, 64);  contiguous_42 = None\n",
      "    reshape_60 = cat_20.reshape(mul_91, -1, 64)\n",
      "    reshape_61 = cat_21.reshape(mul_91, -1, 64);  mul_91 = None\n",
      "    size_42 = reshape_60.size(1)\n",
      "    transpose_83 = reshape_60.transpose(1, 2);  reshape_60 = None\n",
      "    bmm_40 = torch.bmm(view_164, transpose_83);  view_164 = transpose_83 = None\n",
      "    view_165 = bmm_40.view(getitem_161, 12, getitem_162, size_42);  bmm_40 = None\n",
      "    add_42 = view_165 + _prepare_decoder_attention_mask;  view_165 = None\n",
      "    mul_92 = getitem_161 * 12\n",
      "    view_166 = add_42.view(mul_92, getitem_162, size_42);  add_42 = mul_92 = None\n",
      "    softmax_20 = torch.nn.functional.softmax(view_166, dim = -1, _stacklevel = 3, dtype = None);  view_166 = None\n",
      "    view_167 = getitem_158.view(1, -1, 1, 1);  getitem_158 = None\n",
      "    view_168 = softmax_20.view(getitem_161, 12, getitem_162, size_42);  softmax_20 = None\n",
      "    mul_93 = view_167 * view_168;  view_167 = view_168 = None\n",
      "    mul_94 = getitem_161 * 12\n",
      "    view_169 = mul_93.view(mul_94, getitem_162, size_42);  mul_93 = mul_94 = size_42 = None\n",
      "    dropout_61 = torch.nn.functional.dropout(view_169, p = 0.0, training = False, inplace = False);  view_169 = None\n",
      "    bmm_41 = torch.bmm(dropout_61, reshape_61);  dropout_61 = reshape_61 = None\n",
      "    view_170 = bmm_41.view(getitem_161, 12, getitem_162, 64);  bmm_41 = None\n",
      "    transpose_84 = view_170.transpose(1, 2);  view_170 = None\n",
      "    reshape_62 = transpose_84.reshape(getitem_161, getitem_162, 768);  transpose_84 = getitem_161 = getitem_162 = None\n",
      "    model_decoder_layers_10_self_attn_out_proj = getattr(self.model.decoder.layers, \"10\").self_attn.out_proj(reshape_62);  reshape_62 = None\n",
      "    dropout_62 = torch.nn.functional.dropout(model_decoder_layers_10_self_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_10_self_attn_out_proj = None\n",
      "    add_43 = add_41 + dropout_62;  add_41 = dropout_62 = None\n",
      "    model_decoder_layers_10_encoder_attn_layer_norm = getattr(self.model.decoder.layers, \"10\").encoder_attn_layer_norm(add_43)\n",
      "    getitem_166 = getitem_157[slice(-2, None, None)];  getitem_157 = None\n",
      "    size_43 = model_decoder_layers_10_encoder_attn_layer_norm.size()\n",
      "    getitem_167 = size_43[0]\n",
      "    getitem_168 = size_43[1]\n",
      "    getitem_169 = size_43[2];  size_43 = None\n",
      "    model_decoder_layers_10_encoder_attn_q_proj = getattr(self.model.decoder.layers, \"10\").encoder_attn.q_proj(model_decoder_layers_10_encoder_attn_layer_norm);  model_decoder_layers_10_encoder_attn_layer_norm = None\n",
      "    mul_95 = model_decoder_layers_10_encoder_attn_q_proj * 0.125;  model_decoder_layers_10_encoder_attn_q_proj = None\n",
      "    getitem_170 = getitem_166[0]\n",
      "    getitem_171 = getitem_166[1];  getitem_166 = None\n",
      "    mul_96 = getitem_167 * 12\n",
      "    view_171 = mul_95.view(getitem_167, getitem_168, 12, 64);  mul_95 = None\n",
      "    transpose_85 = view_171.transpose(1, 2);  view_171 = None\n",
      "    contiguous_43 = transpose_85.contiguous();  transpose_85 = None\n",
      "    view_172 = contiguous_43.view(mul_96, -1, 64);  contiguous_43 = None\n",
      "    reshape_63 = getitem_170.reshape(mul_96, -1, 64)\n",
      "    reshape_64 = getitem_171.reshape(mul_96, -1, 64);  mul_96 = None\n",
      "    size_44 = reshape_63.size(1)\n",
      "    transpose_86 = reshape_63.transpose(1, 2);  reshape_63 = None\n",
      "    bmm_42 = torch.bmm(view_172, transpose_86);  view_172 = transpose_86 = None\n",
      "    softmax_21 = torch.nn.functional.softmax(bmm_42, dim = -1, _stacklevel = 3, dtype = None);  bmm_42 = None\n",
      "    view_173 = getitem_159.view(1, -1, 1, 1);  getitem_159 = None\n",
      "    view_174 = softmax_21.view(getitem_167, 12, getitem_168, size_44);  softmax_21 = None\n",
      "    mul_97 = view_173 * view_174;  view_173 = view_174 = None\n",
      "    mul_98 = getitem_167 * 12\n",
      "    view_175 = mul_97.view(mul_98, getitem_168, size_44);  mul_97 = mul_98 = size_44 = None\n",
      "    dropout_63 = torch.nn.functional.dropout(view_175, p = 0.0, training = False, inplace = False);  view_175 = None\n",
      "    bmm_43 = torch.bmm(dropout_63, reshape_64);  dropout_63 = reshape_64 = None\n",
      "    view_176 = bmm_43.view(getitem_167, 12, getitem_168, 64);  bmm_43 = None\n",
      "    transpose_87 = view_176.transpose(1, 2);  view_176 = None\n",
      "    reshape_65 = transpose_87.reshape(getitem_167, getitem_168, 768);  transpose_87 = getitem_167 = getitem_168 = None\n",
      "    model_decoder_layers_10_encoder_attn_out_proj = getattr(self.model.decoder.layers, \"10\").encoder_attn.out_proj(reshape_65);  reshape_65 = None\n",
      "    dropout_64 = torch.nn.functional.dropout(model_decoder_layers_10_encoder_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_10_encoder_attn_out_proj = None\n",
      "    add_44 = add_43 + dropout_64;  add_43 = dropout_64 = None\n",
      "    model_decoder_layers_10_final_layer_norm = getattr(self.model.decoder.layers, \"10\").final_layer_norm(add_44)\n",
      "    model_decoder_layers_10_fc1 = getattr(self.model.decoder.layers, \"10\").fc1(model_decoder_layers_10_final_layer_norm);  model_decoder_layers_10_final_layer_norm = None\n",
      "    gelu_10 = torch._C._nn.gelu(model_decoder_layers_10_fc1);  model_decoder_layers_10_fc1 = None\n",
      "    dropout_65 = torch.nn.functional.dropout(gelu_10, p = 0.0, training = False, inplace = False);  gelu_10 = None\n",
      "    model_decoder_layers_10_fc2 = getattr(self.model.decoder.layers, \"10\").fc2(dropout_65);  dropout_65 = None\n",
      "    dropout_66 = torch.nn.functional.dropout(model_decoder_layers_10_fc2, p = 0.0, training = False, inplace = False);  model_decoder_layers_10_fc2 = None\n",
      "    add_45 = add_44 + dropout_66;  add_44 = dropout_66 = None\n",
      "    getitem_172 = past_key_values[11];  past_key_values = None\n",
      "    getitem_173 = decoder_head_mask[11];  decoder_head_mask = None\n",
      "    getitem_174 = cross_attn_head_mask[11];  cross_attn_head_mask = None\n",
      "    model_decoder_layers_11_self_attn_layer_norm = getattr(self.model.decoder.layers, \"11\").self_attn_layer_norm(add_45)\n",
      "    getitem_175 = getitem_172[slice(None, 2, None)]\n",
      "    size_45 = model_decoder_layers_11_self_attn_layer_norm.size()\n",
      "    getitem_176 = size_45[0]\n",
      "    getitem_177 = size_45[1]\n",
      "    getitem_178 = size_45[2];  size_45 = None\n",
      "    model_decoder_layers_11_self_attn_q_proj = getattr(self.model.decoder.layers, \"11\").self_attn.q_proj(model_decoder_layers_11_self_attn_layer_norm)\n",
      "    mul_99 = model_decoder_layers_11_self_attn_q_proj * 0.125;  model_decoder_layers_11_self_attn_q_proj = None\n",
      "    model_decoder_layers_11_self_attn_k_proj = getattr(self.model.decoder.layers, \"11\").self_attn.k_proj(model_decoder_layers_11_self_attn_layer_norm)\n",
      "    view_177 = model_decoder_layers_11_self_attn_k_proj.view(getitem_176, -1, 12, 64);  model_decoder_layers_11_self_attn_k_proj = None\n",
      "    transpose_88 = view_177.transpose(1, 2);  view_177 = None\n",
      "    contiguous_44 = transpose_88.contiguous();  transpose_88 = None\n",
      "    model_decoder_layers_11_self_attn_v_proj = getattr(self.model.decoder.layers, \"11\").self_attn.v_proj(model_decoder_layers_11_self_attn_layer_norm);  model_decoder_layers_11_self_attn_layer_norm = None\n",
      "    view_178 = model_decoder_layers_11_self_attn_v_proj.view(getitem_176, -1, 12, 64);  model_decoder_layers_11_self_attn_v_proj = None\n",
      "    transpose_89 = view_178.transpose(1, 2);  view_178 = None\n",
      "    contiguous_45 = transpose_89.contiguous();  transpose_89 = None\n",
      "    getitem_179 = getitem_175[0]\n",
      "    cat_22 = torch.cat([getitem_179, contiguous_44], dim = 2);  getitem_179 = contiguous_44 = None\n",
      "    getitem_180 = getitem_175[1];  getitem_175 = None\n",
      "    cat_23 = torch.cat([getitem_180, contiguous_45], dim = 2);  getitem_180 = contiguous_45 = None\n",
      "    mul_100 = getitem_176 * 12\n",
      "    view_179 = mul_99.view(getitem_176, getitem_177, 12, 64);  mul_99 = None\n",
      "    transpose_90 = view_179.transpose(1, 2);  view_179 = None\n",
      "    contiguous_46 = transpose_90.contiguous();  transpose_90 = None\n",
      "    view_180 = contiguous_46.view(mul_100, -1, 64);  contiguous_46 = None\n",
      "    reshape_66 = cat_22.reshape(mul_100, -1, 64)\n",
      "    reshape_67 = cat_23.reshape(mul_100, -1, 64);  mul_100 = None\n",
      "    size_46 = reshape_66.size(1)\n",
      "    transpose_91 = reshape_66.transpose(1, 2);  reshape_66 = None\n",
      "    bmm_44 = torch.bmm(view_180, transpose_91);  view_180 = transpose_91 = None\n",
      "    view_181 = bmm_44.view(getitem_176, 12, getitem_177, size_46);  bmm_44 = None\n",
      "    add_46 = view_181 + _prepare_decoder_attention_mask;  view_181 = _prepare_decoder_attention_mask = None\n",
      "    mul_101 = getitem_176 * 12\n",
      "    view_182 = add_46.view(mul_101, getitem_177, size_46);  add_46 = mul_101 = None\n",
      "    softmax_22 = torch.nn.functional.softmax(view_182, dim = -1, _stacklevel = 3, dtype = None);  view_182 = None\n",
      "    view_183 = getitem_173.view(1, -1, 1, 1);  getitem_173 = None\n",
      "    view_184 = softmax_22.view(getitem_176, 12, getitem_177, size_46);  softmax_22 = None\n",
      "    mul_102 = view_183 * view_184;  view_183 = view_184 = None\n",
      "    mul_103 = getitem_176 * 12\n",
      "    view_185 = mul_102.view(mul_103, getitem_177, size_46);  mul_102 = mul_103 = size_46 = None\n",
      "    dropout_67 = torch.nn.functional.dropout(view_185, p = 0.0, training = False, inplace = False);  view_185 = None\n",
      "    bmm_45 = torch.bmm(dropout_67, reshape_67);  dropout_67 = reshape_67 = None\n",
      "    view_186 = bmm_45.view(getitem_176, 12, getitem_177, 64);  bmm_45 = None\n",
      "    transpose_92 = view_186.transpose(1, 2);  view_186 = None\n",
      "    reshape_68 = transpose_92.reshape(getitem_176, getitem_177, 768);  transpose_92 = getitem_176 = getitem_177 = None\n",
      "    model_decoder_layers_11_self_attn_out_proj = getattr(self.model.decoder.layers, \"11\").self_attn.out_proj(reshape_68);  reshape_68 = None\n",
      "    dropout_68 = torch.nn.functional.dropout(model_decoder_layers_11_self_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_11_self_attn_out_proj = None\n",
      "    add_47 = add_45 + dropout_68;  add_45 = dropout_68 = None\n",
      "    model_decoder_layers_11_encoder_attn_layer_norm = getattr(self.model.decoder.layers, \"11\").encoder_attn_layer_norm(add_47)\n",
      "    getitem_181 = getitem_172[slice(-2, None, None)];  getitem_172 = None\n",
      "    size_47 = model_decoder_layers_11_encoder_attn_layer_norm.size()\n",
      "    getitem_182 = size_47[0]\n",
      "    getitem_183 = size_47[1]\n",
      "    getitem_184 = size_47[2];  size_47 = None\n",
      "    model_decoder_layers_11_encoder_attn_q_proj = getattr(self.model.decoder.layers, \"11\").encoder_attn.q_proj(model_decoder_layers_11_encoder_attn_layer_norm);  model_decoder_layers_11_encoder_attn_layer_norm = None\n",
      "    mul_104 = model_decoder_layers_11_encoder_attn_q_proj * 0.125;  model_decoder_layers_11_encoder_attn_q_proj = None\n",
      "    getitem_185 = getitem_181[0]\n",
      "    getitem_186 = getitem_181[1];  getitem_181 = None\n",
      "    mul_105 = getitem_182 * 12\n",
      "    view_187 = mul_104.view(getitem_182, getitem_183, 12, 64);  mul_104 = None\n",
      "    transpose_93 = view_187.transpose(1, 2);  view_187 = None\n",
      "    contiguous_47 = transpose_93.contiguous();  transpose_93 = None\n",
      "    view_188 = contiguous_47.view(mul_105, -1, 64);  contiguous_47 = None\n",
      "    reshape_69 = getitem_185.reshape(mul_105, -1, 64)\n",
      "    reshape_70 = getitem_186.reshape(mul_105, -1, 64);  mul_105 = None\n",
      "    size_48 = reshape_69.size(1)\n",
      "    transpose_94 = reshape_69.transpose(1, 2);  reshape_69 = None\n",
      "    bmm_46 = torch.bmm(view_188, transpose_94);  view_188 = transpose_94 = None\n",
      "    softmax_23 = torch.nn.functional.softmax(bmm_46, dim = -1, _stacklevel = 3, dtype = None);  bmm_46 = None\n",
      "    view_189 = getitem_174.view(1, -1, 1, 1);  getitem_174 = None\n",
      "    view_190 = softmax_23.view(getitem_182, 12, getitem_183, size_48);  softmax_23 = None\n",
      "    mul_106 = view_189 * view_190;  view_189 = view_190 = None\n",
      "    mul_107 = getitem_182 * 12\n",
      "    view_191 = mul_106.view(mul_107, getitem_183, size_48);  mul_106 = mul_107 = size_48 = None\n",
      "    dropout_69 = torch.nn.functional.dropout(view_191, p = 0.0, training = False, inplace = False);  view_191 = None\n",
      "    bmm_47 = torch.bmm(dropout_69, reshape_70);  dropout_69 = reshape_70 = None\n",
      "    view_192 = bmm_47.view(getitem_182, 12, getitem_183, 64);  bmm_47 = None\n",
      "    transpose_95 = view_192.transpose(1, 2);  view_192 = None\n",
      "    reshape_71 = transpose_95.reshape(getitem_182, getitem_183, 768);  transpose_95 = getitem_182 = getitem_183 = None\n",
      "    model_decoder_layers_11_encoder_attn_out_proj = getattr(self.model.decoder.layers, \"11\").encoder_attn.out_proj(reshape_71);  reshape_71 = None\n",
      "    dropout_70 = torch.nn.functional.dropout(model_decoder_layers_11_encoder_attn_out_proj, p = 0.0, training = False, inplace = False);  model_decoder_layers_11_encoder_attn_out_proj = None\n",
      "    add_48 = add_47 + dropout_70;  add_47 = dropout_70 = None\n",
      "    model_decoder_layers_11_final_layer_norm = getattr(self.model.decoder.layers, \"11\").final_layer_norm(add_48)\n",
      "    model_decoder_layers_11_fc1 = getattr(self.model.decoder.layers, \"11\").fc1(model_decoder_layers_11_final_layer_norm);  model_decoder_layers_11_final_layer_norm = None\n",
      "    gelu_11 = torch._C._nn.gelu(model_decoder_layers_11_fc1);  model_decoder_layers_11_fc1 = None\n",
      "    dropout_71 = torch.nn.functional.dropout(gelu_11, p = 0.0, training = False, inplace = False);  gelu_11 = None\n",
      "    model_decoder_layers_11_fc2 = getattr(self.model.decoder.layers, \"11\").fc2(dropout_71);  dropout_71 = None\n",
      "    dropout_72 = torch.nn.functional.dropout(model_decoder_layers_11_fc2, p = 0.0, training = False, inplace = False);  model_decoder_layers_11_fc2 = None\n",
      "    add_49 = add_48 + dropout_72;  add_48 = dropout_72 = None\n",
      "    model_decoder_layer_norm = self.model.decoder.layer_norm(add_49);  add_49 = None\n",
      "    proj_out = self.proj_out(model_decoder_layer_norm);  model_decoder_layer_norm = None\n",
      "    getattr_3 = proj_out.device\n",
      "    to = labels.to(getattr_3);  labels = getattr_3 = None\n",
      "    view_193 = proj_out.view(-1, 51865)\n",
      "    reshape_72 = to.reshape(-1);  to = None\n",
      "    loss_fct = self.loss_fct(view_193, reshape_72);  view_193 = reshape_72 = None\n",
      "    getattr_4 = encoder_outputs.last_hidden_state\n",
      "    getattr_5 = encoder_outputs.hidden_states\n",
      "    getattr_6 = encoder_outputs.attentions;  encoder_outputs = None\n",
      "    return {'loss': loss_fct, 'logits': proj_out, 'past_key_values': ((cat, cat_1, getitem_20, getitem_21), (cat_2, cat_3, getitem_35, getitem_36), (cat_4, cat_5, getitem_50, getitem_51), (cat_6, cat_7, getitem_65, getitem_66), (cat_8, cat_9, getitem_80, getitem_81), (cat_10, cat_11, getitem_95, getitem_96), (cat_12, cat_13, getitem_110, getitem_111), (cat_14, cat_15, getitem_125, getitem_126), (cat_16, cat_17, getitem_140, getitem_141), (cat_18, cat_19, getitem_155, getitem_156), (cat_20, cat_21, getitem_170, getitem_171), (cat_22, cat_23, getitem_185, getitem_186)), 'encoder_last_hidden_state': getattr_4, 'encoder_hidden_states': getattr_5, 'encoder_attentions': getattr_6}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(symbolic_traced.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec0b6e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import get_default_qconfig\n",
    "from quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import QConfigMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "229c0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qconfig = get_default_qconfig(\"x86\")\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d020617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_inputs = (input_features, attention_mask, decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3c310d3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ujan/opt/anaconda3/envs/asr/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (activation_post_process_0): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_1): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (model): Module(\n",
       "    (decoder): Module(\n",
       "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
       "      (embed_positions): Module()\n",
       "      (layers): Module(\n",
       "        (0): Module(\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (1): Module(\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (2): Module(\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (3): Module(\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (4): Module(\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (5): Module(\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (6): Module(\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (7): Module(\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (8): Module(\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (9): Module(\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (10): Module(\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (11): Module(\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (self_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): Module(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_2): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_30): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_3): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_4): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_5): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_33): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_59): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_6): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_7): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_8): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_9): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_10): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_11): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_12): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_13): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_14): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_15): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_16): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_17): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_18): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_19): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_20): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_21): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_22): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_23): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_24): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_25): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_26): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_27): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_28): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_29): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_31): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_32): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_35): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_34): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_36): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_37): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_38): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_39): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_40): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_41): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_42): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_43): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_44): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_45): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_46): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_47): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_48): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_49): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_54): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_56): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_50): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_51): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_52): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_53): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_55): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_57): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_58): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_61): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_60): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_62): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_63): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_64): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_65): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_66): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_67): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_68): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_69): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_70): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_71): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_72): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_73): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_74): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_75): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_76): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_77): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_78): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_79): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_106): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_132): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_80): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_81): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_82): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_83): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_84): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_85): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_86): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_87): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_88): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_89): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_90): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_91): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_92): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_93): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_94): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_95): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_96): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_97): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_98): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_99): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_100): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_101): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_102): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_103): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_104): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_105): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_108): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_107): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_109): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_110): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_111): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_112): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_113): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_114): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_115): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_116): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_117): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_118): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_119): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_120): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_121): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_122): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_127): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_129): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_123): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_124): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_125): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_126): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_128): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_130): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_131): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_134): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_133): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_135): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_136): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_137): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_138): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_139): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_140): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_141): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_142): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_143): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_144): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_145): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_146): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_147): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_148): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_149): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_150): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_151): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_152): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_179): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_205): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_153): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_154): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_155): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_156): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_157): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_158): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_159): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_160): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_161): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_162): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_163): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_164): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_165): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_166): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_167): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_168): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_169): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_170): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_171): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_172): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_173): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_174): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_175): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_176): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_177): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_178): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_181): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_180): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_182): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_183): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_184): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_185): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_186): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_187): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_188): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_189): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_190): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_191): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_192): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_193): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_194): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_195): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_200): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_202): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_196): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_197): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_198): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_199): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_201): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_203): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_204): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_207): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_206): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_208): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_209): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_210): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_211): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_212): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_213): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_214): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_215): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_216): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_217): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_218): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_219): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_220): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_221): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_222): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_223): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_224): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_225): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_252): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_278): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_226): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_227): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_228): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_229): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_230): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_231): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_232): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_233): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_234): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_235): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_236): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_237): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_238): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_239): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_240): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_241): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_242): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_243): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_244): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_245): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_246): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_247): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_248): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_249): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_250): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_251): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_254): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_253): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_255): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_256): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_257): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_258): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_259): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_260): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_261): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_262): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_263): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_264): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_265): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_266): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_267): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_268): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_273): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_275): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_269): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_270): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_271): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_272): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_274): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_276): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_277): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_280): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_279): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_281): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_282): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_283): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_284): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_285): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_286): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_287): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_288): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_289): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_290): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_291): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_292): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_293): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_294): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_295): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_296): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_297): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_298): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_325): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_351): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_299): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_300): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_301): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_302): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_303): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_304): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_305): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_306): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_307): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_308): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_309): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_310): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_311): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_312): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_313): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_314): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_315): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_316): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_317): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_318): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_319): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_320): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_321): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_322): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_323): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_324): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_327): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_326): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_328): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_329): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_330): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_331): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_332): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_333): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_334): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_335): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_336): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_337): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_338): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_339): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_340): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_341): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_346): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_348): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_342): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_343): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_344): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_345): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_347): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_349): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_350): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_353): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_352): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_354): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_355): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_356): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_357): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_358): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_359): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_360): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_361): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_362): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_363): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_364): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_365): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_366): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_367): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_368): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_369): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_370): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_371): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_398): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_424): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_372): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_373): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_374): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_375): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_376): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_377): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_378): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_379): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_380): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_381): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_382): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_383): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_384): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_385): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_386): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_387): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_388): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_389): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_390): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_391): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_392): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_393): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_394): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_395): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_396): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_397): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_400): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_399): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_401): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_402): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_403): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_404): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_405): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_406): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_407): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_408): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_409): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_410): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_411): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_412): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_413): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_414): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_419): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_421): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_415): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_416): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_417): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_418): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_420): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_422): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_423): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_426): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_425): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_427): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_428): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_429): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_430): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_431): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_432): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_433): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_434): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_435): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_436): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_437): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_438): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_439): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_440): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_441): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_442): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_443): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_444): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_471): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_497): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_445): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_446): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_447): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_448): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_449): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_450): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_451): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_452): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_453): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_454): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_455): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_456): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_457): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_458): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_459): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_460): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_461): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_462): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_463): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_464): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_465): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_466): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_467): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_468): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_469): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_470): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_473): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_472): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_474): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_475): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_476): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_477): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_478): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_479): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_480): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_481): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_482): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_483): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_484): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_485): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_486): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_487): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_492): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_494): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_488): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_489): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_490): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_491): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_493): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_495): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_496): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_499): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_498): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_500): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_501): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_502): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_503): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_504): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_505): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_506): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_507): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_508): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_509): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_510): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_511): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_512): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_513): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_514): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_515): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_516): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_517): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_544): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_570): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_518): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_519): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_520): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_521): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_522): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_523): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_524): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_525): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_526): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_527): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_528): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_529): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_530): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_531): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_532): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_533): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_534): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_535): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_536): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_537): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_538): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_539): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_540): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_541): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_542): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_543): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_546): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_545): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_547): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_548): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_549): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_550): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_551): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_552): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_553): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_554): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_555): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_556): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_557): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_558): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_559): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_560): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_565): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_567): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_561): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_562): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_563): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_564): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_566): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_568): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_569): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_572): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_571): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_573): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_574): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_575): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_576): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_577): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_578): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_579): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_580): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_581): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_582): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_583): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_584): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_585): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_586): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_587): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_588): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_589): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_590): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_617): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_643): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_591): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_592): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_593): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_594): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_595): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_596): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_597): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_598): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_599): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_600): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_601): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_602): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_603): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_604): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_605): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_606): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_607): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_608): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_609): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_610): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_611): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_612): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_613): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_614): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_615): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_616): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_619): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_618): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_620): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_621): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_622): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_623): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_624): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_625): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_626): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_627): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_628): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_629): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_630): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_631): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_632): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_633): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_638): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_640): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_634): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_635): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_636): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_637): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_639): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_641): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_642): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_645): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_644): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_646): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_647): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_648): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_649): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_650): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_651): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_652): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_653): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_654): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_655): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_656): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_657): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_658): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_659): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_660): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_661): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_662): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_663): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_690): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_716): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_664): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_665): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_666): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_667): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_668): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_669): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_670): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_671): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_672): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_673): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_674): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_675): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_676): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_677): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_678): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_679): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_680): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_681): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_682): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_683): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_684): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_685): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_686): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_687): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_688): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_689): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_692): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_691): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_693): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_694): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_695): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_696): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_697): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_698): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_699): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_700): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_701): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_702): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_703): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_704): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_705): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_706): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_711): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_713): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_707): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_708): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_709): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_710): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_712): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_714): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_715): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_718): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_717): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_719): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_720): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_721): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_722): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_723): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_724): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_725): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_726): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_727): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_728): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_729): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_730): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_731): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_732): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_733): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_734): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_735): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_736): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_763): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_789): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_737): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_738): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_739): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_740): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_741): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_742): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_743): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_744): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_745): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_746): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_747): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_748): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_749): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_750): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_751): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_752): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_753): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_754): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_755): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_756): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_757): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_758): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_759): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_760): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_761): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_762): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_765): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_764): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_766): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_767): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_768): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_769): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_770): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_771): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_772): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_773): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_774): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_775): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_776): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_777): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_778): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_779): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_784): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_786): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_780): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_781): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_782): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_783): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_785): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_787): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_788): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_791): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_790): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_792): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_793): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_794): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_795): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_796): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_797): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_798): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_799): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_800): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_801): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_802): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_803): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_804): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_805): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_806): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_807): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_808): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_809): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_836): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_862): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_810): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_811): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_812): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_813): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_814): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_815): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_816): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_817): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_818): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_819): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_820): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_821): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_822): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_823): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_824): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_825): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_826): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_827): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_828): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_829): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_830): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_831): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_832): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_833): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_834): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_835): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_838): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_837): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_839): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_840): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_841): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_842): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_843): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_844): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_845): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_846): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_847): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_848): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_849): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_850): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_851): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_852): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_857): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_859): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_853): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_854): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_855): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_856): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_858): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_860): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_861): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_864): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_863): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_865): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_866): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_867): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_868): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_869): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_870): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_871): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_872): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_873): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_874): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_875): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_876): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_877): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_878): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_879): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_880): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_881): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_882): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_883): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
       "  (activation_post_process_884): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_886): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_885): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (activation_post_process_887): HistogramObserver(min_val=inf, max_val=-inf)\n",
       "  (loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_fx(model, qconfig_mapping, example_inputs, concrete_args=decoder_concrete_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3c91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31443de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3670, 0.9916, 0.8972, 0.7970, 0.3965, 0.5842, 0.2741, 0.5255],\n",
       "        [0.6296, 0.4956, 0.1430, 0.8266, 0.5940, 0.7007, 0.2518, 0.1667],\n",
       "        [0.7255, 0.0080, 0.9489, 0.1894, 0.5512, 0.5293, 0.2753, 0.0508],\n",
       "        [0.0584, 0.3933, 0.1139, 0.7904, 0.3747, 0.3514, 0.7094, 0.1210],\n",
       "        [0.2017, 0.0492, 0.1117, 0.2658, 0.8969, 0.0542, 0.2911, 0.9022],\n",
       "        [0.3773, 0.9827, 0.1489, 0.0572, 0.9670, 0.2209, 0.4054, 0.3755],\n",
       "        [0.8901, 0.5765, 0.0042, 0.6808, 0.5876, 0.0787, 0.9588, 0.2276],\n",
       "        [0.7936, 0.0282, 0.0572, 0.2471, 0.8485, 0.9265, 0.1902, 0.8694]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.rand(8,8)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdb5ae0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(8**2) / (2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9179b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 8\n",
    "cols = 8\n",
    "block_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "700f58ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for b in range(rows//block_size):\n",
    "    print(b*block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a993ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ids = [b*block_size for b in range(rows//block_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68426c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ids = [b*block_size for b in range(cols//block_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce90261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a94fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_starts = list(itertools.product(row_ids, col_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c73c8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79cf59b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5500, 0.6808],\n",
      "        [0.8853, 0.0787]])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for starts in block_starts:\n",
    "    block = mask[starts[0]:starts[0]+block_size, starts[1]:starts[1]+block_size]\n",
    "    print(block)\n",
    "    print(block.numel())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1fc1a444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d07f0f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5500, 0.6808, 0.0000, 0.0000, 0.3190, 0.4696, 0.1576, 0.5488],\n",
       "        [0.8853, 0.0787, 0.0000, 0.0000, 0.0588, 0.3720, 0.8529, 0.5314],\n",
       "        [0.5348, 0.5938, 0.3496, 0.1799, 0.9819, 0.6040, 0.5582, 0.3997],\n",
       "        [0.4339, 0.7161, 0.8938, 0.2811, 0.3184, 0.5575, 0.6046, 0.9316],\n",
       "        [0.0519, 0.7054, 0.1441, 0.0785, 0.2620, 0.7403, 0.8035, 0.4934],\n",
       "        [0.6501, 0.0503, 0.0231, 0.4434, 0.7548, 0.2351, 0.0118, 0.4432],\n",
       "        [0.6796, 0.2638, 0.6830, 0.9232, 0.9234, 0.0189, 0.7614, 0.2667],\n",
       "        [0.7023, 0.5816, 0.4335, 0.0157, 0.0692, 0.3074, 0.8151, 0.9757]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "545a9f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0046, 0.6940, 0.1306, 0.8281],\n",
       "        [0.0148, 0.0548, 0.3951, 0.5808],\n",
       "        [0.3264, 0.6692, 0.2732, 0.4666],\n",
       "        [0.0983, 0.0590, 0.1664, 0.3158]], requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((4,4), requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "58817836",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.var(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51d3f25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0705, grad_fn=<VarBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b22dec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0417,  0.0502, -0.0249,  0.0681],\n",
       "        [-0.0403, -0.0350,  0.0104,  0.0351],\n",
       "        [ 0.0012,  0.0469, -0.0059,  0.0199],\n",
       "        [-0.0292, -0.0344, -0.0201, -0.0002]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward(retain_graph=True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "625b2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 2\n",
    "out_features = 8\n",
    "in_features = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49787659",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ids = [b*block_size for b in range(out_features//block_size)]\n",
    "# column indices where a block starts\n",
    "col_ids = [b*block_size for b in range(in_features//block_size)]\n",
    "    # cartesian product\n",
    "# each element is a tuple containing the top left position of each block\n",
    "block_starts = list(itertools.product(row_ids, col_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0137cf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (0, 4),\n",
       " (0, 6),\n",
       " (2, 0),\n",
       " (2, 2),\n",
       " (2, 4),\n",
       " (2, 6),\n",
       " (4, 0),\n",
       " (4, 2),\n",
       " (4, 4),\n",
       " (4, 6),\n",
       " (6, 0),\n",
       " (6, 2),\n",
       " (6, 4),\n",
       " (6, 6)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83b024a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for starts in block_starts:\n",
    "    # get a block\n",
    "    block = mask[starts[0]:starts[0]+block_size, starts[1]:starts[1]+block_size]\n",
    "    mask[starts[0]:starts[0]+block_size, starts[1]:starts[1]+block_size] = 0\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0fe312e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.8972, 0.7970, 0.3965, 0.5842, 0.2741, 0.5255],\n",
       "        [0.0000, 0.0000, 0.1430, 0.8266, 0.5940, 0.7007, 0.2518, 0.1667],\n",
       "        [0.7255, 0.0080, 0.9489, 0.1894, 0.5512, 0.5293, 0.2753, 0.0508],\n",
       "        [0.0584, 0.3933, 0.1139, 0.7904, 0.3747, 0.3514, 0.7094, 0.1210],\n",
       "        [0.2017, 0.0492, 0.1117, 0.2658, 0.8969, 0.0542, 0.2911, 0.9022],\n",
       "        [0.3773, 0.9827, 0.1489, 0.0572, 0.9670, 0.2209, 0.4054, 0.3755],\n",
       "        [0.8901, 0.5765, 0.0042, 0.6808, 0.5876, 0.0787, 0.9588, 0.2276],\n",
       "        [0.7936, 0.0282, 0.0572, 0.2471, 0.8485, 0.9265, 0.1902, 0.8694]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c11cecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e7c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained('/home/ujan/speech-processing/models/whisper/whisper-small-common_voice_13_0-sigmoied_threshold-pruned/pasted_checkpoint-5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40aeebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_old = WhisperForConditionalGeneration.from_pretrained('openai/whisper-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab1f4801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0000,  0.0000,  0.0000,  ..., -0.0388, -0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0231, -0.0000],\n",
       "        [-0.0249, -0.0000,  0.0000,  ..., -0.0173, -0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000, -0.0000,  ..., -0.0000,  0.0000, -0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -0.0000],\n",
       "        [ 0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.encoder.layers[7].self_attn.k_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95adb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoFeatureExtractor, AutoTokenizer, AutoProcessor, AutoModelForSpeechSeq2Seq\n",
    "from modeling_whisper_traceable import WhisperForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "model_name_or_path = 'openai/whisper-small'\n",
    "data_dir = 'mozilla-foundation/common_voice_11_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d7965501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset from mozilla-foundation/common_voice_11_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 10581it [00:00, 23742.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "print('loading dataset from {}'.format(data_dir))\n",
    "\n",
    "raw_datasets = load_dataset(data_dir, \"zh-CN\", split=\"test\", streaming=True)\n",
    "text_column_name = 'sentence'\n",
    "\n",
    "\n",
    "# model, tokenizer, feature extractor, processor\n",
    "\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    #cache_dir=args.cache_dir,\n",
    "    #revision=args.model_revision,\n",
    "    #use_auth_token=True if args.use_auth_token else None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    #cache_dir=args.cache_dir,\n",
    "    #use_fast=model_args.use_fast_tokenizer,\n",
    "    #revision=model_args.model_revision,\n",
    "    #use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "tokenizer.set_prefix_tokens(language='chinese', task='transcribe')\n",
    "\n",
    "\n",
    "    \n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path)\n",
    "\n",
    "\n",
    "dataset = raw_datasets\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "sample = next(iter(dataset))\n",
    "\n",
    "\n",
    "inputs = processor(\n",
    "    sample['audio'][\"array\"],\n",
    "    sampling_rate=feature_extractor.sampling_rate,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\")\n",
    "                \n",
    "input_features = inputs.input_features\n",
    "attention_mask = inputs.attention_mask\n",
    "decoder_input_ids = torch.tensor([model.config.decoder_start_token_id]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8f5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit output_ids = model.generate(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92e58da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546 ms  85.6 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit output_ids = model_old.generate(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "550bdcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 10581it [00:00, 11039.05it/s]\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(dataset))\n",
    "\n",
    "\n",
    "inputs = processor(\n",
    "    sample['audio'][\"array\"],\n",
    "    sampling_rate=feature_extractor.sampling_rate,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\")\n",
    "                \n",
    "input_features = inputs.input_features\n",
    "attention_mask = inputs.attention_mask\n",
    "decoder_input_ids = torch.tensor([model.config.decoder_start_token_id]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40e7502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824 ms  8.85 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit output_ids = model.generate(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11d46b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791 ms  27.5 ms per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit output_ids = model_old.generate(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08956e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "from transformers import WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673a1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained('/home/ujan/speech-processing/models/whisper/whisper-small-common_voice_13_0-sigmoied_threshold-pruned/pasted_checkpoint-5000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb95f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.encoder.layers[1].self_attn.k_proj.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20168b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "def prune_linear_layer(layer: nn.Linear, index: torch.LongTensor, dim: int = 0) -> nn.Linear:\n",
    "    \"\"\"\n",
    "    Prune a linear layer to keep only entries in index.\n",
    "\n",
    "    Used to remove heads.\n",
    "\n",
    "    Args:\n",
    "        layer (`torch.nn.Linear`): The layer to prune.\n",
    "        index (`torch.LongTensor`): The indices to keep in the layer.\n",
    "        dim (`int`, *optional*, defaults to 0): The dimension on which to keep the indices.\n",
    "\n",
    "    Returns:\n",
    "        `torch.nn.Linear`: The pruned layer as a new layer with `requires_grad=True`.\n",
    "    \"\"\"\n",
    "    index = index.to(layer.weight.device)\n",
    "    W = layer.weight.index_select(dim, index).clone().detach()\n",
    "    if layer.bias is not None:\n",
    "        if dim == 1:\n",
    "            b = layer.bias.clone().detach()\n",
    "        else:\n",
    "            b = layer.bias[index].clone().detach()\n",
    "    new_size = list(layer.weight.size())\n",
    "    new_size[dim] = len(index)\n",
    "    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n",
    "    new_layer.weight.requires_grad = False\n",
    "    new_layer.weight.copy_(W.contiguous())\n",
    "    new_layer.weight.requires_grad = True\n",
    "    if layer.bias is not None:\n",
    "        new_layer.bias.requires_grad = False\n",
    "        new_layer.bias.copy_(b.contiguous())\n",
    "        new_layer.bias.requires_grad = True\n",
    "    return new_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(8, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85971635",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = torch.LongTensor([1,5,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd81806",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_linear = prune_linear_layer(m, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de47762",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491ef2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ef633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163d56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 12\n",
    "heads = [1,3,7]\n",
    "head_size = 64\n",
    "already_pruned_heads = set()\n",
    "\n",
    "mask = torch.ones(n_heads, head_size)\n",
    "heads = set(heads) - already_pruned_heads  # Convert to set and remove already pruned heads\n",
    "for head in heads:\n",
    "    # Compute how many pruned heads are before the head and move the index accordingly\n",
    "    head = head - sum(1 if h < head else 0 for h in already_pruned_heads)\n",
    "    mask[head] = 0\n",
    "mask = mask.view(-1).contiguous().eq(1)\n",
    "index: torch.LongTensor = torch.arange(len(mask))[mask].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ab5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e169eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Jul  5 2023, 15:34:07) [Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "70ee1ab45e8b937f09fa6841b13701829ac76ec7fe39d355982d785d83845075"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
