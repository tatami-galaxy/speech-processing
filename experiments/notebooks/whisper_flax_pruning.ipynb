{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "| Size   | Layers | Width | Heads | Parameters | English-only                                         | Multilingual                                      |\n",
    "|--------|--------|-------|-------|------------|------------------------------------------------------|---------------------------------------------------|\n",
    "| tiny   | 4      | 384   | 6     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny.)  |\n",
    "| base   | 6      | 512   | 8     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)   |\n",
    "| small  | 12     | 768   | 12    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)  |\n",
    "| medium | 24     | 1024  | 16    | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium) |\n",
    "| large  | 32     | 1280  | 20    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)  |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from os.path import dirname, abspath\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm.auto import tqdm\n",
    "import time, math\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import jax_utils, traverse_util\n",
    "from flax.jax_utils import unreplicate, pad_shard_unpad\n",
    "import orbax\n",
    "from flax.training import (\n",
    "    train_state,\n",
    "    orbax_utils\n",
    ")\n",
    "from flax.training.common_utils import (\n",
    "    onehot,\n",
    "    shard,\n",
    "    get_metrics\n",
    ")\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    GenerationConfig,\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    FlaxWhisperForConditionalGeneration,\n",
    ")\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    is_tensorboard_available,\n",
    ")\n",
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "import datasets\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    load_dataset,\n",
    "    DatasetDict,\n",
    "    Audio\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from multiprocess import set_start_method\n",
    "set_start_method(\"spawn\")\n",
    "\n",
    "\n",
    "#jax.config.update('jax_array', False) -> only works below jax and jaxlib 0.4.6\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# setup logging, we only want one process per machine to log things on the screen.\n",
    "logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n",
    "if jax.process_index() == 0:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "\n",
    "# sending telemetry\n",
    "# tracking the example usage helps us better allocate resources to maintain them\n",
    "# the information sent is the one passed as arguments along with your Python/PyTorch versions\n",
    "send_example_telemetry(\"run_summarization\", framework=\"flax\")\n",
    "\n",
    "\n",
    "# get root directory\n",
    "root = abspath(__file__)\n",
    "while root.split('/')[-1] != 'speech-processing':\n",
    "    root = dirname(root)\n",
    "\n",
    "# constants\n",
    "LANG_TO_ID = {\"hindi\" : \"<|hi|>\",\n",
    "              \"chinese\" : \"<|zh|>\"}\n",
    "\n",
    "\n",
    "seed = 42\n",
    "# set seed\n",
    "set_seed(seed)\n",
    "\n",
    "# model\n",
    "model_name_or_path = 'openai/whisper-tiny'\n",
    "model_lang = 'hindi'\n",
    "task = 'transcribe'\n",
    "dtype = 'float32'  # float16\n",
    "sampling_rate = 16000\n",
    "generation_max_length = 225\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "eval_batch_size = int(per_device_eval_batch_size) * jax.device_count()\n",
    "num_beams = 1\n",
    "label_smoothing_factor = 0.0\n",
    "learning_rate = 1e-5\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "adam_epsilon = 1e-6\n",
    "weight_decay = 0.0\n",
    "train_steps = 2000\n",
    "warmup_steps = 0\n",
    "max_to_keep = 3\n",
    "\n",
    "# flags\n",
    "freeze_encoder = False\n",
    "\n",
    "# data\n",
    "data_dir = 'mozilla-foundation/common_voice_11_0'\n",
    "data_lang = 'hi'\n",
    "max_train_samples = 100  # None\n",
    "max_test_samples = 20  # None\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "# output / checkpoint directory\n",
    "model_str = model_name_or_path.split('/')[-1]\n",
    "data_str = data_dir.split('/')[-1]\n",
    "output_dir = root+'/models/whisper/'+model_str+'_jax_'+data_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate multi gpu\n",
    "\n",
    "#import os\n",
    "#flags = os.environ.get('XLA_FLAGS', '')\n",
    "#os.environ['XLA_FLAGS'] = flags + \" --xla_force_host_platform_device_count=4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#class TrainState(train_state.TrainState):\n",
    "    #dropout_rng: jnp.ndarray\n",
    "\n",
    "    #def replicate(self):\n",
    "        #return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng.reshape(-1)))\n",
    "    \n",
    "\n",
    "def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n",
    "    \"\"\"\n",
    "    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n",
    "    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        batch_idx = jax.random.permutation(rng, len(dataset))\n",
    "        batch_idx = np.asarray(batch_idx)\n",
    "    else:\n",
    "        batch_idx = np.arange(len(dataset))\n",
    "\n",
    "    if drop_last:\n",
    "        steps_per_epoch = len(dataset) // batch_size\n",
    "        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n",
    "    else:\n",
    "        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n",
    "        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n",
    "\n",
    "    for idx in batch_idx:\n",
    "        batch = dataset[idx]\n",
    "        batch = {k: np.array(v) for k, v in batch.items()}\n",
    "\n",
    "        yield batch\n",
    "    \n",
    "\n",
    "\n",
    "# in Flax, for seq2seq models we need to pass `decoder_input_ids`\n",
    "# as the Flax models don't accept `labels`, we need to prepare the decoder_input_ids here\n",
    "# `shift_tokens_right` function\n",
    "# copied from transformers.models.bart.modeling_flax_bart.shift_tokens_right\n",
    "def shift_tokens_right(input_ids: np.array, pad_token_id: int, decoder_start_token_id: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "\n",
    "    shifted_input_ids = np.zeros_like(input_ids)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1]\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "    shifted_input_ids = np.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "\n",
    "# label smoothed cross entropy\n",
    "def loss_fn(logits, labels, padding_mask, label_smoothing_factor=0.0):\n",
    "    \"\"\"\n",
    "    The label smoothing implementation is adapted from Flax's official example:\n",
    "    https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104\n",
    "    \"\"\"\n",
    "    vocab_size = logits.shape[-1]\n",
    "    confidence = 1.0 - label_smoothing_factor\n",
    "    low_confidence = (1.0 - confidence) / (vocab_size - 1)\n",
    "    normalizing_constant = -(\n",
    "        confidence * jnp.log(confidence) + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)\n",
    "    )\n",
    "    soft_labels = onehot(labels, vocab_size, on_value=confidence, off_value=low_confidence)\n",
    "\n",
    "    loss = optax.softmax_cross_entropy(logits, soft_labels)\n",
    "    loss = loss - normalizing_constant\n",
    "\n",
    "    # ignore padded tokens from loss\n",
    "    loss = loss * padding_mask\n",
    "    loss = loss.sum()\n",
    "    num_labels = padding_mask.sum()  # number of actual values (rest padding)\n",
    "    \n",
    "    return loss, num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractor, tokenizer, processor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=model_lang, task=task)\n",
    "\n",
    "# we only need to set the task id when the language is specified (i.e. in a multilingual setting)\n",
    "tokenizer.set_prefix_tokens(language=args.model_lang, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(args.model_name_or_path, language=model_lang, task=task)\n",
    "    \n",
    "\n",
    "# model\n",
    "# FlaxWhisperForConditionalGeneration uses the FlaxWhisperPreTrainedModel forward method,\n",
    "# overrides the __call__ special method\n",
    "# FlaxWhisperForConditionalGeneration -> module_class = FlaxWhisperForConditionalGenerationModule\n",
    "# FlaxWhisperForConditionalGeneration(FlaxWhisperPreTrainedModel)\n",
    "# FlaxWhisperPreTrainedModel -> module = self.module_class\n",
    "# FlaxWhisperPreTrainedModel -> __call__ -> self.module.apply\n",
    "# FlaxWhisperForConditionalGenerationModule -> __call__ -> self.model -> FlaxWhisperModule\n",
    "# input_shape: typing.Tuple[int] = (b, 80, 3000)\n",
    "model = FlaxWhisperForConditionalGeneration.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    seed=seed,\n",
    "    dtype=getattr(jnp, dtype)\n",
    ")\n",
    "\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")  \n",
    "if freeze_encoder:\n",
    "    model.freeze_encoder()\n",
    "    model.model.encoder.gradient_checkpointing = False\n",
    "\n",
    "\n",
    "# dataset\n",
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(args.data_dir, args.data_lang, split=\"train+validation\", use_auth_token=True)\n",
    "common_voice[\"test\"] = load_dataset(args.data_dir, args.data_lang, split=\"test\", use_auth_token=True)\n",
    "\n",
    "# remove unused columns\n",
    "common_voice = common_voice.remove_columns(\n",
    "    [\n",
    "        \"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# select small dataset for testing\n",
    "if max_train_samples is not None:\n",
    "    common_voice[\"train\"] = common_voice[\"train\"].select(range(max_train_samples))\n",
    "\n",
    "if max_test_samples is not None:\n",
    "    common_voice[\"test\"] = common_voice[\"test\"].select(range(max_test_samples))\n",
    "\n",
    "# resample to 16kHz\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "\n",
    "\n",
    "# tokenizer and generation max length\n",
    "max_length = (\n",
    "    generation_max_length if generation_max_length is not None else model.config.max_length\n",
    ")\n",
    "\n",
    "# function to vectorize dataset\n",
    "# flax models need decoder_input_ids instead of labels\n",
    "# we need fixed length inputs for jitted functions\n",
    "# https://github.com/huggingface/transformers/blob/v4.29.1/src/transformers/models/whisper/feature_extraction_whisper.py#L254\n",
    "#if return_attention_mask: # rescale from sample (48000) to feature (3000)\n",
    "def prepare_dataset(batch, rank):\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(rank % jax.device_count())\n",
    "\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    # 80 x 3000\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    labels = tokenizer(\n",
    "        batch[\"sentence\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"np\")\n",
    "\n",
    "    # labels to compute loss\n",
    "    # 1 x generation length or max length\n",
    "    batch[\"labels\"] = labels[\"input_ids\"].flatten()\n",
    "    decoder_input_ids = shift_tokens_right(\n",
    "        labels[\"input_ids\"], model.config.pad_token_id, model.config.decoder_start_token_id\n",
    "    )\n",
    "    # decoder_input_ids to feed into the flax model\n",
    "    batch[\"decoder_input_ids\"] = np.asarray(decoder_input_ids).flatten()\n",
    "\n",
    "    # we need decoder_attention_mask so we can ignore pad tokens from loss\n",
    "    # completely masks decoder_input_ids\n",
    "    # leaves first pad token (after input ids) unmasked in labels\n",
    "    # need different mask for labels?\n",
    "    batch[\"decoder_attention_mask\"] = labels[\"attention_mask\"].flatten()\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "# vectorize dataset\n",
    "# input_features, decoder_input_ids, decoder_attention_mask, labels\n",
    "## multithreading errors ##\n",
    "common_voice = common_voice.map(\n",
    "    prepare_dataset,\n",
    "    with_rank=True,\n",
    "    remove_columns=common_voice.column_names[\"train\"],\n",
    "    desc=\"vectorize dataset\",\n",
    "    num_proc=num_workers,\n",
    ") \n",
    "\n",
    "# train and test datasets\n",
    "train_dataset = common_voice[\"train\"]\n",
    "test_dataset = common_voice[\"test\"]\n",
    "\n",
    "\n",
    "# metrics\n",
    "cer = evaluate.load(\"cer\")\n",
    "wer = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    result = {}\n",
    "    predictions = processor.batch_decode(\n",
    "        preds,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    references = processor.batch_decode(\n",
    "        labels,\n",
    "        group_tokens=False,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    # compute cer, wer\n",
    "    result[\"cer\"] = cer.compute(predictions=predictions, references=references)\n",
    "    result[\"wer\"] = wer.compute(predictions=predictions, references=references)\n",
    "    return result\n",
    "        \n",
    "\n",
    "# enable tensorboard only on the master node\n",
    "has_tensorboard = is_tensorboard_available()\n",
    "if has_tensorboard and jax.process_index() == 0:\n",
    "    try:\n",
    "        from flax.metrics.tensorboard import SummaryWriter\n",
    "        summary_writer = SummaryWriter(log_dir=Path(args.output_dir))\n",
    "    except ImportError as ie:\n",
    "        has_tensorboard = False\n",
    "        logger.warning(\n",
    "            f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n",
    "        )\n",
    "else:\n",
    "    logger.warning(\n",
    "        \"Unable to display metrics through TensorBoard because the package is not installed: \"\n",
    "        \"Please run pip install tensorboard to enable.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# compute effective batch size\n",
    "train_batch_size = int(per_device_train_batch_size) * jax.device_count()\n",
    "eval_batch_size = int(per_device_eval_batch_size) * jax.device_count()\n",
    "\n",
    "# eval steps in eval_dataset\n",
    "# different from args.eval_steps\n",
    "eval_steps = math.ceil(len(common_voice[\"test\"]) / eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create learning rate schedule\n",
    "warmup_fn = optax.linear_schedule(\n",
    "    init_value=0.0, end_value=learning_rate, transition_steps=warmup_steps\n",
    ")\n",
    "decay_fn = optax.linear_schedule(\n",
    "    init_value=learning_rate,\n",
    "    end_value=0,\n",
    "    transition_steps=train_steps - warmup_steps,\n",
    ")\n",
    "linear_decay_lr_schedule_fn = optax.join_schedules(\n",
    "    schedules=[warmup_fn, decay_fn], boundaries=[warmup_steps]\n",
    ")\n",
    "\n",
    "# we use Optax's \"masking\" functionality to not apply weight decay\n",
    "# to bias and LayerNorm scale parameters. decay_mask_fn returns a\n",
    "# mask boolean with the same structure as the parameters.\n",
    "# the mask is True for parameters that should be decayed.\n",
    "def decay_mask_fn(params):\n",
    "    flat_params = traverse_util.flatten_dict(params)\n",
    "    # find out all LayerNorm parameters\n",
    "    layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n",
    "    layer_norm_named_params = {\n",
    "        layer[-2:]\n",
    "        for layer_norm_name in layer_norm_candidates\n",
    "        for layer in flat_params.keys()\n",
    "        if layer_norm_name in \"\".join(layer).lower()\n",
    "    }\n",
    "    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n",
    "    return traverse_util.unflatten_dict(flat_mask)\n",
    "    \n",
    "# create adam optimizer\n",
    "adamw = optax.adamw(\n",
    "    learning_rate=linear_decay_lr_schedule_fn,\n",
    "    b1=adam_beta1,\n",
    "    b2=adam_beta2,\n",
    "    eps=adam_epsilon,\n",
    "    weight_decay=weight_decay,\n",
    "    mask=decay_mask_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup train state\n",
    "# FlaxWhisperForConditionalGenerationModule -> __call__ -> self.model -> FlaxWhisperModule\n",
    "#state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)\n",
    "state = train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw)\n",
    "    \n",
    "\n",
    "# define gradient update step fn\n",
    "# batch -> input_features, decoder_input_ids, decoder_attention_mask, labels\n",
    "# cant print values inside a jit compiled function\n",
    "\n",
    "# pmap -> replicate your model on devices, shard your data,\n",
    "# and have each calculate their individual loss and gradients.\n",
    "# pmean to average them across all devices and apply your gradient (psum)\n",
    "\n",
    "# use pjit for model sharding\n",
    "def train_step(state, batch, dropout_rng, label_smoothing_factor=label_smoothing_factor):\n",
    "    #dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n",
    "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
    "\n",
    "    def compute_loss(params):\n",
    "        labels = batch.pop(\"labels\")\n",
    "        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
    "        # decoder_attention_mask completely masks decoder_input_ids\n",
    "        # leaves first pad token (after input ids) unmasked in labels\n",
    "        loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
    "        return loss, num_labels\n",
    "\n",
    "    # value_and_grad\n",
    "    # creates a function that evaluates both fun and the gradient of fun\n",
    "    # returns a function with the same arguments as fun that evaluates both fun \n",
    "    # and the gradient of fun and returns them as a pair\n",
    "    # argnums -> which positional argument(s) to differentiate with respect to (default 0).\n",
    "    # if has_aux is True then a tuple of ((value, auxiliary_data), gradient) is returned.\n",
    "    grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n",
    "    (loss, num_labels), grad = grad_fn(state.params)\n",
    "    num_labels = jax.lax.psum(num_labels, \"batch\")  # AllReduce\n",
    "\n",
    "    # true loss = total loss / total samples\n",
    "    loss = jax.lax.psum(loss, \"batch\")  # AllReduce\n",
    "    loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
    "\n",
    "    # true grad = total grad / total samples\n",
    "    grad = jax.lax.psum(grad, \"batch\")  # AllReduce\n",
    "    # divide replicated grad by num_labels (number of actual values)\n",
    "    grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)\n",
    "    #new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n",
    "    new_state = state.apply_gradients(grads=grad)\n",
    "\n",
    "    metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step), \"num_labels\": num_labels}\n",
    "    return new_state, metrics, new_dropout_rng\n",
    "\n",
    "\n",
    "# define eval fn\n",
    "def eval_step(params, batch, label_smoothing_factor=0.0):\n",
    "\n",
    "    labels = batch.pop(\"labels\")\n",
    "    logits = model(**batch, params=params, train=False)[0]\n",
    "\n",
    "    loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n",
    "    num_labels = jax.lax.psum(num_labels, \"batch\") # AllReduce\n",
    "\n",
    "    # true loss = total loss / total samples\n",
    "    loss = jax.lax.psum(loss, \"batch\")  # AllReduce\n",
    "    loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n",
    "\n",
    "    metrics = {\"loss\": loss}\n",
    "    return metrics\n",
    "    \n",
    "\n",
    "# generation functions\n",
    "\n",
    "def make_generation_config(supress_en=False):\n",
    "\n",
    "    generation_config = GenerationConfig.from_pretrained(model_name_or_path)\n",
    "    gen_dict = generation_config.to_dict()\n",
    "    # add attributes to genration_config\n",
    "    # generation_config does not have \"langauge\", but generate() tries to use it\n",
    "    # can be empty dict here since being set in generate_step\n",
    "    gen_dict[\"language\"] = LANG_TO_ID[model_lang]\n",
    "    if supress_en:\n",
    "        # en tokens to suppress from multilingual vocab\n",
    "        en_tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny.en\")  # change if loaded locally\n",
    "        suppress_en_list = []\n",
    "        for key in en_tokenizer.encoder.keys():\n",
    "            if key in tokenizer.encoder.keys() and key.isalpha():\n",
    "                suppress_en_list.append(key)\n",
    "        # supress english tokens\n",
    "        gen_dict['suppress_tokens'].extend(tokenizer.encode(suppress_en_list, add_special_tokens=False))\n",
    "\n",
    "    # reload with new attributes\n",
    "    generation_config = GenerationConfig.from_dict(gen_dict)\n",
    "\n",
    "    return generation_config\n",
    "\n",
    "\n",
    "# max_length defined after tokenizer\n",
    "num_beams = num_beams if num_beams is not None else model.config.num_beams\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "# generation config\n",
    "generation_config = make_generation_config(supress_en=False)\n",
    "\n",
    "# batch -> input_features, decoder_input_ids, decoder_attention_mask, labels\n",
    "def generate_step(params, batch):\n",
    "    model.params = params\n",
    "    output_ids = model.generate(\n",
    "        batch[\"input_features\"],\n",
    "        generation_config=generation_config,\n",
    "        task=task,\n",
    "        language=LANG_TO_ID[model_lang],  # set lang here\n",
    "        is_multilingual=True,\n",
    "        **gen_kwargs\n",
    "    )   \n",
    "    return output_ids.sequences\n",
    "\n",
    "# create parallel version of the train and eval step\n",
    "# applying pmap() to a function will compile the function with XLA (similarly to jit()),\n",
    "# then execute it in parallel on XLA devices, such as multiple GPUs or multiple TPU cores.\n",
    "# it eplicates the function and executes each replica on its own XLA device in parallel.\n",
    "# donate_argnums -> specify which positional argument buffers are “donated” to the computation.\n",
    "# it is safe to donate argument buffers if you no longer need them once the computation has finished.\n",
    "# you should not reuse buffers that you donate to a computation,\n",
    "# jax will raise an error if you try to.\n",
    "# donate_argnums only work for positional arguments.\n",
    "p_train_step = jax.pmap(\n",
    "    partial(train_step, label_smoothing_factor=label_smoothing_factor), \"batch\", donate_argnums=(0,)\n",
    ")\n",
    "p_eval_step = jax.pmap(partial(eval_step, label_smoothing_factor=label_smoothing_factor), \"batch\")\n",
    "p_generate_step = jax.pmap(generate_step, \"batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total steps\n",
    "global_step = 0  \n",
    "train_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init checkpointer\n",
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=max_to_keep, create=True)\n",
    "# checkpoint manager\n",
    "checkpoint_manager = orbax.checkpoint.CheckpointManager(\n",
    "    output_dir,\n",
    "    orbax_checkpointer,\n",
    "    options\n",
    ")     \n",
    "\n",
    "# load from previous checkpoint\n",
    "if os.path.isdir(output_dir):\n",
    "    print('checkpoints found')\n",
    "    # get latest checkpoint\n",
    "    step = checkpoint_manager.latest_step()  # or choose step\n",
    "    print('restoring step : {}'.format(step))\n",
    "\n",
    "    # empty state and config to load state into\n",
    "    empty_state = train_state.TrainState.create(\n",
    "        apply_fn=model.__call__,\n",
    "        params=jax.tree_map(np.zeros_like, model.params),  # values of the tree leaf doesn't matter\n",
    "        tx=adamw,\n",
    "        #dropout_rng=dropout_rng\n",
    "    )\n",
    "    empty_config = model.config\n",
    "    #target = {'model': empty_state, 'config': empty_config, 'data': [jnp.zeros_like(x1)]}\n",
    "    target = {'state': empty_state, 'config': empty_config}  # state or model -> automate maybe\n",
    "\n",
    "    # restore\n",
    "    restored = checkpoint_manager.restore(step, items=target)\n",
    "    state = restored['state']\n",
    "    global_step = step\n",
    "\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"no checkpoint found\"\n",
    "    )\n",
    "\n",
    "\n",
    "# write fixed hyoerparameters to tensorboard\n",
    "if has_tensorboard and jax.process_index() == 0:\n",
    "    summary_writer.scalar(\"train_batch_size\", train_batch_size, global_step + 1)\n",
    "    summary_writer.scalar(\"eval_batch_size\", eval_batch_size, global_step + 1)\n",
    "\n",
    "    \n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(common_voice['train'])}\")\n",
    "logger.info(f\"  Num steps = {train_steps}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n",
    "\n",
    "\n",
    "# replicate the train state on each device\n",
    "#state = state.replicate()\n",
    "state = jax_utils.replicate(state)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# initialize training\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "dropout_rngs = jax.random.split(rng, jax.device_count())  # jax.local_device_count()\n",
    "\n",
    "# main progress bar\n",
    "progress_bar = tqdm(range(global_step, train_steps), position=0)\n",
    "\n",
    "train_start = time.time()\n",
    "train_metrics = []\n",
    "\n",
    "def train():\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # train\n",
    "        # create sampling rng\n",
    "        rng, input_rng = jax.random.split(rng)\n",
    "        train_loader = data_loader(input_rng, train_dataset, train_batch_size, shuffle=True)\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # input_features : b x 80 x 3000\n",
    "            # decoder_input_ids : b x max_length\n",
    "            # decoder_attention_mask : b X max_length\n",
    "            # labels : b X max_length\n",
    "\n",
    "            # check with multi gpu\n",
    "            # shard changes dim\n",
    "            batch = shard(batch) \n",
    "            state, train_metric, dropout_rngs = p_train_step(state, batch, dropout_rngs) \n",
    "            train_metrics.append(train_metric)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            # eval\n",
    "            # eval_loss with eval_step\n",
    "            # cer, wer with generate_step\n",
    "            if (global_step + 1) % eval_steps == 0:\n",
    "            #if True:  # for debugging eval step\n",
    "                train_time += time.time() - train_start\n",
    "                eval_metrics = []\n",
    "                eval_preds = []\n",
    "                eval_labels = []\n",
    "                result_dict = {}\n",
    "\n",
    "                train_metric = unreplicate(train_metric)\n",
    "                    \n",
    "                progress_bar.write(\n",
    "                    f\"Step ({global_step + 1} | Loss: {train_metric['loss']}, Learning Rate:\"\n",
    "                    f\" {train_metric['learning_rate']})\"\n",
    "                )\n",
    "\n",
    "                eval_loader = data_loader(input_rng, test_dataset, eval_batch_size, shuffle=True)\n",
    "\n",
    "                # eval progress bar\n",
    "                eval_bar = tqdm(range(eval_steps), position=1)\n",
    "                for batch in eval_loader:\n",
    "                    labels = batch[\"labels\"]\n",
    "                    metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
    "                        state.params, batch, min_device_batch=per_device_eval_batch_size)\n",
    "                    eval_metrics.append(metrics)\n",
    "\n",
    "                    # generation\n",
    "                    generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n",
    "                    eval_preds.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
    "                    eval_labels.extend(labels)\n",
    "        \n",
    "                    eval_bar.update(1)\n",
    "\n",
    "                # train metrics (loss)\n",
    "                train_metrics = get_metrics(train_metrics)  # dict\n",
    "                train_metrics = jax.tree_util.tree_map(jnp.mean, train_metrics)  # dict\n",
    "                # normalize eval metrics\n",
    "                # eval metrics (loss)\n",
    "                eval_metrics = get_metrics(eval_metrics)  # dict\n",
    "                eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)  # dict\n",
    "                # cer, wer\n",
    "                result = compute_metrics(eval_preds, eval_labels)\n",
    "                eval_metrics.update(result)\n",
    "\n",
    "                # collect results together\n",
    "                result_dict['train_time'] = train_time\n",
    "                result_dict['train_loss'] = train_metrics['loss']\n",
    "                result_dict['eval_loss'] = eval_metrics['loss']\n",
    "                result_dict['cer'] = eval_metrics['cer']\n",
    "                result_dict['wer'] = eval_metrics['wer']\n",
    "\n",
    "                # write to terminal and tensorboard\n",
    "                for key, val in result_dict.items():\n",
    "                    print('{} : {}'.format(key, val))\n",
    "                if has_tensorboard and jax.process_index() == 0:\n",
    "                    for key, val in result_dict.items():\n",
    "                        summary_writer.scalar(key, val, global_step + 1)\n",
    "\n",
    "                # save the model, optimizer, lr_scheduler, and seed states \n",
    "                ckpt = {'state': unreplicate(state), 'config': model.config}\n",
    "                save_args = orbax_utils.save_args_from_target(ckpt)\n",
    "                checkpoint_manager.save(global_step + 1, ckpt, save_kwargs={'save_args': save_args})\n",
    "\n",
    "\n",
    "            global_step += 1\n",
    "            train_metrics = []\n",
    "\n",
    "            if global_step >= train_steps : return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = []\n",
    "eval_preds = []\n",
    "eval_labels = []\n",
    "result_dict = {}\n",
    "\n",
    "rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "eval_loader = data_loader(input_rng, test_dataset, eval_batch_size, shuffle=True)\n",
    "\n",
    "# eval progress bar\n",
    "eval_bar = tqdm(range(eval_steps), position=0)\n",
    "for batch in eval_loader:\n",
    "    labels = batch[\"labels\"]\n",
    "    \n",
    "    metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n",
    "        state.params, batch, min_device_batch=per_device_eval_batch_size)  # new_state\n",
    "    eval_metrics.append(metrics)\n",
    "\n",
    "    # generation\n",
    "    generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)  # new_state\n",
    "    eval_preds.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n",
    "    eval_labels.extend(labels)\n",
    "    \n",
    "    eval_bar.update(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr_jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4 (main, Mar 31 2022, 03:38:35) [Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d381011b8b2da54efbee7772c701c6e43d8e200fe4417a4d92316b7d5561dbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
