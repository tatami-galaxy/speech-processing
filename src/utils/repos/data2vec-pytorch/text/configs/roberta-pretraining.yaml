modality: 'text'
device: 'cuda'
train:
  batch_size: 32
  num_epochs: 20
  checkpoints_dir: 'text/checkpoints/roberta-pretrain'
  log_dir: 'text/logs/roberta-pretrain'
  save_ckpt_freq: 20
criterion:
  loss_beta: 4
optimizer:
  lr: 0.0002
  weight_decay: 0.01
dataset:
  name: 'wikitext-103-v1'
  mlm_probability: 0.15
  valid_seq_lenghts: [12, 512]
  clean_dataset: false
model:
  average_top_k_layers: 10
  embed_dim: 768
  num_classes: null
  encoder_checkpoint: 'roberta-base'
  normalize_targets: false
  ema_decay: 0.999
  ema_end_decay: 0.9999
  ema_anneal_end_step: 300000

