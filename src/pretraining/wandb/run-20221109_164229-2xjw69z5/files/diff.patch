diff --git a/src/pretraining/wandb/latest-run b/src/pretraining/wandb/latest-run
index ce24169..6579ba5 120000
--- a/src/pretraining/wandb/latest-run
+++ b/src/pretraining/wandb/latest-run
@@ -1 +1 @@
-run-20221109_152648-1708iwv7
\ No newline at end of file
+run-20221109_164229-2xjw69z5
\ No newline at end of file
diff --git a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/code/src/pretraining/wav2vec2_pretraining.py b/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/code/src/pretraining/wav2vec2_pretraining.py
deleted file mode 100644
index 3e17f42..0000000
--- a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/code/src/pretraining/wav2vec2_pretraining.py
+++ /dev/null
@@ -1,560 +0,0 @@
-import os
-import socket
-from datetime import datetime
-import argparse
-import math
-import torch
-import torch.nn as nn
-import transformers
-import datasets
-from torch.utils.data.dataloader import DataLoader
-from torch.cuda import amp
-from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForPreTraining, Wav2Vec2Config
-from transformers import AdamW, SchedulerType, get_scheduler
-from transformers import SchedulerType, set_seed, is_wandb_available
-from datasets import load_from_disk
-from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices
-from accelerate import Accelerator
-from accelerate import DistributedDataParallelKwargs
-from accelerate.logging import get_logger
-from tqdm.auto import tqdm
-from dataclasses import dataclass
-from typing import Dict, List, Optional, Union
-
-logger = get_logger(__name__)
-
-
-def parse_args():
-    parser = argparse.ArgumentParser(description="Pretrain a Wav2Vec2 model")
-
-    parser.add_argument(
-        "--dataset",
-        type=str,
-        default="/home/ujan/speech-processing/data/processed/libri_vectorized",
-        help="Vectorized dataset directory",
-    )
-    parser.add_argument("--sampling_rate", type=int, default=16000,
-        help="Audio sampling rate. Default=16000 for wav2vec2",
-    )
-    parser.add_argument(
-        "--logging_steps",
-        type=int,
-        default=500,
-        help="Number of steps between each logging",
-    )
-    parser.add_argument(
-        "--saving_steps",
-        type=int,
-        default=500,
-        help="Number of steps between each logging",
-    )
-    parser.add_argument(
-        "--model_name_or_path",
-        type=str,
-        default="./",
-        help="Path to pretrained model or model identifier from huggingface.co/models.",
-    )
-    parser.add_argument(
-        "--per_device_train_batch_size",
-        type=int,
-        default=16,
-        help="Batch size (per device) for the training dataloader.",
-    )
-    parser.add_argument(
-        "--per_device_eval_batch_size",
-        type=int,
-        default=16,
-        help="Batch size (per device) for the evaluation dataloader.",
-    )
-    parser.add_argument(
-        "--learning_rate",
-        type=float,
-        default=5e-5,
-        help="Initial learning rate (after the potential warmup period) to use.",
-    )
-    parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
-    parser.add_argument("--num_train_epochs", type=int, default=3, help="Total number of training epochs to perform.")
-    parser.add_argument(
-        "--max_train_steps",
-        type=int,
-        default=None,
-        help="Total number of training steps to perform. If provided, overrides num_train_epochs.",
-    )
-    parser.add_argument(
-        "--gradient_accumulation_steps",
-        type=int,
-        default=1,
-        help="Number of updates steps to accumulate before performing a backward/update pass.",
-    )
-    parser.add_argument(
-        "--gradient_checkpointing",
-        action="store_true",
-        help="If True, use gradient checkpointing to save memory at the expense of slower backward pass.",
-    )
-    parser.add_argument(
-        "--lr_scheduler_type",
-        type=SchedulerType,
-        default="linear",
-        help="The scheduler type to use.",
-        choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
-    )
-    parser.add_argument(
-        "--num_warmup_steps", type=int, default=0, help="Number of steps for the warmup in the lr scheduler."
-    )
-    parser.add_argument("--output_dir", type=str, default="/home/ujan/speech-processing/models", help="Where to store the final model.") 
-    parser.add_argument("--seed", type=int, default=1, help="A seed for reproducible training.")
-    parser.add_argument(
-        "--max_gumbel_temperature",
-        type=float,
-        default=2.0,
-        help="Maximum temperature for gumbel softmax.",
-    )
-    parser.add_argument(
-        "--min_gumbel_temperature",
-        type=float,
-        default=0.5,
-        help="Minimum temperature for gumbel softmax.",
-    )
-    parser.add_argument(
-        "--gumbel_temperature_decay", type=float, default=0.999995, help="Decay of gumbel temperature during training."
-    )
-    # if you use mixed precision, you need all your tensors to have dimensions that are multiple of 8s
-    # to maximize the benefits of your tensor cores
-    # so pad_to_multiple_of=8 is a good value, unless you model has some pooling (like Funnel Transformer)
-    # in which case those 8 might be divided by 2 
-    # you’d need pad_to_multiple_of=32 for this model for instance, since there are two pooling operations
-    parser.add_argument(
-        "--pad_to_multiple_of",
-        type=int,
-        default=32,
-        help=(
-            "If set will pad the sequence to a multiple of the provided value. This is especially useful to enable the"
-            " use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta). Make it same as dataset.py"
-        ),
-    )
-    parser.add_argument(
-        "--adam_beta1",
-        type=float,
-        default=0.9,
-        help="Beta1 for AdamW optimizer",
-    )
-    parser.add_argument(
-        "--adam_beta2",
-        type=float,
-        default=0.999,
-        help="Beta2 for AdamW optimizer",
-    )
-    parser.add_argument(
-        "--adam_epsilon",
-        type=float,
-        default=1e-8,
-        help="Epsilon for AdamW optimizer",
-    )
-
-    args = parser.parse_args()
-
-
-    if args.output_dir is not None:
-        os.makedirs(args.output_dir, exist_ok=True)
-
-    return args
-
-
-@dataclass
-class DataCollatorForWav2Vec2Pretraining:
-    """
-    Data collator that will dynamically pad the inputs received and prepare masked indices
-    for self-supervised pretraining.
-    Args:
-        model (:class:`~transformers.Wav2Vec2ForPreTraining`):
-            The Wav2Vec2 model used for pretraining. The data collator needs to have access
-            to config and ``_get_feat_extract_output_lengths`` function for correct padding.
-        feature_extractor (:class:`~transformers.Wav2Vec2FeatureExtractor`):
-            The processor used for proccessing the data.
-        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):
-            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
-            among:
-            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
-              sequence if provided).
-            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the
-              maximum acceptable input length for the model if that argument is not provided.
-            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
-              different lengths).
-        max_length (:obj:`int`, `optional`):
-            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).
-        pad_to_multiple_of (:obj:`int`, `optional`):
-            If set will pad the sequence to a multiple of the provided value.
-            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
-            7.5 (Volta).
-    """
-
-    model: Wav2Vec2ForPreTraining
-    feature_extractor: Wav2Vec2FeatureExtractor
-    padding: Union[bool, str] = "longest"
-    pad_to_multiple_of: Optional[int] = None
-
-    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
-        # reformat list to dict and set to pytorch format
-        batch = self.feature_extractor.pad(
-            features,
-            padding=self.padding,
-            pad_to_multiple_of=self.pad_to_multiple_of,
-            return_tensors="pt",
-        )
-
-        device = batch["input_values"].device
-        batch_size = batch["input_values"].shape[0]
-
-        mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch["input_values"].shape[-1])
-        #print(mask_indices_seq_length) 
-        # make sure masked sequence length is a Python scalar
-        mask_indices_seq_length = int(mask_indices_seq_length)
-
-        # make sure that no loss is computed on padded inputs
-        if batch.get("attention_mask") is not None:
-            #print('attention mask not none')
-            # compute real output lengths according to convolution formula
-            batch["sub_attention_mask"] = self.model._get_feature_vector_attention_mask(
-                mask_indices_seq_length, batch["attention_mask"]
-            )
-
-        features_shape = (batch_size, mask_indices_seq_length)
-
-        # sample randomly masked indices
-        # Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for
-        # ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on
-        # CPU as part of the preprocessing during training.
-        mask_time_indices = _compute_mask_indices(
-            features_shape,
-            self.model.config.mask_time_prob,
-            self.model.config.mask_time_length,
-            attention_mask=batch.get("sub_attention_mask"),
-            min_masks = 1,
-        )
-
-        # sample negative indices
-        # for contrastive loss
-        sampled_negative_indices = _sample_negative_indices(
-            features_shape,
-            self.model.config.num_negatives,
-            mask_time_indices=mask_time_indices,
-        )
-        batch["mask_time_indices"] = torch.tensor(mask_time_indices, dtype=torch.long, device=device)
-        batch["sampled_negative_indices"] = torch.tensor(sampled_negative_indices, dtype=torch.long, device=device)
-
-        return batch
-
-
-def get_grad_norm(params, scale=1):
-    """Compute grad norm given a gradient scale."""
-    total_norm = 0.0
-    for p in params:
-        if p.grad is not None:
-            param_norm = (p.grad.detach().data / scale).norm(2)
-            total_norm += param_norm.item() ** 2
-    total_norm = total_norm**0.5
-    return total_norm
-
-
-def multiply_grads(params, c):
-    """Multiplies grads by a constant *c*."""
-    for p in params:
-        if p.grad is not None:
-            if torch.is_tensor(c):
-                c = c.to(p.grad.device)
-            p.grad.data.mul_(c)
-
-
-
-def main():
-
-    args = parse_args()
-
-    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.
-
-    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
-
-    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])
-
-    logger.info(accelerator.state, main_process_only=False)
-    if accelerator.is_local_main_process:
-
-        datasets.utils.logging.set_verbosity_warning()
-        transformers.utils.logging.set_verbosity_info()
-
-        # wandb
-        if is_wandb_available():
-            import wandb
-            wandb.init(project="wav2vec2", entity="suicune")
-            wandb.config = {"learning_rate": args.learning_rate, "epochs": args.num_train_epochs,
-            "train_batch_size": args.per_device_train_batch_size,
-            "eval_batch_size": args.per_device_eval_batch_size}
-
-    else:
-        datasets.utils.logging.set_verbosity_error()
-        transformers.utils.logging.set_verbosity_error()
-
-    # seed
-    if args.seed is not None:
-        set_seed(args.seed)
-
-
-    # load vectorized dataset
-    vectorized_datasets = load_from_disk(args.dataset)
-
-    # config
-
-    # huggingface pretraining script only supports "newer" stable layer norm architecture
-    # apply_spec_augment has to be True, mask_feature_prob has to be 0.0
-
-    # feat_extract_norm (str, optional, defaults to "group") — the norm to be applied to 1D convolutional layers in feature encoder
-    # one of "group" for group normalization of only the first 1D convolutional layer or
-    # "layer" for layer normalization of all 1D convolutional layers.
-
-    # apply_spec_augment (bool, optional, defaults to True) — Whether to apply SpecAugment data augmentation to the outputs of the feature encoder
-    # for reference see SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition
-
-    # mask_feature_prob (float, optional, defaults to 0.0) — Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked
-    # the masking procecure generates ”mask_feature_problen(feature_axis)/mask_time_length” independent masks over the axis
-    # if reasoning from the propability of each feature vector to be chosen as the start of the vector span to be masked,
-    # mask_feature_prob should be `prob_vector_startmask_feature_length
-    # note that overlap may decrease the actual percentage of masked vectors
-    # this is only relevant if apply_spec_augment is True`
-
-    config = Wav2Vec2Config(feat_extract_norm='layer')
-    
-    # model
-    model = Wav2Vec2ForPreTraining(config)
-
-    # define feature extractor (same as data proprocessing)
-    feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=args.sampling_rate,
-    padding_value=0.0, do_normalize=True, return_attention_mask=True, pad_to_multiple_of=args.pad_to_multiple_of) 
-
-    # data collator
-    data_collator = DataCollatorForWav2Vec2Pretraining(
-        model=model, feature_extractor=feature_extractor, pad_to_multiple_of=args.pad_to_multiple_of
-    )
-
-    # dataloaders
-    train_dataloader = DataLoader(
-        vectorized_datasets["train"],
-        shuffle=True,
-        collate_fn=data_collator,
-        batch_size=args.per_device_train_batch_size,
-    )
-    eval_dataloader = DataLoader(
-        vectorized_datasets["validation"], collate_fn=data_collator, batch_size=args.per_device_eval_batch_size
-    )
-
-    # optimizer
-    optimizer = AdamW(
-        list(model.parameters()),
-        lr=args.learning_rate,
-        betas=[args.adam_beta1, args.adam_beta2],
-        eps=args.adam_epsilon,
-    )
-
-    # prepare everything with our `accelerator`.
-    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
-        model, optimizer, train_dataloader, eval_dataloader
-    )
-
-    # scheduler and math around the number of training steps.
-    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
-
-    if args.max_train_steps is None:
-        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
-
-    lr_scheduler = get_scheduler(
-        name=args.lr_scheduler_type,
-        optimizer=optimizer,
-        num_warmup_steps=args.num_warmup_steps,
-        num_training_steps=args.max_train_steps,
-    )
-
-    # afterwards we recalculate our number of training epochs
-    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
-
-    # train
-    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
-
-    logger.info("***** Running training *****")
-    logger.info(f"  Num examples = {len(vectorized_datasets['train'])}")
-    logger.info(f"  Num Epochs = {args.num_train_epochs}")
-    logger.info(f"  Instantaneous batch size per device = {args.per_device_train_batch_size}")
-    logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
-    logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
-    logger.info(f"  Total optimization steps = {args.max_train_steps}")
-    completed_steps = 0
-    starting_epoch = 0
-
-    # Only show the progress bar once on each machine.
-    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
-    completed_steps = 0
-    starting_epoch = 0
-    for epoch in range(starting_epoch, args.num_train_epochs):
-
-        model.train()
-        for step, batch in enumerate(train_dataloader):
-            # compute num of losses
-            num_losses = batch["mask_time_indices"].sum()
-            sub_attention_mask = batch.pop("sub_attention_mask", None)
-            sub_attention_mask = (
-                sub_attention_mask if sub_attention_mask is not None else torch.ones_like(batch["mask_time_indices"])
-            )
-            percent_masked = num_losses / sub_attention_mask.sum()
-
-            # forward
-            outputs = model(**batch)
-
-            # divide loss by gradient accumulation steps since gradients
-            # are accumulated for multiple backward passes in PyTorch
-            loss = outputs.loss / args.gradient_accumulation_steps
-            accelerator.backward(loss)
-
-            # make sure that `num_losses` is summed for distributed training
-            # and average gradients over losses of all devices
-            if accelerator.state.num_processes > 1:
-                num_losses = accelerator.gather_for_metrics(num_losses).sum()
-                gradient_multiplier = accelerator.state.num_processes / num_losses
-                multiply_grads(model.module.parameters(), gradient_multiplier)
-            else:
-                multiply_grads(model.parameters(), 1 / num_losses)
-
-            # update step
-            if (step + 1) % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:
-
-                # compute grad norm for monitoring
-                scale = (
-                    accelerator.scaler._scale.item()
-                    if hasattr(accelerator, "scaler") and accelerator.scaler is not None
-                    else 1
-                )
-                if accelerator.state.num_processes > 1:
-                    grad_norm = get_grad_norm(model.module.parameters(), scale)
-                else:
-                    grad_norm = get_grad_norm(model.parameters(), scale)
-
-                # update parameters
-                optimizer.step()
-                optimizer.zero_grad()
-
-                if not accelerator.optimizer_step_was_skipped:
-                    lr_scheduler.step()
-                elif accelerator.is_local_main_process:
-                    progress_bar.write(
-                        f"Gradients have overflown - skipping update step... Updating gradient scale to {scale}..."
-                    )
-
-                # update gumbel temperature
-                gumbel_temperature = max(
-                    args.max_gumbel_temperature * args.gumbel_temperature_decay**completed_steps,
-                    args.min_gumbel_temperature,
-                )
-                if hasattr(model, "module"):
-                    model.module.set_gumbel_temperature(gumbel_temperature)
-                else:
-                    model.set_gumbel_temperature(gumbel_temperature)
-
-                progress_bar.update(1)
-                completed_steps += 1
-
-            # log all results
-            if (step + 1) % (args.gradient_accumulation_steps * args.logging_steps) == 0:
-                loss.detach()
-                outputs.contrastive_loss.detach()
-                outputs.diversity_loss.detach()
-
-                if accelerator.state.num_processes > 1:
-                    loss = accelerator.gather_for_metrics(loss).sum()
-                    outputs.contrastive_loss = accelerator.gather_for_metrics(outputs.contrastive_loss).sum()
-                    outputs.diversity_loss = accelerator.gather_for_metrics(outputs.diversity_loss).sum()
-                    percent_masked = accelerator.gather_for_metrics(percent_masked).sum()
-
-                train_logs = {
-                    "loss": (loss * args.gradient_accumulation_steps) / num_losses,
-                    "constrast_loss": outputs.contrastive_loss / num_losses,
-                    "div_loss": outputs.diversity_loss / num_losses,
-                    "%_mask_idx": percent_masked / accelerator.num_processes,
-                    "ppl": outputs.codevector_perplexity,
-                    "lr": torch.tensor(optimizer.param_groups[0]["lr"]),
-                    "temp": torch.tensor(gumbel_temperature),
-                    "grad_norm": torch.tensor(grad_norm),
-                }
-                log_str = ""
-                for k, v in train_logs.items():
-                    log_str += "| {}: {:.3e}".format(k, v.item())
-
-                if accelerator.is_local_main_process:
-                    progress_bar.write(log_str)
-                    if is_wandb_available():
-                        wandb.log(train_logs)
-
-            # save model every `args.saving_steps` steps
-            if (step + 1) % (args.gradient_accumulation_steps * args.saving_steps) == 0:
-                if (args.push_to_hub and epoch < args.num_train_epochs - 1) or args.output_dir is not None:
-                    accelerator.wait_for_everyone()
-                    unwrapped_model = accelerator.unwrap_model(model)
-                    unwrapped_model.save_pretrained(
-                        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
-                    )
-
-
-            # if completed steps > `args.max_train_steps` stop
-            if completed_steps >= args.max_train_steps:
-                break
-
-        
-        # validate!
-        model.eval()
-
-        # init logs
-        val_logs = {
-            "val_loss": 0,
-            "val_contrastive_loss": 0,
-            "val_diversity_loss": 0,
-            "val_num_losses": 0,
-        }
-        for step, batch in enumerate(eval_dataloader):
-            with torch.no_grad():
-                batch.pop("sub_attention_mask", None)
-                outputs = model(**batch)
-
-            val_logs["val_loss"] += outputs.loss
-            val_logs["val_contrastive_loss"] += outputs.contrastive_loss
-            val_logs["val_diversity_loss"] += outputs.diversity_loss
-            val_logs["val_num_losses"] += batch["mask_time_indices"].sum()
-
-        # sum over devices in multi-processing
-        if accelerator.num_processes > 1:
-            val_logs = {k: accelerator.gather_for_metrics(v).sum() for k, v in val_logs.items()}
-
-        val_logs = {k: v / val_logs["val_num_losses"] for k, v in val_logs.items()}
-
-        log_str = ""
-        for k, v in val_logs.items():
-            log_str += "| {}: {:.3e}".format(k, v.item())
-
-        if accelerator.is_local_main_process:
-            progress_bar.write(log_str)
-            if is_wandb_available():
-                wandb.log(val_logs)
-
-        if args.output_dir is not None:
-            accelerator.wait_for_everyone()
-            unwrapped_model = accelerator.unwrap_model(model)
-            unwrapped_model.save_pretrained(
-                args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
-            )
-
-
-if __name__ == "__main__":
-    main()
-
-
-
-
-
-
-        
-  
diff --git a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/conda-environment.yaml b/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/conda-environment.yaml
deleted file mode 100644
index 4371816..0000000
--- a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/conda-environment.yaml
+++ /dev/null
@@ -1,179 +0,0 @@
-name: asr
-channels:
-  - pytorch
-  - defaults
-dependencies:
-  - _libgcc_mutex=0.1=main
-  - _openmp_mutex=5.1=1_gnu
-  - blas=1.0=mkl
-  - brotlipy=0.7.0=py310h7f8727e_1002
-  - bzip2=1.0.8=h7b6447c_0
-  - ca-certificates=2022.07.19=h06a4308_0
-  - certifi=2022.9.14=py310h06a4308_0
-  - cffi=1.15.1=py310h74dc2b5_0
-  - charset-normalizer=2.0.4=pyhd3eb1b0_0
-  - cryptography=37.0.1=py310h9ce1e76_0
-  - cudatoolkit=11.3.1=h2bc3f7f_2
-  - ffmpeg=4.3=hf484d3e_0
-  - freetype=2.11.0=h70c0345_0
-  - giflib=5.2.1=h7b6447c_0
-  - gmp=6.2.1=h295c915_3
-  - gnutls=3.6.15=he1e5248_0
-  - idna=3.3=pyhd3eb1b0_0
-  - intel-openmp=2021.4.0=h06a4308_3561
-  - jpeg=9e=h7f8727e_0
-  - lame=3.100=h7b6447c_0
-  - lcms2=2.12=h3be6417_0
-  - ld_impl_linux-64=2.38=h1181459_1
-  - lerc=3.0=h295c915_0
-  - libdeflate=1.8=h7f8727e_5
-  - libffi=3.3=he6710b0_2
-  - libgcc-ng=11.2.0=h1234567_1
-  - libgomp=11.2.0=h1234567_1
-  - libiconv=1.16=h7f8727e_2
-  - libidn2=2.3.2=h7f8727e_0
-  - libpng=1.6.37=hbc83047_0
-  - libstdcxx-ng=11.2.0=h1234567_1
-  - libtasn1=4.16.0=h27cfd23_0
-  - libtiff=4.4.0=hecacb30_0
-  - libunistring=0.9.10=h27cfd23_0
-  - libuuid=1.0.3=h7f8727e_2
-  - libwebp=1.2.2=h55f646e_0
-  - libwebp-base=1.2.2=h7f8727e_0
-  - lz4-c=1.9.3=h295c915_1
-  - mkl=2021.4.0=h06a4308_640
-  - mkl-service=2.4.0=py310h7f8727e_0
-  - mkl_fft=1.3.1=py310hd6ae3a3_0
-  - mkl_random=1.2.2=py310h00e6091_0
-  - ncurses=6.3=h5eee18b_3
-  - nettle=3.7.3=hbbd107a_1
-  - numpy=1.23.1=py310h1794996_0
-  - numpy-base=1.23.1=py310hcba007f_0
-  - openh264=2.1.1=h4ff587b_0
-  - openssl=1.1.1q=h7f8727e_0
-  - pillow=9.2.0=py310hace64e9_1
-  - pip=22.1.2=py310h06a4308_0
-  - pycparser=2.21=pyhd3eb1b0_0
-  - pyopenssl=22.0.0=pyhd3eb1b0_0
-  - pysocks=1.7.1=py310h06a4308_0
-  - python=3.10.4=h12debd9_0
-  - pytorch=1.12.1=py3.10_cuda11.3_cudnn8.3.2_0
-  - pytorch-mutex=1.0=cuda
-  - readline=8.1.2=h7f8727e_1
-  - requests=2.28.1=py310h06a4308_0
-  - six=1.16.0=pyhd3eb1b0_1
-  - sqlite=3.39.2=h5082296_0
-  - tk=8.6.12=h1ccaba5_0
-  - torchaudio=0.12.1=py310_cu113
-  - torchvision=0.13.1=py310_cu113
-  - typing_extensions=4.3.0=py310h06a4308_0
-  - tzdata=2022c=h04d1e81_0
-  - urllib3=1.26.11=py310h06a4308_0
-  - wheel=0.37.1=pyhd3eb1b0_0
-  - xz=5.2.5=h7f8727e_1
-  - zlib=1.2.12=h5eee18b_3
-  - zstd=1.5.2=ha4553b6_0
-  - pip:
-    - accelerate==0.15.0.dev0
-    - aiohttp==3.8.3
-    - aiosignal==1.2.0
-    - appdirs==1.4.4
-    - argon2-cffi==21.3.0
-    - argon2-cffi-bindings==21.2.0
-    - asttokens==2.0.8
-    - async-timeout==4.0.2
-    - attrs==22.1.0
-    - audioread==3.0.0
-    - backcall==0.2.0
-    - beautifulsoup4==4.11.1
-    - bleach==5.0.1
-    - click==8.1.3
-    - datasets==2.5.1
-    - debugpy==1.6.3
-    - decorator==5.1.1
-    - defusedxml==0.7.1
-    - dill==0.3.5.1
-    - docker-pycreds==0.4.0
-    - entrypoints==0.4
-    - executing==1.1.0
-    - fastjsonschema==2.16.2
-    - filelock==3.8.0
-    - frozenlist==1.3.1
-    - fsspec==2022.8.2
-    - gitdb==4.0.9
-    - gitpython==3.1.27
-    - huggingface-hub==0.10.1
-    - ipykernel==6.16.0
-    - ipython==8.5.0
-    - ipython-genutils==0.2.0
-    - jedi==0.18.1
-    - jinja2==3.1.2
-    - joblib==1.2.0
-    - jsonschema==4.16.0
-    - jupyter-client==7.3.5
-    - jupyter-core==4.11.1
-    - jupyterlab-pygments==0.2.2
-    - librosa==0.9.2
-    - llvmlite==0.39.1
-    - markupsafe==2.1.1
-    - matplotlib-inline==0.1.6
-    - mistune==2.0.4
-    - multidict==6.0.2
-    - multiprocess==0.70.13
-    - nbclient==0.6.8
-    - nbconvert==7.1.0
-    - nbformat==5.6.1
-    - nest-asyncio==1.5.6
-    - notebook==6.4.12
-    - numba==0.56.2
-    - packaging==21.3
-    - pandas==1.5.0
-    - pandocfilters==1.5.0
-    - parso==0.8.3
-    - pathtools==0.1.2
-    - pexpect==4.8.0
-    - pickleshare==0.7.5
-    - pooch==1.6.0
-    - prometheus-client==0.14.1
-    - promise==2.3
-    - prompt-toolkit==3.0.31
-    - protobuf==3.20.2
-    - psutil==5.9.2
-    - ptyprocess==0.7.0
-    - pure-eval==0.2.2
-    - pyarrow==9.0.0
-    - pygments==2.13.0
-    - pyparsing==3.0.9
-    - pyrsistent==0.18.1
-    - python-dateutil==2.8.2
-    - pytz==2022.2.1
-    - pyyaml==6.0
-    - pyzmq==24.0.1
-    - regex==2022.9.13
-    - resampy==0.4.2
-    - responses==0.18.0
-    - scikit-learn==1.1.2
-    - scipy==1.9.1
-    - send2trash==1.8.0
-    - sentry-sdk==1.9.8
-    - setproctitle==1.3.2
-    - setuptools==59.8.0
-    - shortuuid==1.0.9
-    - smmap==5.0.0
-    - soundfile==0.10.3.post1
-    - soupsieve==2.3.2.post1
-    - stack-data==0.5.1
-    - terminado==0.16.0
-    - threadpoolctl==3.1.0
-    - tinycss2==1.1.1
-    - tokenizers==0.12.1
-    - tornado==6.2
-    - tqdm==4.64.1
-    - traitlets==5.4.0
-    - transformers==4.24.0.dev0
-    - wandb==0.13.3
-    - wcwidth==0.2.5
-    - webencodings==0.5.1
-    - xxhash==3.0.0
-    - yarl==1.8.1
-prefix: /home/ujan/anaconda3/envs/asr
diff --git a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/config.yaml b/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/config.yaml
deleted file mode 100644
index 821ac21..0000000
--- a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/config.yaml
+++ /dev/null
@@ -1,35 +0,0 @@
-wandb_version: 1
-
-_wandb:
-  desc: null
-  value:
-    cli_version: 0.13.3
-    code_path: code/src/pretraining/wav2vec2_pretraining.py
-    framework: huggingface
-    huggingface_version: 4.24.0.dev0
-    is_jupyter_run: false
-    is_kaggle_kernel: false
-    python_version: 3.10.4
-    start_time: 1667987808.369962
-    t:
-      1:
-      - 1
-      - 11
-      - 49
-      - 51
-      - 55
-      - 71
-      2:
-      - 1
-      - 11
-      - 49
-      - 51
-      - 55
-      - 71
-      3:
-      - 23
-      4: 3.10.4
-      5: 0.13.3
-      6: 4.24.0.dev0
-      8:
-      - 5
diff --git a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/diff.patch b/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/diff.patch
deleted file mode 100644
index b874e9c..0000000
--- a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/diff.patch
+++ /dev/null
@@ -1,25 +0,0 @@
-diff --git a/src/pretraining/wav2vec2_pretraining.py b/src/pretraining/wav2vec2_pretraining.py
-index 8f41ad6..3e17f42 100644
---- a/src/pretraining/wav2vec2_pretraining.py
-+++ b/src/pretraining/wav2vec2_pretraining.py
-@@ -15,6 +15,7 @@ from transformers import SchedulerType, set_seed, is_wandb_available
- from datasets import load_from_disk
- from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices
- from accelerate import Accelerator
-+from accelerate import DistributedDataParallelKwargs
- from accelerate.logging import get_logger
- from tqdm.auto import tqdm
- from dataclasses import dataclass
-@@ -270,7 +271,11 @@ def main():
-     args = parse_args()
- 
-     # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.
--    accelerator = Accelerator()
-+
-+    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
-+
-+    accelerator = Accelerator(kwargs_handlers=[ddp_kwargs])
-+
-     logger.info(accelerator.state, main_process_only=False)
-     if accelerator.is_local_main_process:
- 
diff --git a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/requirements.txt b/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/requirements.txt
deleted file mode 100644
index fe4e670..0000000
--- a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/requirements.txt
+++ /dev/null
@@ -1,125 +0,0 @@
-accelerate==0.15.0.dev0
-aiohttp==3.8.3
-aiosignal==1.2.0
-appdirs==1.4.4
-argon2-cffi-bindings==21.2.0
-argon2-cffi==21.3.0
-asttokens==2.0.8
-async-timeout==4.0.2
-attrs==22.1.0
-audioread==3.0.0
-backcall==0.2.0
-beautifulsoup4==4.11.1
-bleach==5.0.1
-brotlipy==0.7.0
-certifi==2022.9.14
-cffi==1.15.1
-charset-normalizer==2.0.4
-click==8.1.3
-cryptography==37.0.1
-datasets==2.5.1
-debugpy==1.6.3
-decorator==5.1.1
-defusedxml==0.7.1
-dill==0.3.5.1
-docker-pycreds==0.4.0
-entrypoints==0.4
-executing==1.1.0
-fastjsonschema==2.16.2
-filelock==3.8.0
-frozenlist==1.3.1
-fsspec==2022.8.2
-gitdb==4.0.9
-gitpython==3.1.27
-huggingface-hub==0.10.1
-idna==3.3
-ipykernel==6.16.0
-ipython-genutils==0.2.0
-ipython==8.5.0
-jedi==0.18.1
-jinja2==3.1.2
-joblib==1.2.0
-jsonschema==4.16.0
-jupyter-client==7.3.5
-jupyter-core==4.11.1
-jupyterlab-pygments==0.2.2
-librosa==0.9.2
-llvmlite==0.39.1
-markupsafe==2.1.1
-matplotlib-inline==0.1.6
-mistune==2.0.4
-mkl-fft==1.3.1
-mkl-random==1.2.2
-mkl-service==2.4.0
-multidict==6.0.2
-multiprocess==0.70.13
-nbclient==0.6.8
-nbconvert==7.1.0
-nbformat==5.6.1
-nest-asyncio==1.5.6
-notebook==6.4.12
-numba==0.56.2
-numpy==1.23.1
-packaging==21.3
-pandas==1.5.0
-pandocfilters==1.5.0
-parso==0.8.3
-pathtools==0.1.2
-pexpect==4.8.0
-pickleshare==0.7.5
-pillow==9.2.0
-pip==22.1.2
-pooch==1.6.0
-prometheus-client==0.14.1
-promise==2.3
-prompt-toolkit==3.0.31
-protobuf==3.20.2
-psutil==5.9.2
-ptyprocess==0.7.0
-pure-eval==0.2.2
-pyarrow==9.0.0
-pycparser==2.21
-pygments==2.13.0
-pyopenssl==22.0.0
-pyparsing==3.0.9
-pyrsistent==0.18.1
-pysocks==1.7.1
-python-dateutil==2.8.2
-pytz==2022.2.1
-pyyaml==6.0
-pyzmq==24.0.1
-regex==2022.9.13
-requests==2.28.1
-resampy==0.4.2
-responses==0.18.0
-scikit-learn==1.1.2
-scipy==1.9.1
-send2trash==1.8.0
-sentry-sdk==1.9.8
-setproctitle==1.3.2
-setuptools==59.8.0
-shortuuid==1.0.9
-six==1.16.0
-smmap==5.0.0
-soundfile==0.10.3.post1
-soupsieve==2.3.2.post1
-stack-data==0.5.1
-terminado==0.16.0
-threadpoolctl==3.1.0
-tinycss2==1.1.1
-tokenizers==0.12.1
-torch==1.12.1
-torchaudio==0.12.1
-torchvision==0.13.1
-tornado==6.2
-tqdm==4.64.1
-traitlets==5.4.0
-transformers==4.24.0.dev0
-typing-extensions==4.3.0
-urllib3==1.26.11
-wandb==0.13.3
-wcwidth==0.2.5
-webencodings==0.5.1
-wheel==0.37.1
-xxhash==3.0.0
-yarl==1.8.1
\ No newline at end of file
diff --git a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/wandb-metadata.json b/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/wandb-metadata.json
deleted file mode 100644
index e07a0a9..0000000
--- a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/wandb-metadata.json
+++ /dev/null
@@ -1,24 +0,0 @@
-{
-    "os": "Linux-5.4.0-128-generic-x86_64-with-glibc2.31",
-    "python": "3.10.4",
-    "heartbeatAt": "2022-11-09T09:56:49.378563",
-    "startedAt": "2022-11-09T09:56:48.362865",
-    "docker": null,
-    "gpu": "NVIDIA A100-SXM4-80GB",
-    "gpu_count": 5,
-    "cpu_count": 128,
-    "cuda": null,
-    "args": [],
-    "state": "running",
-    "program": "/home/ujan/speech-processing/src/pretraining/wav2vec2_pretraining.py",
-    "codePath": "src/pretraining/wav2vec2_pretraining.py",
-    "git": {
-        "remote": "https://github.com/tatami-galaxy/speech-processing.git",
-        "commit": "9fcd9438ec086d3f6a778580a9a1fc1d9246dc20"
-    },
-    "email": "ujandev@gmail.com",
-    "root": "/home/ujan/speech-processing",
-    "host": "frsd-DGX-Station-A100",
-    "username": "ujan",
-    "executable": "/home/ujan/anaconda3/envs/asr/bin/python"
-}
diff --git a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/wandb-summary.json b/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/wandb-summary.json
deleted file mode 100644
index 9e26dfe..0000000
--- a/src/pretraining/wandb/run-20221109_152648-1708iwv7/files/wandb-summary.json
+++ /dev/null
@@ -1 +0,0 @@
-{}
\ No newline at end of file
diff --git a/src/pretraining/wandb/run-20221109_152648-1708iwv7/run-1708iwv7.wandb b/src/pretraining/wandb/run-20221109_152648-1708iwv7/run-1708iwv7.wandb
deleted file mode 100644
index bec0c2b..0000000
Binary files a/src/pretraining/wandb/run-20221109_152648-1708iwv7/run-1708iwv7.wandb and /dev/null differ
diff --git a/src/pretraining/wav2vec2_pretraining.py b/src/pretraining/wav2vec2_pretraining.py
index 3e17f42..a16f6fd 100644
--- a/src/pretraining/wav2vec2_pretraining.py
+++ b/src/pretraining/wav2vec2_pretraining.py
@@ -57,19 +57,19 @@ def parse_args():
     parser.add_argument(
         "--per_device_train_batch_size",
         type=int,
-        default=16,
+        default=8, # 16
         help="Batch size (per device) for the training dataloader.",
     )
     parser.add_argument(
         "--per_device_eval_batch_size",
         type=int,
-        default=16,
+        default=8, # 16
         help="Batch size (per device) for the evaluation dataloader.",
     )
     parser.add_argument(
         "--learning_rate",
         type=float,
-        default=5e-5,
+        default=5e-5,  # 5e-5
         help="Initial learning rate (after the potential warmup period) to use.",
     )
     parser.add_argument("--weight_decay", type=float, default=0.0, help="Weight decay to use.")
@@ -492,7 +492,7 @@ def main():
 
             # save model every `args.saving_steps` steps
             if (step + 1) % (args.gradient_accumulation_steps * args.saving_steps) == 0:
-                if (args.push_to_hub and epoch < args.num_train_epochs - 1) or args.output_dir is not None:
+                if (epoch < args.num_train_epochs - 1) or args.output_dir is not None:
                     accelerator.wait_for_everyone()
                     unwrapped_model = accelerator.unwrap_model(model)
                     unwrapped_model.save_pretrained(
